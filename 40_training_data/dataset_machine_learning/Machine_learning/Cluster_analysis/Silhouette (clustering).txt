Silhouette is a method of interpretation and validation of consistency within clusters of data. The technique provides a succinct graphical representation of how well each object has been classified. It was proposed by Belgian statistician Peter Rousseeuw in 1987.

The silhouette value is a measure of how similar an object is to its own cluster (cohesion) compared to other clusters (separation). The silhouette value ranges from &minus;1 to +1, where a high value indicates that the object is well matched to its own cluster and poorly matched to neighboring clusters. If most objects have a high value, then the clustering configuration is appropriate. If many points have a low or negative value, then the clustering configuration may have too many or too few clusters. A clustering with an average silhouette width of over 0.7 is considered to be "strong", a value over 0.5 "reasonable", and over 0.25 "weak". However, with an increasing dimensionality of the data, it becomes difficult to achieve such high values because of the curse of dimensionality, as the distances become more similar.  The silhouette score is specialized for measuring cluster quality when the clusters are convex-shaped, and may not perform well if the data clusters have irregular shapes or are of varying sizes. The silhouette value can be calculated with any distance metric, such as Euclidean distance or Manhattan distance.

According to a scientific article published in PeerJ Computer Science in 2025, the Silhouette coefficient (along with the Davies-Bouldin index) can be more informative than other popular clustering internal metrics such as Dunn index, Calinski-Harabasz index, Shannon entropy, and Gap statistic when applied to convex-shaped clusters.

Definition

data mining suite. At the bottom of the plot, silhouette identifies dolphin and porpoise as outliers in the group of mammals.]]

Assume the data have been clustered via any technique, such as k-medoids or k-means, into k clusters.

For data point i \in C_i (data point i in the cluster C_i), let

a(i) = \frac{1} \sum_{j \in C_j} d(i, j)

to be the smallest (hence the \min operator in the formula) mean distance of i to all points in any other cluster (i.e., in any cluster of which i is not a member). The cluster with the smallest mean dissimilarity is said to be the "neighboring cluster" of i because it is the next best fit cluster for point i.

We now define a silhouette (value) of one data point i

s(i) = \frac{b(i) - a(i)}{\max\{a(i),b(i)\}} , if |C_i| > 1

and

s(i) = 0, if |C_i| = 1,

which can also be written as

s(i) = \begin{cases}

1-\frac{a(i)}{b(i)}, & \mbox{ if } a(i)  b(i) \\

\end{cases}

From the above definition, s(i) is bounded to the interval [-1,1], i.e.  -1 \le s(i) \le 1.

Note that a(i) is not clearly defined for clusters with size = 1, in which case we set s(i)=0. This choice is arbitrary, but neutral in the sense that it is at the midpoint of the bounds, -1 and 1.

Kaufman et al. introduced the term silhouette coefficient for the maximum value of the mean s(i) over all data of the entire dataset, i.e.,

SC = \max_k \tilde{s}\left(k\right),

where \tilde{s}\left(k\right) represents the mean s(i) over all data of the entire dataset for a specific number of clusters k.

Simplified Silhouette and Medoid Silhouette

Computing the silhouette coefficient needs all \mathcal{O}(N^2) pairwise distances, making this evaluation much more costly than clustering with k-means. For a clustering with centers \mu_{C_I} for each cluster C_I, we can use the following simplified Silhouette for each point i\in C_I instead, which can be computed using only \mathcal{O}(Nk) distances:

a'(i)=d(i,\mu_{C_I}) and b'(i)=\min_{C_J\neq C_I} d(i,\mu_{C_J}),

which has the additional benefit that a'(i) is always defined, then define accordingly the simplified silhouette and simplified silhouette coefficient

s'(i) = \frac{b'(i) - a'(i)}{\max\{a'(i),b'(i)\}}

SC' = \max_k \frac1N \sum_i s'\left(i\right).

If the cluster centers are medoids (as in k-medoids clustering) instead of arithmetic means (as in k-means clustering), this is also called the medoid-based silhouette or medoid silhouette.

If every object is assigned to the nearest medoid (as in k-medoids clustering), we know that a'(i)\leq b'(i), and hence s'(i) = \frac{b'(i) - a'(i)}{b'(i)} = 1 - \frac{a'(i)}{b'(i)} .

By adopting recent improvements to the PAM algorithm, FastMSC reduces the runtime using the medoid silhouette to just \mathcal{O}(N^2i).

This algorithm needs pairwise distances and is typically implemented with a pairwise distance matrix. The \mathcal{O}(N^2) memory requirement is the main limiting factor for applying this to very large data sets.

See also

Cluster analysis

Daviesâ€“Bouldin index

Calinski-Harabasz index

Dunn index

Determining the number of clusters in a data set

Density-based clustering validation

References

Category:Clustering criteria

Category:Cluster analysis

Category:Statistical methods