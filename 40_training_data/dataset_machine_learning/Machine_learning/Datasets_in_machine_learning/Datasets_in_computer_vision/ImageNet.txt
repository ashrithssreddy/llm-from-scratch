The ImageNet project is a large visual database designed for use in visual object recognition software research. More than 14 million images have been hand-annotated by the project to indicate what objects are pictured and in at least one million of the images, bounding boxes are also provided. ImageNet contains more than 20,000 categories, The database of annotations of third-party image URLs is freely available directly from ImageNet, though the actual images are not owned by ImageNet. Since 2010, the ImageNet project runs an annual software contest, the ImageNet Large Scale Visual Recognition Challenge (ILSVRC), where software programs compete to correctly classify and detect objects and scenes. The challenge uses a "trimmed" list of one thousand non-overlapping classes. In 2007, Li met with Princeton professor Christiane Fellbaum, one of the creators of WordNet, to discuss the project. As a result of this meeting, Li went on to build ImageNet starting from the roughly 22,000 nouns of WordNet and using many of its features. that the average person recognizes roughly 30,000 different kinds of objects.

As an assistant professor at Princeton, Li assembled a team of researchers to work on the ImageNet project. They used Amazon Mechanical Turk to help with the classification of images. Labeling started in July 2008 and ended in April 2010. It took 49K workers from 167 countries filtering and labeling over 160M candidate images. They had enough budget to have each of the 14 million images labelled three times.

They presented their database for the first time as a poster at the 2009 Conference on Computer Vision and Pattern Recognition (CVPR) in Florida, titled "ImageNet: A Preview of a Large-scale HierarchicalÂ Dataset". The poster was reused at Vision Sciences Society 2009.

In 2009, Alex Berg suggested adding object localization as a task. Li approached PASCAL Visual Object Classes contest in 2009 for a collaboration. It resulted in the subsequent  ImageNet Large Scale Visual Recognition Challenge starting in 2010, which has 1000 classes and object localization, as compared to PASCAL VOC which had just 20 classes and 19,737 images (in 2010). achieved a top-5 error of 15.3% in the ImageNet 2012 Challenge, more than 10.8 percentage points lower than that of the runner-up. Using convolutional neural networks was feasible due to the use of graphics processing units (GPUs) during training,

In 2015, AlexNet was outperformed by Microsoft's very deep CNN with over 100 layers, which won the ImageNet 2015 contest, having 3.57% error on the test set.

Andrej Karpathy estimated in 2014 that with concentrated effort, he could reach 5.1% error rate, and ~10 people from his lab reached ~12-13% with less effort. It was estimated that with maximal effort, a human could reach 2.4%.

In 2012, ImageNet was the world's largest academic user of Mechanical Turk. The average worker identified 50 images per minute.

Total number of non-empty synsets: 21841

Total number of images: 14,197,122

Number of images with bounding box annotations: 1,034,908

Number of synsets with SIFT features: 1000

Number of images with SIFT features: 1.2 million

Categories

The categories of ImageNet were filtered from the WordNet concepts. Each concept, since it can contain multiple synonyms (for example, "kitty" and "young cat"), so each concept is called a "synonym set" or "synset". There were more than 100,000 synsets in WordNet 3.0, majority of them are nouns (80,000+). The ImageNet dataset filtered these to 21,841 synsets that are countable nouns that can be visually illustrated.

Each synset in WordNet 3.0 has a "WordNet ID" (wnid), which is a concatenation of part of speech and an "offset" (a unique identifying number). Every wnid starts with "n" because ImageNet only includes nouns. For example, the wnid of synset "dog, domestic dog, Canis familiaris" is "n02084071".

The categories in ImageNet fall into 9 levels, from level 1 (such as "mammal") to level 9 (such as "German shepherd").

ImageNet consists of images in RGB format with varying resolutions. For example, in ImageNet 2012, "fish" category, the resolution ranges from 4288 x 2848 to 75 x 56. In machine learning, these are typically preprocessed into a standard constant resolution, and whitened, before further processing by neural networks.

For example, in PyTorch, ImageNet images are by default normalized by dividing the pixel values so that they fall between 0 and 1, then subtracting by [0.485, 0.456, 0.406], then dividing by [0.229, 0.224, 0.225]. These are the mean and standard deviations for ImageNet, so this whitens the input data.

Labels and annotations

Each image is labelled with exactly one wnid.

Dense SIFT features (raw SIFT descriptors, quantized codewords, and coordinates of each descriptor/codeword) for ImageNet-1K were available for download, designed for bag of visual words.

The bounding boxes of objects were available for about 3000 popular synsets with on average 150 images in each synset.

Furthermore, some images have attributes. They released 25 attributes for ~400 popular synsets:

Color: black, blue, brown, gray, green, orange, pink, red, violet, white, yellow

Pattern: spotted, striped

Shape: long, round, rectangular, square

Texture: furry, smooth, rough, shiny, metallic, vegetation, wooden, wet

ImageNet-21K

The full original dataset is referred to as ImageNet-21K. ImageNet-21k contains 14,197,122 images divided into 21,841 classes. Some papers round this up and name it ImageNet-22k.

The full ImageNet-21k was released in Fall of 2011, as fall11_whole.tar. There is no official train-validation-test split for ImageNet-21k. Some classes contain only 1-10 samples, while others contain thousands.

Each category in ImageNet-1K is a leaf category, meaning that there are no child nodes below it, unlike ImageNet-21K. For example, in ImageNet-21K, there are some images categorized as simply "mammal", whereas in ImageNet-1K, there are only images categorized as things like "German shepherd", since there are no child-words below "German shepherd".

In 2021 winter, ImageNet-21k was updated. 2702 categories in the "person" subtree were removed to prevent "problematic behaviors" in a trained model. The result was that only 130 synsets in "person" subtree remained. Furthermore, in 2021, ImageNet-1k was updated by blurring out faces appearing in the 997 non-person categories. They found, out of all 1,431,093 images in ImageNet-1k, 243,198 images (17%) contain at least one face. And the total number of faces adds up to 562,626. They found training models on the dataset with these faces blurred caused minimal loss in performance.

ImageNet-C is an adversarially perturbed version of ImageNet constructed in 2019.

ImageNetV2 was a new dataset containing three test sets with 10,000 each, constructed by the same methodology as the original ImageNet.

ImageNet-21K-P was a filtered and cleaned subset of ImageNet-21K, with 12,358,688 images from 11,221 categories. All Images were resized to 224 x 224px. It achieved 52.9% in classification accuracy and 71.8% in top-5 accuracy. It was trained for 4 days on three 8-core machines (dual quad-core 2&nbsp;GHz Intel Xeon CPU).

The second competition in 2011 had fewer teams, with another SVM winning at top-5 error rate 25%. Fisher vectors. It achieved 74.2% in top-5 accuracy.

In 2012, a deep convolutional neural net called AlexNet achieved 84.7% in top-5 accuracy, a great leap forward. The second place was by Oxford VGG, which uses the previous generic architecture of SVM, SIFT, color statistics, Fisher vectors, etc. In the next couple of years, top-5 accuracy grew to above 90%. While the 2012 breakthrough "combined pieces that were all there before", the dramatic quantitative improvement marked the start of an industry-wide artificial intelligence boom. The winning entry for classification was an ensemble of multiple CNNs by Clarifai. The winning entry for localization was VGGNet. In 2017, 29 of 38 competing teams had greater than 95% accuracy. In 2017 ImageNet stated it would roll out a new, much more difficult challenge in 2018 that involves classifying 3D objects using natural language. Because creating 3D data is more costly than annotating a pre-existing 2D image, the dataset is expected to be smaller. The applications of progress in this area would range from robotic navigation to augmented reality. However, as one of the challenge's organizers, Olga Russakovsky, pointed out in 2015, the  ILSVRC is over only 1000 categories; humans can recognize a larger number of categories, and also (unlike the programs) can judge the context of an image.

In 2016, the winning entry was CUImage, an ensemble model of 6 networks: Inception v3, Inception v4, Inception ResNet v2, ResNet 200, Wide ResNet 68, and Wide ResNet 3. The runner-up was ResNeXt, which combines the Inception module with ResNet.

In 2017, the winning entry was the Squeeze-and-Excitation Network (SENet), reducing the top-5 error to 2.251%.

The organizers of the competition stated in 2017 that the 2017 competition would be the last one, since the benchmark has been solved and no longer posed a challenge. They also stated that they would organize a new competition on 3D images. It is also found that around 10% of ImageNet-1k contains ambiguous or erroneous labels, and that, when presented with a model's prediction and the original ImageNet label, human annotators prefer the prediction of a state of the art model in 2020 trained on the original ImageNet, suggesting that ImageNet-1k has been saturated.

A study of the history of the multiple layers (taxonomy, object classes and labeling) of ImageNet and WordNet in 2019 described how bias is deeply embedded in most classification approaches for all sorts of images. ImageNet is working to address various sources of bias.

One downside of WordNet use is the categories may be more "elevated" than would be optimal for ImageNet: "Most people are more interested in Lady Gaga or the iPod Mini than in this rare kind of diplodocus."

See also

Computer vision

List of datasets for machine learning research

WordNet

References

Primary sources

External links

Category:Computer science competitions

Category:2009 in computing

Category:Object recognition and categorization

Category:Databases

Category:Datasets in computer vision