{{Infobox software

name = ROCm

logo = ROCm logo.png

logo size = 250px

screenshot =

developer = AMD

genre = GPGPU libraries and APIs

released =

latest_release_version = 7.1.0

latest_release_date =

repo = Meta-repository

operating_system = Linux, Windows

platform = Supported GPUs

programming language = C, C++, Python, Fortran, Julia

middleware = HIP

engine = AMDgpu kernel driver, HIPCC, a LLVM-based compiler

size = Islands]]

! SeaIslands

! VolcanicIslands

! ArcticIslands/Polaris

! Vega

! Navi 1X

! Navi 2X

! Navi 3X

! Navi 4X

- style="border-top:2px solid grey"

!  | Released

Jan 2012

Sep 2013

Jun 2015

Jun 2016

Jun 2017

Jul 2019

Nov 2020

Dec 2022

Jan 2025

-

!  | Marketing Name

Radeon HD 7000

Radeon Rx 200

Radeon Rx 300

Radeon RX 400/500

Radeon RX Vega/Radeon VII(7&nbsp;nm)

Radeon RX 5000

Radeon RX 6000

Radeon RX 7000

Radeon RX 9000

-

!  | AMD support

colspan="3"  || colspan=6

-

!  | Instruction set

colspan=5 | GCN instruction set

colspan=4 | RDNA instruction set

-

!  | Microarchitecture

GCN 1st gen

GCN 2nd gen

GCN 3rd gen

GCN 4th gen

GCN 5th gen

RDNA

RDNA 2

RDNA 3

RDNA 4

-

!  | Type

colspan="9" |Unified shader model

- style="border-top:2px solid grey"

!  | ROCm

colspan=3

colspan=1

colspan=1

colspan=1 {{efn|Navi 1.x support in rocBLAS was broken until version 6.1. Before version 5.3 an environmental variable could be used to force the use of gfx1030 code on these GPUs for some functionality, but version 5.3 caused gfx1030 code to no longer run on gfx1010 GPUs.}}

colspan=3

-

!  | OpenCL

1.2 (on Linux: 1.1 (no Image support) with Mesa 3D)

colspan=4 | 2.0 (Adrenalin driver on Win7+)(on Linux: 1.1 (no Image support) with Mesa 3D, 2.0 with AMD drivers or AMD ROCm)

2.0

colspan=3 | 2.1

-

!  | Vulkan

1.0(Win 7+ or Mesa 17+)

colspan=6 | 1.2 (Adrenalin 20.1, Linux Mesa 3D 20.0)

colspan=2 | 1.3

-

!  | Shader model

5.1

colspan=3 | 5.16.3

colspan=2 | 6.4

6.5

colspan="2" |6.7

-

!  | OpenGL

colspan=9 | 4.6 (on Linux: 4.6 (Mesa 3D 20.0))

-

!  | Direct3D

11 (11_1)12 (11_1)

colspan=3 | 11 (12_0)12 (12_0)

colspan=2 | 11 (12_1)12 (12_1)

11 (12_1)12 (12_2)

-

!  class="table-rh" | /drm/amdgpu

colspan=2

colspan=7

}

{{notelist|refs=

}}

Software ecosystem

Machine learning

Various deep learning frameworks have a ROCm backend:

PyTorch

TensorFlow

ONNX

MXNet

CuPy

MIOpen

Caffe

Iree (which uses LLVM Multi-Level Intermediate Representation (MLIR))

llama.cpp

Supercomputing

ROCm is gaining significant traction in the top 500.

ROCm is used with the Exascale supercomputers El Capitan and Frontier.

Some related software is to be found at AMD Infinity hub.

Other acceleration & graphics interoperation

As of version 3.0, Blender can now use HIP compute kernels for its renderer cycles.

Other Languages

Julia

Julia has the AMDGPU.jl package, which integrates with LLVM and selects components of the ROCm stack. Instead of compiling code through HIP, AMDGPU.jl uses Julia's compiler to generate LLVM IR directly, which is later consumed by LLVM to generate native device code. AMDGPU.jl uses ROCr's HSA implementation to upload native code onto the device and execute it, similar to how HIP loads its own generated device code.

AMDGPU.jl also supports integration with ROCm's rocBLAS (for BLAS), rocRAND (for random number generation), and rocFFT (for FFTs). Future integration with rocALUTION, rocSOLVER, MIOpen, and certain other ROCm libraries is planned.

Software distribution

Official

Installation instructions are provided for Linux and Windows in the official AMD ROCm documentation. ROCm software is currently spread across several public GitHub repositories. Within the main public meta-repository, there is an XML manifest for each official release: using git-repo, a version control tool built on top of Git, is the recommended way to synchronize with the stack locally.

AMD starts distributing containerized applications for ROCm, notably scientific research applications gathered under AMD Infinity Hub.

AMD distributes itself packages tailored to various Linux distributions.

Third-party

There is a growing third-party ecosystem packaging ROCm.

Linux distributions are officially packaging (natively) ROCm, with various degrees of advancement: Arch Linux, Gentoo, Debian, Fedora

, GNU Guix, and NixOS.

There are Spack packages.

Components

There is one kernel-space component, ROCk, and the rest - there is roughly a hundred components in the stack - is made of user-space modules.

The unofficial typographic policy is to use: uppercase ROC lowercase following for low-level libraries, i.e. ROCt, and the contrary for user-facing libraries, i.e. rocBLAS.

AMD is active developing with the LLVM community, but upstreaming is not instantaneous, and as of January 2022, is still lagging. AMD still officially packages various LLVM forks It is different from the ROC Common Language Runtime.

ROCm  CompilerSupport

ROCm code object manager is in charge of interacting with LLVM intermediate representation.

Mid-level

ROCclr Common Language Runtime

The common language runtime is an indirection layer adapting calls to ROCr on Linux and PAL on windows.

It used to be able to route between different compilers, like the HSAIL-compiler. It is now being absorbed by the upper indirection layers (HIP and OpenCL).

OpenCL

ROCm ships its installable client driver (ICD) loader and an OpenCL implementation bundled together.

As of January 2022, ROCm 4.5.2 ships OpenCL 2.2, and is lagging behind competition.

HIP  Heterogeneous Interface for Portability

The AMD implementation for its GPUs is called HIPAMD. There is also a CPU implementation mostly for demonstration purposes.

HIPCC

HIP builds a `HIPCC` compiler that either wraps Clang and compiles with LLVM open AMDGPU backend, or redirects to the NVIDIA compiler.

HIPIFY

HIPIFY is a source-to-source compiling tool. It translates CUDA to HIP and reverse, either using a Clang-based tool, or a sed-like Perl script.

GPUFORT

Like HIPIFY, GPUFORT is a tool compiling source code into other third-generation-language sources, allowing users to migrate from CUDA Fortran to HIP Fortran. It is also in the repertoire of research projects, even more so.

High-level

ROCm high-level libraries are usually consumed directly by application software, such as machine learning frameworks. Most of the following libraries are in the General Matrix Multiply (GEMM) category, which GPU architecture excels at.

The majority of these user-facing libraries comes in dual-form: hip for the indirection layer that can route to Nvidia hardware, and roc for the AMD implementation.

rocBLAS / hipBLAS

rocBLAS and hipBLAS are central in high-level libraries, it is the AMD implementation for Basic Linear Algebra Subprograms.

It uses the library Tensile privately.

rocSOLVER / hipSOLVER

This pair of libraries constitutes the LAPACK implementation for ROCm and is strongly coupled to rocBLAS.

Utilities

ROCm developer tools: Debug, tracer, profiler, System Management Interface, Validation suite, Cluster management.

GPUOpen tools: GPU analyzer, memory visualizer...

External tools: radeontop (TUI overview)

Comparison with competitors

ROCm competes with other GPU computing stacks: Nvidia CUDA and Intel OneAPI.

Nvidia CUDA

Nvidia's CUDA is closed-source, whereas AMD ROCm is open source. There is open-source software built on top of the closed-source CUDA, for instance RAPIDS.

CUDA is able to run on consumer GPUs, whereas ROCm support is mostly offered for professional hardware such as  AMD Instinct and AMD Radeon Pro.

Nvidia provides a C/C++-centered frontend and its Parallel Thread Execution (PTX) LLVM GPU backend as the Nvidia CUDA Compiler (NVCC).

Intel OneAPI

All the oneAPI corresponding libraries are published on its GitHub Page.

Unified Acceleration Foundation (UXL)

Unified Acceleration Foundation (UXL) is a new technology consortium that are working on the continuation of the OneAPI initiative, with the goal to create a new open standard accelerator software ecosystem, related open standards and specification projects through Working Groups and Special Interest Groups (SIGs). The goal will compete with Nvidia's CUDA. The main companies behind it are Intel, Google, Arm, Qualcomm, Samsung, Imagination, and VMware.

See also

AMD Software – a general overview of AMD's drivers, APIs, and development endeavors.

GPUOpen – AMD's complementary graphics stack

AMD Radeon Software – AMD's software distribution channel

References

External links

— Docker containers for scientific applications.

Category:AMD software

Category:Application programming interfaces

Category:Concurrent computing

Category:GPGPU

Category:GPGPU libraries

Category:Graphics cards

Category:Graphics hardware

Category:Heterogeneous computing

Category:Machine learning

Category:Parallel computing

Category:Supercomputers