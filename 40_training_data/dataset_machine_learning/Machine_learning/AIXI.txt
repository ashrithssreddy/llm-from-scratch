AIXI  is a theoretical mathematical formalism for artificial general intelligence.

It combines Solomonoff induction with sequential decision theory.

AIXI was first proposed by Marcus Hutter in 2000 and several results regarding AIXI are proved in Hutter's 2005 book Universal Artificial Intelligence.

AIXI is a reinforcement learning (RL) agent. It maximizes the expected total rewards received from the environment. Intuitively, it simultaneously considers every computable hypothesis (or environment). In each time step, it looks at every possible program and evaluates how many rewards that program generates depending on the next action taken. The promised rewards are then weighted by the subjective belief that this program constitutes the true environment. This belief is computed from the length of the program: longer programs are considered less likely, in line with Occam's razor. AIXI then selects the action that has the highest expected total reward in the weighted sum of all these programs.

Etymology

According to Hutter, the word "AIXI" can have several interpretations. AIXI can stand for AI based on Solomonoff's distribution, denoted by \xi (which is the Greek letter xi), or e.g. it can stand for AI "crossed" (X) with induction (I). There are other interpretations.

Definition

AIXI is a reinforcement learning agent that interacts with some stochastic and unknown but computable environment \mu. The interaction proceeds in time steps, from t=1 to t=m, where m \in \mathbb{N} is the lifespan of the AIXI agent. At time step t, the agent chooses an action a_t \in \mathcal{A} (e.g. a limb movement) and executes it in the environment, and the environment responds with a "percept" e_t \in \mathcal{E} = \mathcal{O} \times \mathbb{R}, which consists of an "observation" o_t \in \mathcal{O} (e.g., a camera image) and a reward r_t \in \mathbb{R}, distributed according to the conditional probability \mu(o_t r_t | a_1 o_1 r_1 ... a_{t-1} o_{t-1} r_{t-1} a_t), where a_1 o_1 r_1 ... a_{t-1} o_{t-1} r_{t-1} a_t is the "history" of actions, observations and rewards. The environment \mu is thus mathematically represented as a probability distribution over "percepts" (observations and rewards) which depend on the full history, so there is no Markov assumption (as opposed to other RL algorithms). Note again that this probability distribution is unknown to the AIXI agent. Furthermore, note again that \mu is computable, that is, the observations and rewards received by the agent from the environment \mu can be computed by some program (which runs on a Turing machine), given the past actions of the AIXI agent.

The only goal of the AIXI agent is to maximize \sum_{t=1}^m r_t, that is, the sum of rewards from time step 1 to m.

The AIXI agent is associated with a stochastic policy \pi : (\mathcal{A} \times \mathcal{E})^* \rightarrow \mathcal{A}, which is the function it uses to choose actions at every time step, where \mathcal{A} is the space of all possible actions that AIXI can take and \mathcal{E} is the space of all possible "percepts" that can be produced by the environment. The environment (or probability distribution) \mu can also be thought of as a stochastic policy (which is a function): \mu  : (\mathcal{A} \times \mathcal{E})^* \times \mathcal{A} \rightarrow \mathcal{E} , where the * is the Kleene star operation.

In general, at time step t (which ranges from 1 to m), AIXI, having previously executed actions a_1\dots a_{t-1} (which is often abbreviated in the literature as a_{) and having observed the history of percepts o_1 r_1 ... o_{t-1} r_{t-1} (which can be abbreviated as e_{), chooses and executes in the environment the action, a_t, defined as follows:

However, AIXI does have limitations. It is restricted to maximizing rewards based on percepts as opposed to external states. It also assumes it interacts with the environment solely through action and percept channels, preventing it from considering the possibility of being damaged or modified. Colloquially, this means that it doesn't consider itself to be contained by the environment it interacts with. It also assumes the environment is computable.

Computational aspects

Like Solomonoff induction, AIXI is incomputable. However, there are computable approximations of it. One such approximation is AIXItl, which performs at least as well as the provably best time t and space l limited agent.

See also

Gödel machine

References

"Universal Algorithmic Intelligence: A mathematical top->down approach", Marcus Hutter, ; also in Artificial General Intelligence, eds. B. Goertzel and C. Pennachin, Springer, 2007, , pp.&nbsp;227–290, .

Category:Optimal decisions

Category:Decision theory

Category:Machine learning