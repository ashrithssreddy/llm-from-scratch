algorithms can only learn very simple decision boundaries (top panel). Manifold learning can draw a decision boundary between the natural classes of the unlabeled data, under the assumption that close-together points probably belong to the same class, and so the decision boundary should avoid areas with many unlabeled points. This is one version of semi-supervised learning.]]

In machine learning, manifold regularization is a technique for using the shape of a dataset to constrain the functions that should be learned on that dataset. In many machine learning problems, the data to be learned do not cover the entire input space. For example, a facial recognition system may not need to classify any possible image, but only the subset of images that contain faces. The technique of manifold learning assumes that the relevant subset of data comes from a manifold, a mathematical structure with useful properties. The technique also assumes that the function to be learned is smooth: data with different labels are not likely to be close together, and so the labeling function should not change quickly in areas where there are likely to be many data points. Because of this assumption, a manifold regularization algorithm can use unlabeled data to inform where the learned function is allowed to change quickly and where it is not, using an extension of the technique of Tikhonov regularization. Manifold regularization algorithms can extend supervised learning algorithms in semi-supervised learning and transductive learning settings, where unlabeled data are available. The technique has been used for applications including medical imaging, geographical imaging, and object recognition.

Manifold regularizer

Motivation

Manifold regularization is a type of regularization, a family of techniques that reduces overfitting and ensures that a problem is well-posed by penalizing complex solutions. In particular, manifold regularization extends the technique of Tikhonov regularization as applied to Reproducing kernel Hilbert spaces (RKHSs). Under standard Tikhonov regularization on RKHSs, a learning algorithm attempts to learn a function f from among a hypothesis space of functions \mathcal{H}. The hypothesis space is an RKHS, meaning that it is associated with a kernel K, and so every candidate function f has a norm \left\| f \right\|_K, which represents the complexity of the candidate function in the hypothesis space. When the algorithm considers a candidate function, it takes its norm into account in order to penalize complex functions.

Formally, given a set of labeled training data (x_1, y_1), \ldots, (x_{\ell}, y_{\ell}) with x_i \in X, y_i \in Y and a loss function V, a learning algorithm using Tikhonov regularization will attempt to solve the expression

\underset{f \in \mathcal{H}}{\arg\!\min} \frac{1}{\ell} \sum_{i=1}^{\ell} V(f(x_i), y_i) + \gamma \left\| f \right\|_K^2

where \gamma is a hyperparameter that controls how much the algorithm will prefer simpler functions over functions that fit the data better.

embedded in three-dimensional space (left). Manifold regularization attempts to learn a function that is smooth on the unrolled manifold (right).]]

Manifold regularization adds a second regularization term, the intrinsic regularizer, to the ambient regularizer used in standard Tikhonov regularization. Under the manifold assumption in machine learning, the data in question do not come from the entire input space X, but instead from a nonlinear manifold M\subset X. The geometry of this manifold, the intrinsic space, is used to determine the regularization norm.

Laplacian norm

There are many possible choices for the intrinsic regularizer \left\| f \right\|_I. Many natural choices involve the gradient on the manifold  \nabla_{M} , which can provide a measure of how smooth a target function is. A smooth function should change slowly where the input data are dense; that is, the gradient  \nabla_{M} f(x)  should be small where the marginal probability density \mathcal{P}_X(x) , the probability density of a randomly drawn data point appearing at x, is large. This gives one appropriate choice for the intrinsic regularizer:

\left\| f \right\|_I^2 = \int_{x \in M} \left\| \nabla_{M} f(x) \right\|^2 \, d \mathcal{P}_X(x)

In practice, this norm cannot be computed directly because the marginal distribution \mathcal{P}_X is unknown, but it can be estimated from the provided data.

Graph-based approach of the Laplacian norm

When the distances between input points are interpreted as a graph, then the Laplacian matrix of the graph can help to estimate the marginal distribution. Suppose that the input data include \ell labeled examples (pairs of an input x and a label y) and u unlabeled examples (inputs without associated labels). Define W to be a matrix of edge weights for a graph, where W_{ij} is a similarity built from distance measure between the data points x_i and x_j (so that more close implies higher W_{ij}). Define D to be a diagonal matrix with D_{ii} = \sum_{j=1}^{\ell + u} W_{ij} and L to be the Laplacian matrix D-W. Then, as the number of data points \ell + u increases, L converges to the Laplace–Beltrami operator \Delta_{M}, which is the divergence of the gradient \nabla_M. Then, if \mathbf{f} is a vector of the values of f at the data, \mathbf{f} = [f(x_1), \ldots, f(x_{l+u})]^{\mathrm{T}}, the intrinsic norm can be estimated:

\left\| f \right\|_I^2 = \frac{1}{(\ell+u)^2} \mathbf{f}^{\mathrm{T}} L \mathbf{f}

As the number of data points \ell + u increases, this empirical definition of  \left\| f \right\|_I^2 converges to the definition when \mathcal{P}_X is known.

This second approach to the Laplacian norm is to put in relation with meshfree methods, that contrast with the finite difference method in PDE.

Applications

Manifold regularization can extend a variety of algorithms that can be expressed using Tikhonov regularization, by choosing an appropriate loss function V and hypothesis space \mathcal{H}. Two commonly used examples are the families of support vector machines and regularized least squares algorithms. (Regularized least squares includes the ridge regression algorithm; the related algorithms of LASSO and elastic net regularization can be expressed as support vector machines.) The extended versions of these algorithms are called Laplacian Regularized Least Squares (abbreviated LapRLS) and Laplacian Support Vector Machines (LapSVM), respectively.

medical imaging,

object detection,

spectroscopy,

document classification,

drug-protein interactions,

and compressing images and videos.

Laplacian Support Vector Machines (LapSVM)

Support vector machines (SVMs) are a family of algorithms often used for classifying data into two or more groups, or classes. Intuitively, an SVM draws a boundary between classes so that the closest labeled examples to the boundary are as far away as possible. This can be directly expressed as a linear program, but it is also equivalent to Tikhonov regularization with the hinge loss function, V(f(x), y) = \max(0, 1 - yf(x)):

f^* = \underset{f \in \mathcal{H}}{\arg\!\min} \frac{1}{\ell} \sum_{i=1}^{\ell} \max(0, 1 - y_if(x_i)) + \gamma \left\| f \right\|_K^2

Adding the intrinsic regularization term to this expression gives the LapSVM problem statement:

f^* = \underset{f \in \mathcal{H}}{\arg\!\min} \frac{1}{\ell} \sum_{i=1}^{\ell} \max(0, 1 - y_if(x_i)) + \gamma_A \left\| f \right\|_K^2 + \frac{\gamma_I}{(\ell+u)^2} \mathbf{f}^{\mathrm{T}} L \mathbf{f}

Again, the representer theorem allows the solution to be expressed in terms of the kernel evaluated at the data points:

f^*(x) = \sum_{i=1}^{\ell + u} \alpha_i^* K(x_i, x)

\alpha can be found by writing the problem as a linear program and solving the dual problem. Again letting K be the kernel matrix and J be the block matrix \begin{bmatrix} I_{\ell} & 0 \\ 0 & 0_u \end{bmatrix} , the solution can be shown to be

\alpha = \left( 2 \gamma_A I + 2 \frac{\gamma_I}{(\ell + u)^2} L K \right)^{-1} J^{\mathrm{T}} Y \beta^*

where \beta^* is the solution to the dual problem

\begin{align}

& & \beta^* = \max_{\beta \in \mathbf{R}^{\ell}} & \sum_{i=1}^{\ell} \beta_i - \frac{1}{2} \beta^{\mathrm{T}} Q \beta \\

& \text{subject to} && \sum_{i=1}^{\ell} \beta_i y_i = 0 \\

& && 0 \le \beta_i \le \frac{1}{\ell}\; i = 1, \ldots, \ell

\end{align}

and Q is defined by

Q = YJK \left( 2 \gamma_A I + 2 \frac{\gamma_I}{(\ell + u)^2} L K \right)^{-1} J^{\mathrm{T}} Y

medical imaging,

face recognition,

machine maintenance,

and brain–computer interfaces.

Limitations

Manifold regularization assumes that data with different labels are not likely to be close together. This assumption is what allows the technique to draw information from unlabeled data, but it only applies to some problem domains. Depending on the structure of the data, it may be necessary to use a different semi-supervised or transductive learning algorithm.

In some datasets, the intrinsic norm of a function \left\| f \right\|_I can be very close to the ambient norm \left\| f \right\|_K: for example, if the data consist of two classes that lie on perpendicular lines, the intrinsic norm will be equal to the ambient norm. In this case, unlabeled data have no effect on the solution learned by manifold regularization, even if the data fit the algorithm's assumption that the separator should be smooth. Approaches related to co-training have been proposed to address this limitation.

If there are a very large number of unlabeled examples, the kernel matrix K becomes very large, and a manifold regularization algorithm may become prohibitively slow to compute. Online algorithms and sparse approximations of the manifold may help in this case.

See also

Manifold learning

Manifold hypothesis

Semi-supervised learning

Transduction (machine learning)

Spectral graph theory

Reproducing kernel Hilbert space

Tikhonov regularization

Differential geometry

References

External links

Software

The ManifoldLearn library and the Primal LapSVM library implement LapRLS and LapSVM in MATLAB.

The Dlib library for C++ includes a linear manifold regularization function.

Category:Machine learning