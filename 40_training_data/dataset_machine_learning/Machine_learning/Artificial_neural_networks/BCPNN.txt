A Bayesian Confidence Propagation Neural Network (BCPNN) is an artificial neural network inspired by Bayes' theorem, which regards neural computation and processing as probabilistic inference. Neural unit activations represent probability ("confidence") in the presence of input features or categories, synaptic weights are based on estimated correlations and the spread of activation corresponds to calculating posterior probabilities. It was originally proposed by Anders Lansner and Örjan Ekeberg at KTH Royal Institute of Technology. This probabilistic neural network model can also be run in generative mode to produce spontaneous activations and temporal sequences.

The basic model is a feedforward neural network comprising neural units with continuous activation, having a bias representing prior, and being connected by Bayesian weights in the form of point-wise mutual information. The original network has been extended to a modular structure of minicolumns and hypercolumns, representing discrete coded features or attributes. The units can also be connected as a recurrent neural network (losing the strict interpretation of their activations as probabilities) but becoming a possible abstract model of biological neural networks and associative memory.

BCPNN has been used for machine learning classification and data mining, for example for discovery of adverse drug reactions.  The BCPNN learning rule has also been used to model biological synaptic plasticity and intrinsic excitability in large-scale spiking neural network (SNN) models of cortical associative memory and reward learning in Basal ganglia.

Network architecture

The BCPNN network architecture is modular in terms of hypercolumns and minicolumns. This modular structure is inspired by and generalized from the modular structure of the mammalian cortex. In abstract models, the minicolumns serve as the smallest units and they typically feature a membrane time constant and adaptation. In spiking models of cortex, a layer 2/3 minicolumn is typically represented by some 30 pyramidal cells and one double bouquet cell. The latter turns the negative BCPNN-weights formed between neurons with anti-correlated activity into di-synaptic inhibition.

Lateral inhibition within the hypercolumn makes it a soft winner-take-all module. Looking at real cortex, the number of minicolumns within a hypercolumn is on the order of a hundred, which makes the activity sparse, at the level of 1% or less, given that hypercolumns can also be silent. A BCPNN network with a size of the human neocortex would have a couple of million hypercolumns, partitioned into some hundred areas. In addition to sparse activity, a large-scale BCPNN would also have very sparse connectivity, given that the real cortex is sparsely connected at the level of 0.01 - 0.001% on average.

Bayesian-Hebbian learning rule

The BCPNN learning rule was derived from Bayes rule and is Hebbian such that neural units with activity correlated over time get excitatory connections between them whereas anti-correlation generates inhibition and lack of correlation gives zero connections. The independence assumptions are the same as in naïve Bayes formalism. BCPNN represents a straight-forward way of deriving a neural network from Bayes rule. Cortical oscillations in the range from theta, over alpha and beta to gamma are generated by this model. The embedded memories can be recalled from partial input and when activated they show signs of fixpoint attractor dynamics, though neural adaptation and synaptic depression terminates activity within some hundred milliseconds. Notably, a few cycles of gamma oscillations are generated during such a brief memory recall. Cognitive phenomena like attentional blink and its modulation by benzodiazepine has also been replicated in this model.

In recent years, Hebbian plasticity has been incorporated into this cortex model and simulated with abstract non-spiking as well as spiking neural units. This made it possible to demonstrate online learning of temporal sequences as well as one-shot encoding and immediate recall in human word list learning.

Machine learning applications

The point-wise mutual information weights of BCPNN is since long one of the standard methods for detection of drug adverse reactions. The performance is notably slightly lower than that of the best methods that employ end-to-end error back-propagation. However, the extreme performance comes with a cost of lower biological plausibility and higher complexity of the learning machinery. The BCPNN method is also quite well suited for semi-supervised learning.

Hardware designs for BCPNN

The structure of BCPNN with its cortex-like modular architecture and massively parallel correlation based Hebbian learning makes it quite hardware friendly. Implementation with reduced number of bits in synaptic state variables have been shown to be feasible. BCPNN has further been the target for parallel simulators on cluster computers and GPU:s. It was recently implemented on the SpiNNaker  compute platform as well as in a series of dedicated neuromorphic VLSI designs. From these it has been estimated that a human cortex sized BCPNN with continuous learning could be executed in real time with a power dissipation on the order of few kW.

References

Category:Artificial neural networks