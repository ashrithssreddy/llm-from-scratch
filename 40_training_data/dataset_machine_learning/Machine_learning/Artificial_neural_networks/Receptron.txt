The receptron (short for "reservoir perceptron") is a neuromorphic data processing model — specifically neuromorphic computing — that generalizes the traditional  perceptron, by incorporating non-linear interactions between inputs. Unlike classical perceptron, which rely on linearly independent weights, the receptron leverages complexity in physical substrates, such as the electric conduction properties of nanostructured materials or optical speckle fields, to perform classification tasks. The receptron bridges unconventional computing and neural network principles, enabling solutions that do not require the training approaches typical of artificial neural networks based on the perceptron model.

Algorithm

The receptron is an algorithm for supervised learning of binary classifiers, so a classification algorithm that makes its predictions based on a predictor function, combining a set of weights with the feature vector. The mathematical model is based on the sum of inputs with non-linear interactions:

S = \sum_{k=1}^n x_j \widetilde{w}_j (\vec{x}) | S \in R      (1)

where j \in [1, n] and \widetilde{w}_j  are non-linear weight functions depending on the inputs, \vec{x}. Nonlinearity will typically make the system extremely complex, and allowing for the solution of problems not solvable through the simpler rules of a linear system, such as the perceptron or McCulloch Pitts neurons, which is based on the sum of linearly independent weights:

S = \sum_{k=1}^n x_j w_j^p      (2)

where w_jare constant real values. A consequence of this simplicity is the limitation to linearly separable functions, which necessitates multi-layer architectures and training algorithms like backpropagation

As in the perceptron case, the summation in Eq. 1 origins the activation of the receptron output through the thresholding process,

Y(x_1, ..., x_n) =  \begin{cases} 1 & \text{if } S > \text{th} \\ 0 & \text{if } S \leq \text{th} \end{cases}    (3)

where th is a constant threshold parameter. Equation 3 can be written by using the  Heaviside step function.

The weight functions \widetilde{w} (\vec{x}) can be written with a finite number of parameters w_{j_1...j_n}, simplifying the model representation. One can Taylor-expand  \widetilde{w} (\vec{x}) and use the idempotency of Boolean variables (x_j)^q = x_j \forall q \geq 1 such that S' = b + \sum_{k=1}^n x_j \widetilde{w}_j (\vec{x}) can be written as

S'(\vec{x}) = b + \sum_{j} w_j x_j + \sum_{j    (4)

where w_{j_1...j_n} are independent parameters that can be seen as the components of a tensor W (“weight tensor”) of rank n and type (n,0).

The sum in Eq.  reduces to the perceptron case when off-diagonal terms of W vanish. If one considers n=2, one gets:

S'(\vec{x}) = b + x_1 w_{11} + x_2 w_{22} + x_1 x_2 w_{12}        (5)

in the perceptron case, the vanishing of w_{12} implies linearity S(1,1)=S(0,1) + S(1,0). In the receptron case S(1,1) \neq S(0,1) + S(1,0), meaning that the superposition principle is no longer valid, the latter terms being responsible of the more complex non-linear interaction between the inputs.

Design and implementations

1. Electrical Receptron

Substrate: Nanostructured and nanocomposite films (Au, Pt, Zr Au/Zr). These films form disordered networks of nanoparticles with resistive switching and non-linear electrical conduction.

2. Optical Receptron

Substrate: Optical speckle fields generated by random interference of light emerging from a disordered medium illuminated by a laser or coherent radiation.

Key features

Physical Substrate Computing: The receptron does not require digital training; instead, it exploits the natural complexity of materials (e.g., nanowire networks, diffractive media) to perform computations.

Non-Linear Separability: Unlike traditional perceptrons, which fail on problems like the XOR function, the receptron can solve such tasks due to its inherent non-linearity.

Training-Free Operation: Classification is achieved through the physical system's response rather than iterative weight adjustments, reducing computational overhead.

References

Category:Artificial neural networks