{{Infobox software

name = Latent Diffusion Model

logo =

screenshot =

author = CompVis

developer =

released = December 20, 2021

latest release =

genre =

license = MIT

repo =

programming language = Python

operating system =

website =

}}

The Latent Diffusion Model (LDM) is a diffusion model architecture developed by the CompVis (Computer Vision & Learning) group at LMU Munich.

Introduced in 2015, diffusion models (DMs) are trained with the objective of removing successive applications of noise (commonly Gaussian) on training images. The LDM is an improvement on standard DM by performing diffusion modeling in a latent space, and by allowing self-attention and cross-attention conditioning.

LDMs are widely used in practical diffusion models. For instance, Stable Diffusion versions 1.1 to 2.1 were based on the LDM architecture.

Version history

Diffusion models were introduced in 2015 as a method to learn a model that can sample from a highly complex probability distribution. They used techniques from non-equilibrium thermodynamics, especially diffusion. It was accompanied by a software implementation in Theano.

A 2019 paper proposed the noise conditional score network (NCSN) or score-matching with Langevin dynamics (SMLD). The paper was accompanied by a software package written in PyTorch release on GitHub.

A 2020 paper proposed the Denoising Diffusion Probabilistic Model (DDPM), which improves upon the previous method  by variational inference. The paper was accompanied by a software package written in TensorFlow release on GitHub. It was reimplemented in PyTorch by lucidrains.

On December 20, 2021, the LDM paper was published on arXiv, and both Stable Diffusion and LDM repositories were published on GitHub. However, they remained roughly the same. Substantial information concerning Stable Diffusion v1 was only added to GitHub on August 10, 2022.

All of Stable Diffusion (SD) versions 1.1 to XL were particular instantiations of the LDM architecture.

SD 1.1 to 1.4 were released by CompVis in August 2022. There is no "version 1.0". SD 1.1 was a LDM trained on the laion2B-en dataset. SD 1.1 was finetuned to 1.2 on more aesthetic images. SD 1.2 was finetuned to 1.3, 1.4 and 1.5, with 10% of text-conditioning dropped, to improve classifier-free guidance.

Architecture

While the LDM can work for generating arbitrary data conditional on arbitrary data, for concreteness, we describe its operation in conditional text-to-image generation.

LDM consists of a variational autoencoder (VAE), a modified U-Net, and a text encoder.

The VAE encoder compresses the image from pixel space to a smaller dimensional latent space, capturing a more fundamental semantic meaning of the image. Gaussian noise is iteratively applied to the compressed latent representation during forward diffusion. The U-Net block, composed of a ResNet backbone, denoises the output from forward diffusion backwards to obtain a latent representation. Finally, the VAE decoder generates the final image by converting the representation back into pixel space.

In the implemented version,

In pseudocode,

def ResBlock(x, time, residual_channels):

x_in = x

time_embedding = feedforward_network(time)

x = concatenate(x, residual_channels)

x = conv_layer_1(activate(normalize_1(x))) + time_embedding

x = conv_layer_2(dropout(activate(normalize_2(x))))

return x_in + x

def SpatialTransformer(x, cond):

x_in = x

x = normalize(x)

x = proj_in(x)

x = cross_attention(x, cond)

x = proj_out(x)

return x_in + x

def unet(x, time, cond):

residual_channels = []

for resblock, spatialtransformer in downscaling_layers:

x = resblock(x, time)

residual_channels.append(x)

x = spatialtransformer(x, cond)

x = middle_layer.resblock_1(x, time)

x = middle_layer.spatialtransformer(x, time)

x = middle_layer.resblock_2(x, time)

for resblock, spatialtransformer in upscaling_layers:

residual = residual_channels.pop()

x = resblock(concatenate(x, residual), time)

x = spatialtransformer(x, cond)

return x

The detailed architecture may be found in.

Training and inference

The LDM is trained by using a Markov chain to gradually add noise to the training images. The model is then trained to reverse this process, starting with a noisy image and gradually removing the noise until it recovers the original image.

More specifically, the training process can be described as follows:

Forward diffusion process: Given a real image x_0, a sequence of latent variables x_{1:T} are generated by gradually adding Gaussian noise to the image, according to a pre-determined "noise schedule".

Reverse diffusion process: Starting from a Gaussian noise sample x_T, the model learns to predict the noise added at each step, in order to reverse the diffusion process and obtain a reconstruction of the original image x_0.

The model is trained to minimize the difference between the predicted noise and the actual noise added at each step. This is typically done using a mean squared error (MSE) loss function.

Once the model is trained, it can be used to generate new images by simply running the reverse diffusion process starting from a random noise sample. The model gradually removes the noise from the sample, guided by the learned noise distribution, until it generates a final image.

See the diffusion model page for details.

See also

Diffusion model

Generative adversarial network

Variational autoencoder

Stable Diffusion

References

Further reading

Category:Deep learning

Category:Generative artificial intelligence

Category:Image processing

Category:Artificial intelligence art

Category:Text-to-image generation

Category:Unsupervised learning

Category:2021 software