{{Infobox software

name = DeepSpeed

logo = DeepSpeed logo.svg

screenshot =

screenshot size =

caption =

author = Microsoft Research

developer = Microsoft

released =

latest release version = v0.16.5

latest release date =

repo =

programming language = Python, CUDA, C++

operating system =

genre = Software library

license = Apache License 2.0

website =

}}

DeepSpeed is an open source deep learning optimization library for PyTorch.

Library

The library is designed to reduce computing power and memory use and to train large distributed models with better parallelism on existing computer hardware. DeepSpeed is optimized for low latency, high throughput training. It includes the Zero Redundancy Optimizer (ZeRO) for training models with 1 trillion or more parameters. Features include mixed precision training, single-GPU, multi-GPU, and multi-node training as well as custom model parallelism. The DeepSpeed source code is licensed under MIT License and available on GitHub.

The team claimed to achieve up to a 6.2x throughput improvement, 2.8x faster convergence, and 4.6x less communication.

See also

Comparison of deep learning software

Deep learning

Machine learning

TensorFlow

References

Further reading

External links

AI at Scale - Microsoft Research

GitHub - microsoft/DeepSpeed

ZeRO & DeepSpeed: New system optimizations enable training models with over 100 billion parameters - Microsoft Research

Category:C++ libraries

Category:Python (programming language) scientific libraries

Category:Python (programming language) libraries

Category:Free and open-source software

Category:Microsoft development tools

Category:Microsoft free software

Category:Microsoft Research

Category:Software using the MIT license

Category:2020 software

Category:Deep learning software

Category:Software using the Apache license