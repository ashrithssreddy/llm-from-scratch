Long short-term memory (LSTM) and a forget gate. data processing, time series analysis tasks,  machine translation, speech activity detection, robot control, video games, healthcare.

Motivation

In theory, classic RNNs can keep track of arbitrary long-term dependencies in the input sequences. The problem with classic RNNs is computational (or practical) in nature: when training a classic RNN using back-propagation, the long-term gradients which are back-propagated can "vanish", meaning they can tend to zero due to very small numbers creeping into the computations, causing the model to effectively stop learning. RNNs using LSTM units partially solve the vanishing gradient problem, because LSTM units allow gradients to also flow with little to no attenuation. However, LSTM networks can still suffer from the exploding gradient problem.

The intuition behind the LSTM architecture is to create an additional module in a neural network that learns when to remember and when to forget pertinent information. An LSTM might process the sentence "Dave, as a result of his controversial claims, is now a pariah" by remembering the (statistically likely) grammatical gender and number of the subject Dave. Note that this information is pertinent for the pronoun his and note that this information is no longer important after the verb is.

Variants

In the equations below, the lowercase variables represent vectors. Matrices W_q and U_q contain, respectively, the weights of the input and recurrent connections, where the subscript _q can either be the input gate i, output gate o, the forget gate f or the memory cell c, depending on the activation being calculated. In this section, we are thus using a "vector notation". So, for example, c_t \in \mathbb{R}^{h} is not just one unit of one LSTM cell, but contains h LSTM cell's units.

See

\begin{align}

f_t &= \sigma_g(W_{f} x_t + U_{f} h_{t-1} + b_f) \\

i_t &= \sigma_g(W_{i} x_t + U_{i} h_{t-1} + b_i) \\

o_t &= \sigma_g(W_{o} x_t + U_{o} h_{t-1} + b_o) \\

\tilde{c}_t &= \sigma_c(W_{c} x_t + U_{c} h_{t-1} + b_c) \\

c_t &= f_t \odot c_{t-1} + i_t \odot \tilde{c}_t \\

h_t &= o_t \odot \sigma_h(c_t)

\end{align}

where the initial values are c_0 = 0 and h_0 = 0 and the operator \odot denotes the Hadamard product (element-wise product). The subscript t indexes the time step.

Variables

Letting the superscripts d and h refer to the number of input features and number of hidden units, respectively:

x_t \in \mathbb{R}^{d}: input vector to the LSTM unit

f_t \in {(0,1)}^{h}: forget gate's activation vector

i_t \in {(0,1)}^{h}: input/update gate's activation vector

o_t \in {(0,1)}^{h}: output gate's activation vector

h_t \in {(-1,1)}^{h}: hidden state vector also known as output vector of the LSTM unit

\tilde{c}_t \in {(-1,1)}^{h}: cell input activation vector

c_t \in \mathbb{R}^{h}: cell state vector

W \in \mathbb{R}^{h \times d}, U \in \mathbb{R}^{h \times h}  and b \in \mathbb{R}^{h}: weight matrices and bias vector parameters which need to be learned during training

Activation functions

\sigma_g: sigmoid function.

\sigma_c: hyperbolic tangent function.

\sigma_h: hyperbolic tangent function or, as the peephole LSTM paper The * denotes the convolution operator.

\begin{align}

f_t &= \sigma_g(W_{f} * x_t + U_{f} * h_{t-1} + V_{f} \odot c_{t-1} + b_f) \\

i_t &= \sigma_g(W_{i} * x_t + U_{i} * h_{t-1} + V_{i} \odot c_{t-1} + b_i) \\

c_t &= f_t \odot c_{t-1} + i_t \odot \sigma_c(W_{c} * x_t + U_{c} * h_{t-1} + b_c) \\

o_t &= \sigma_g(W_{o} * x_t + U_{o} * h_{t-1} + V_{o} \odot c_{t} + b_o) \\

h_t &= o_t \odot \sigma_h(c_t)

\end{align}

Training

An RNN using LSTM units can be trained in a supervised fashion on a set of training sequences, using an optimization algorithm like gradient descent combined with backpropagation through time to compute the gradients needed during the optimization process, in order to change each weight of the LSTM network in proportion to the derivative of the error (at the output layer of the LSTM network) with respect to corresponding weight.

A problem with using gradient descent for standard RNNs is that error gradients vanish exponentially quickly with the size of the time lag between important events. This is due to \lim_{n \to \infty}W^n = 0 if the spectral radius of W is smaller than 1.

However, with LSTM units, when error values are back-propagated from the output layer, the error remains in the LSTM unit's cell. This "error carousel" continuously feeds error back to each of the LSTM unit's gates, until they learn to cut off the value.

CTC score function

Many applications use stacks of LSTM RNNs and train them by connectionist temporal classification (CTC) to find an RNN weight matrix that maximizes the probability of the label sequences in a training set, given the corresponding input sequences. CTC achieves both alignment and recognition.

Alternatives

Sometimes, it can be advantageous to train (parts of) an LSTM by neuroevolution

Time series prediction

Speech recognition

Rhythm learning

Hydrological rainfall–runoff modeling

Music composition

Grammar learning

Handwriting recognition

Human action recognition

Sign language translation

Protein homology detection

Predicting subcellular localization of proteins

Time series anomaly detection

Several prediction tasks in the area of business process management

Prediction in medical care pathways

Semantic parsing

Object co-segmentation

Airport passenger management

Short-term traffic forecast

Drug design

Financial forecasting

Activity classification in video

2015: Google started using an LSTM trained by CTC for speech recognition on Google Voice. According to the official blog post, the new model cut transcription errors by 49%.

2016: Google started using an LSTM to suggest messages in the Allo conversation app. In the same year, Google released the Google Neural Machine Translation system for Google Translate which used LSTMs to reduce translation errors by 60%.

Apple announced in its Worldwide Developers Conference that it would start using the LSTM for quicktype in the iPhone and for Siri.

Amazon released Polly, which generates the voices behind Alexa, using a bidirectional LSTM for the text-to-speech technology.

2017:  Facebook performed some 4.5 billion automatic translations every day using long short-term memory networks.

2018: OpenAI used LSTM trained by policy gradients to beat humans in the complex video game of Dota 2, cited by the LSTM paper. His supervisor, Jürgen Schmidhuber, considered the thesis highly significant.

An early version of LSTM was published in 1995 in a technical report by Sepp Hochreiter and Jürgen Schmidhuber, then published in the NIPS 1996 conference. By introducing Constant Error Carousel (CEC) units, LSTM deals with the vanishing gradient problem. The initial version of LSTM block included cells, input and output gates.

(Felix Gers, Jürgen Schmidhuber, and Fred Cummins, 1999) introduced the forget gate (also called "keep gate") into the LSTM architecture in 1999, enabling the LSTM to reset its own state. published a simplified variant of the forget gate LSTM Concurrently, the ResNet architecture was developed. It is equivalent to an open-gated or gateless highway network.

A modern upgrade of LSTM called xLSTM is published by a team led by Sepp Hochreiter (Maximilian et al, 2024). One of the 2 blocks (mLSTM) of the architecture are parallelizable like the Transformer architecture, the other ones (sLSTM) allow state tracking.

Applications

2001: Gers and Schmidhuber trained LSTM to learn languages unlearnable by traditional models such as Hidden Markov Models.

2004: First successful application of LSTM to speech Alex Graves et al.

2005: Daan Wierstra, Faustino Gomez, and Schmidhuber trained LSTM by neuroevolution without a teacher.

Hochreiter, Heuesel, and Obermayr applied LSTM to protein homology detection the field of biology. One was the most accurate model in the competition and another was the fastest. This was the first time an RNN won international competitions. Their time-aware LSTM (T-LSTM) performs better on certain data sets than standard LSTM.

See also

Attention (machine learning)

Deep learning

Differentiable neural computer

Gated recurrent unit

Highway network

Long-term potentiation

Prefrontal cortex basal ganglia working memory

Recurrent neural network

Seq2seq

Transformer (machine learning model)

Time series

References

Further reading

original with two chapters devoted to explaining recurrent neural networks, especially LSTM.

External links

Recurrent Neural Networks with over 30 LSTM papers by Jürgen Schmidhuber's group at IDSIA

Category:Neural network architectures

Category:Deep learning

Category:1997 in artificial intelligence