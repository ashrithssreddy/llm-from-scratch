{{Infobox software

name = word2vec

logo =

screenshot =

screenshot size =

caption =

author = Google AI

developer =

released =

latest release version =

latest release date =

repo = https://code.google.com/archive/p/word2vec/

programming language =

operating system =

replaces =

replaced_by =

genre =

license = Apache-2.0

website =

}}

Word2vec is a technique in natural language processing for obtaining vector representations of words. These vectors capture information about the meaning of the word based on the surrounding words. The word2vec algorithm estimates these representations by modeling text in a large corpus. Once trained, such a model can detect synonymous words or suggest additional words for a partial sentence. Word2vec was developed by Tomáš Mikolov, Kai Chen, Greg Corrado, Ilya Sutskever and Jeff Dean at Google, and published in 2013. The skip-gram architecture weighs nearby context words more heavily than more distant context words. According to the authors' note, CBOW is faster while skip-gram does a better job for infrequent words.

After the model is trained, the learned word embeddings are positioned in the vector space such that words that share common contexts in the corpus — that is, words that are semantically and syntactically similar — are located close to one another in the space.

A corpus is a sequence of words. Both CBOW and skip-gram are methods to learn one vector per word appearing in the corpus.

Let V ("vocabulary") be the set of all words appearing in the corpus C. Our goal is to learn one vector v_w \in \R^n for each word w \in V.

The idea of skip-gram is that the vector of a word should be close to the vector of each of its neighbors. The idea of CBOW is that the vector-sum of a word's neighbors should be close to the vector of the word.

Continuous bag-of-words (CBOW)

The idea of CBOW is to represent each word with a vector, such that it is possible to predict a word using the sum of the vectors of its neighbors. Specifically, for each word w_i in the corpus, the one-hot encoding of the word is used as the input to the neural network. The output of the neural network is a probability distribution over the dictionary, representing a prediction of individual words in the neighborhood of w_i. The objective of training is to maximize \sum_i\ln \Pr(w_i \mid w_{i + j}\colon j \in N) .

For example, if we want each word in the corpus to be predicted by every other word in a small span of 4 words. The set of relative indexes of neighbor words will be: N = \{-2, -1, +1, +2\} , and the objective is \sum_i\ln \Pr(w_i \mid w_{i-2}, w_{i-1}, w_{i+1}, w_{i+2}) .

In standard bag-of-words, a word's context is represented by a word-count (aka a word histogram) of its neighboring words. For example, the "sat" in "the cat sat on the mat" is represented as {"the": 2, "cat": 1, "on": 1}. Note that the last word "mat" is not used to represent "sat", because it is outside the neighborhood N = \{-2, -1, +1, +2\} .

In continuous bag-of-words, the histogram is multiplied by a matrix V to obtain a continuous representation of the word's context. The matrix V is also called a dictionary. Its columns are the word vectors. It has D columns, where D is the size of the dictionary. Let d be the length of each word vector. We have V \in \R^{d \times D}.

For example, multiplying the word histogram {"the": 2, "cat": 1, "on": 1} with V, we obtain 2 v_{\text{the}} + v_{\text{cat}} + v_{\text{on}}.

This is then multiplied with another matrix V' of shape \R^{D \times d}. Each row of it is a word vector v'. This results in a vector of length D, one entry per dictionary entry. Then, apply the softmax to obtain a probability distribution over the dictionary.

This system can be visualized as a neural network, similar in spirit to an autoencoder, of architecture linear-linear-softmax, as depicted in the diagram. The system is trained by gradient descent to minimize the cross-entropy loss.

In full formula, the cross-entropy loss is:-\sum_i \ln \frac{e^{v_{w_i}' \cdot (\sum_{j \in N} v_{w_{j+i}})}}{\sum_{w'} e^{v_{w'}' \cdot (\sum_{j \in N} v_{w_{j+i}})}}where the outer summation \sum_i is over the words in a corpus, the quantity \sum_{j \in N} v_{w_{j+i}} is the sum of a word's neighbors' vectors, etc.

Once such a system is trained, we have two trained matrices V, V'. Either the column vectors of V or the row vectors of V' can serve as the dictionary. For example, the word "sat" can be represented as either the "sat"-th column of V or the "sat"-th row of V'. It is also possible to simply define V' = V^\top, in which case there would no longer be a choice.

Skip-gram

The idea of skip-gram is to represent each word with a vector, such that it is possible to predict the vectors of its neighbors using the vector of a word.

The architecture is still linear-linear-softmax, the same as CBOW, but the input and the output are switched. Specifically, for each word w_i in the corpus, the one-hot encoding of the word is used as the input to the neural network. The output of the neural network is a probability distribution over the dictionary, representing a prediction of individual words in the neighborhood of w_i. The objective of training is to maximize \sum_i \sum_{j \in N} \ln \Pr(w_{j+i} \mid w_i) .

In full formula, the loss function is-\sum_i \sum_{j \in N} \ln \frac{e^{v_{w_{j+i}}' \cdot v_{w_i}}}{\sum_{w'} e^{v_{w'}' \cdot v_{w_i}}}Same as CBOW, once such a system is trained, we have two trained matrices V, V'. Either the column vectors of V or the row vectors of V' can serve as the dictionary. It is also possible to simply define V' = V^\top, in which case there would no longer be a choice.

Essentially, skip-gram and CBOW are exactly the same in architecture. They only differ in the objective function during training.

History

During the 1980s, there were some early attempts at using neural networks to represent words and concepts as vectors.

In 2010, Tomáš Mikolov (then at Brno University of Technology) with co-authors applied a simple recurrent neural network with a single hidden layer to language modelling.

Word2vec was created, patented, and published in 2013 by a team of researchers led by Mikolov at Google over two papers. Other researchers helped analyse and explain the algorithm.

Embedding vectors created using the Word2vec algorithm have some advantages compared to earlier algorithms Mikolov argued that the comparison was unfair as GloVe was trained on more data, and that the fastText project showed that word2vec is superior when trained on the same data.

Parameterization

Results of word2vec training can be sensitive to parametrization. The following are some important parameters in word2vec training.

Training algorithm

A Word2vec model can be trained with hierarchical softmax and/or negative sampling. To approximate the conditional log-likelihood a model seeks to maximize, the hierarchical softmax method uses a Huffman tree to reduce calculation. The negative sampling method, on the other hand, approaches the maximization problem by minimizing the log-likelihood of sampled negative instances. According to the authors, hierarchical softmax works better for infrequent words while negative sampling works better for frequent words and better with low dimensional vectors.

Sub-sampling

High-frequency and low-frequency words often provide little information. Words with a frequency above a certain threshold, or below a certain threshold, may be subsampled or removed to speed up training.

Dimensionality

Quality of word embedding increases with higher dimensionality. But after reaching some point, marginal gain diminishes. doc2vec has been implemented in the C, Python and Java/Scala tools (see below), with the Java and Python versions also supporting inference of document embeddings on new, unseen documents.

doc2vec estimates the distributed representations of documents much like how word2vec estimates representations of words: doc2vec utilizes either of two model architectures, both of which are allegories to the architectures used in word2vec. The first, Distributed Memory Model of Paragraph Vectors (PV-DM), is identical to CBOW other than it also provides a unique document identifier as a piece of additional context. The second architecture, Distributed Bag of Words version of Paragraph Vector (PV-DBOW), is identical to the skip-gram model except that it attempts to predict the window of surrounding context words from the paragraph identifier instead of the current word. respectively, and various governmental institutions.

top2vec

Another extension of word2vec is top2vec, which leverages both document and word embeddings to estimate distributed representations of topics. top2vec takes document embeddings learned from a doc2vec model and reduces them into a lower dimension (typically using UMAP). The space of documents is then scanned using HDBSCAN, and clusters of similar documents are found. Next, the centroid of documents identified in a cluster is considered to be that cluster's topic vector. Finally, top2vec searches the semantic space for word embeddings located near to the topic vector to ascertain the 'meaning' of the topic. Named bio-vectors (BioVec) to refer to biological sequences in general with protein-vectors (ProtVec) for proteins (amino-acid sequences) and gene-vectors (GeneVec) for gene sequences, this representation can be widely used in applications of machine learning in proteomics and genomics. The results suggest that BioVectors can characterize biological sequences in terms of biochemical and biophysical interpretations of the underlying patterns.

Radiology and intelligent word embeddings (IWE)

An extension of word vectors for creating a dense vector representation of unstructured radiology reports has been proposed by Banerjee et al. One of the biggest challenges with Word2vec is how to handle unknown or out-of-vocabulary (OOV) words and morphologically similar words. If the Word2vec model has not encountered a particular word before, it will be forced to use a random vector, which is generally far from its ideal representation. This can particularly be an issue in domains like medicine where synonyms and related words can be used depending on the preferred style of radiologist, and words may have been used infrequently in a large corpus.

IWE combines Word2vec with a semantic dictionary mapping technique to tackle the major challenges of information extraction from clinical texts, which include ambiguity of free text narrative style, lexical variations, use of ungrammatical and telegraphic phases, arbitrary ordering of words, and frequent appearance of abbreviations and acronyms.  Of particular interest, the IWE model (trained on the one institutional dataset) successfully translated to a different institutional dataset which demonstrates good generalizability of the approach across institutions.

Analysis

The reasons for successful word embedding learning in the word2vec framework are poorly understood. Goldberg and Levy point out that the word2vec objective function causes words that occur in similar contexts to have similar embeddings (as measured by cosine similarity) and note that this is in line with J. R. Firth's distributional hypothesis. However, they note that this explanation is "very hand-wavy" and argue that a more formal explanation would be preferable. show that much of the superior performance of word2vec or similar embeddings in downstream tasks is not a result of the models per se, but of the choice of specific hyperparameters. Transferring these hyperparameters to more 'traditional' approaches yields similar performances in downstream tasks. Arora et al. (2016) explain word2vec and related algorithms as performing inference for a simple generative model for text, which involves a random walk generation process based upon loglinear topic model. They use this to explain some properties of word embeddings, including their use to solve analogies.

Preservation of semantic and syntactic relationships

The word embedding approach is able to capture multiple different degrees of similarity between words. Mikolov et al. (2013) found that semantic and syntactic patterns can be reproduced using vector arithmetic. Patterns such as "Man is to Woman as Brother is to Sister" can be generated through algebraic operations on the vector representations of these words such that the vector representation of "Brother" - "Man" + "Woman" produces a result which is closest to the vector representation of "Sister" in the model. Such relationships can be generated for a range of semantic relations (such as Country–Capital) as well as syntactic relations (e.g. present tense–past tense).

This facet of word2vec has been exploited in a variety of other contexts. For example, word2vec has been used to map a vector space of words in one language to a vector space constructed from another language. Relationships between translated words in both spaces can be used to assist with machine translation of new words.

Assessing the quality of a model

Mikolov et al. (2013) or develop their own test set which is meaningful to the corpora which make up the model. This approach offers a more challenging test than simply arguing that the words most similar to a given test word are intuitively plausible. They found that Word2vec has a steep learning curve, outperforming another word-embedding technique, latent semantic analysis (LSA), when it is trained with medium to large corpus size (more than 10 million words). However, with a small training corpus, LSA showed better performance. Additionally they show that the best parameter setting depends on the task and the training corpus. Nevertheless, for skip-gram models trained in medium size corpora, with 50 dimensions, a window size of 15 and 10 negative samples seems to be a good parameter setting.

See also

Autoencoder

Document-term matrix

Feature extraction

Feature learning

Vector space model

Thought vector

fastText

GloVe

ELMo

BERT (language model)

Normalized compression distance

References

External links

Wikipedia2Vec([https://wikipedia2vec.github.io/wikipedia2vec/ introduction)

Implementations

C

C#

Python (Spark)

Python (TensorFlow)

Python (Gensim)

Java/Scala

R

Category:Free science software

Category:Natural language processing toolkits

Category:Artificial neural networks

Category:Semantic relations

Category:2013 software

Category:2013 in artificial intelligence