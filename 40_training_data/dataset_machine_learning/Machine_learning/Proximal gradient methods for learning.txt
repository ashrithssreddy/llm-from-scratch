Proximal gradient (forward backward splitting) methods for learning is an area of research in optimization and statistical learning theory which studies algorithms for a general class of convex regularization problems where the regularization penalty may not be differentiable. One such example is \ell_1 regularization (also known as Lasso) of the form

\min_{w\in\mathbb{R}^d} \frac{1}{n}\sum_{i=1}^n (y_i- \langle w,x_i\rangle)^2+ \lambda \|w\|_1, \quad \text{ where } x_i\in \mathbb{R}^d\text{ and } y_i\in\mathbb{R}.

Proximal gradient methods offer a general framework for solving regularization problems from statistical learning theory with penalties that are tailored to a specific problem application. Such customized penalties can help to induce certain structure in problem solutions, such as sparsity (in the case of lasso) or group structure (in the case of  group lasso).

Relevant background

Proximal gradient methods are applicable in a wide variety of scenarios for solving convex optimization problems of the form

\min_{x\in \mathcal{H}} F(x)+R(x),

where F is convex and differentiable with Lipschitz continuous gradient,  R is a convex, lower semicontinuous function which is possibly nondifferentiable, and \mathcal{H} is some set, typically a Hilbert space. The usual criterion of  x minimizes  F(x)+R(x) if and only if  \nabla (F+R)(x) = 0 in the convex, differentiable setting is now replaced by

0\in \partial (F+R)(x),

where \partial \varphi denotes the subdifferential of a real-valued, convex function  \varphi.

Given a convex function \varphi:\mathcal{H} \to \mathbb{R} an important operator to consider is its proximal operator \operatorname{prox}_{\varphi}:\mathcal{H}\to\mathcal{H}  defined by

\operatorname{prox}_{\varphi}(u) = \operatorname{arg}\min_{x\in\mathcal{H}} \varphi(x)+\frac{1}{2}\|u-x\|_2^2,

which is well-defined because of the strict convexity of the  \ell_2 norm. The proximal operator can be seen as a generalization of a projection.

We see that the proximity operator is important because  x^*  is a minimizer to the problem  \min_{x\in\mathcal{H}} F(x)+R(x) if and only if

x^* = \operatorname{prox}_{\gamma R}\left(x^*-\gamma\nabla F(x^*)\right), where \gamma>0 is any positive real number. The Moreau decomposition can be seen to be a generalization of the usual orthogonal decomposition of a vector space, analogous with the fact that proximity operators are generalizations of projections.

Solving for L1 proximity operator

For simplicity we restrict our attention to the problem where \lambda=1. To solve the problem

\min_{w\in\mathbb{R}^d} \frac{1}{n}\sum_{i=1}^n (y_i- \langle w,x_i\rangle)^2+  \|w\|_1,

we consider our objective function in two parts: a convex, differentiable term F(w) = \frac{1}{n}\sum_{i=1}^n (y_i- \langle w,x_i\rangle)^2 and a convex function R(w) = \|w\|_1. Note that R is not strictly convex.

Let us compute the proximity operator for R(w). First we find an alternative characterization of the proximity operator \operatorname{prox}_{R}(x) as follows:

\begin{align}

u = \operatorname{prox}_R(x) \iff & 0\in \partial \left(R(u)+\frac{1}{2}\|u-x\|_2^2\right)\\

\iff & 0\in \partial R(u) + u-x\\

\iff & x-u\in \partial R(u).

\end{align}

For R(w) = \|w\|_1 it is easy to compute \partial R(w): the ith entry of \partial R(w) is precisely

\partial |w_i| = \begin{cases}

1,&w_i>0\\

-1,&w_i

Using the recharacterization of the proximity operator given above, for the choice of R(w) = \|w\|_1 and \gamma>0 we have that \operatorname{prox}_{\gamma R}(x) is defined entrywise by

\left(\operatorname{prox}_{\gamma R}(x)\right)_i = \begin{cases}

x_i-\gamma,&x_i>\gamma\\

0,&|x_i|\leq\gamma\\

x_i+\gamma,&x_i

which is known as the soft thresholding operator S_{\gamma}(x)=\operatorname{prox}_{\gamma \|\cdot\|_1}(x).

Fixed point iterative schemes

To finally solve the lasso problem we consider the fixed point equation shown earlier:

x^* = \operatorname{prox}_{\gamma R}\left(x^*-\gamma\nabla F(x^*)\right).

Given that we have computed the form of the proximity operator explicitly, then we can define a standard fixed point iteration procedure. Namely, fix some initial w^0\in\mathbb{R}^d, and for k=1,2,\ldots define

w^{k+1} = S_{\gamma}\left(w^k - \gamma \nabla F\left(w^k\right)\right).

Note here the effective trade-off between the empirical error term F(w)  and the regularization penalty R(w). This  fixed point method has decoupled the effect of the two different convex functions which comprise the objective function into a gradient descent step ( w^k - \gamma \nabla F\left(w^k\right)) and a soft thresholding step (via S_\gamma).

Convergence of this fixed point scheme is well-studied in the literature Such methods have been studied extensively in previous years.

For more general learning problems where the proximity operator cannot be computed explicitly for some regularization term R, such fixed point schemes can still be carried out using approximations to both the gradient and the proximity operator.

Practical considerations

There have been numerous developments within the past decade in convex optimization techniques which have influenced the application of proximal gradient methods in statistical learning theory. Here we survey a few important topics which can greatly improve practical algorithmic performance of these methods.

Adaptive step size

In the fixed point iteration scheme

w^{k+1} = \operatorname{prox}_{\gamma R}\left(w^k-\gamma \nabla F\left(w^k\right)\right),

one can allow variable step size \gamma_k instead of a constant \gamma. Numerous adaptive step size schemes have been proposed throughout the literature. Applications of these schemes  suggest that these can offer substantial improvement in number of iterations required for fixed point convergence.

Elastic net (mixed norm regularization)

Elastic net regularization offers an alternative to pure \ell_1 regularization. The problem of lasso (\ell_1) regularization involves the penalty term R(w) = \|w\|_1, which is not strictly convex. Hence, solutions to \min_w F(w) + R(w), where F is some empirical loss function, need not be unique. This is often avoided by the inclusion of an additional strictly convex term, such as an \ell_2 norm regularization penalty. For example, one can consider the problem

\min_{w\in\mathbb{R}^d} \frac{1}{n}\sum_{i=1}^n (y_i- \langle w,x_i\rangle)^2+ \lambda \left((1-\mu)\|w\|_1+\mu \|w\|_2^2\right),

where x_i\in \mathbb{R}^d\text{ and } y_i\in\mathbb{R}.

For 0 the penalty term \lambda \left((1-\mu)\|w\|_1+\mu \|w\|_2^2\right) is now strictly convex, and hence the minimization problem now admits a unique solution. It has been observed that for sufficiently small \mu > 0, the additional penalty term \mu \|w\|_2^2 acts as a preconditioner and can substantially improve convergence while not adversely affecting the sparsity of solutions.

Exploiting group structure

Proximal gradient methods provide a general framework which is applicable to a wide variety of problems in statistical learning theory. Certain problems in learning can often involve data which has additional structure that is known  a priori. In the past several years there have been new developments which incorporate information about group structure to provide methods which are tailored to different applications. Here we survey a few such methods.

Group lasso

Group lasso is a generalization of the lasso method when features are grouped into disjoint blocks. Suppose the features are grouped into blocks \{w_1,\ldots,w_G\}. Here we take as a regularization penalty

R(w) =\sum_{g=1}^G \|w_g\|_2,

which is the sum of the \ell_2 norm on corresponding feature vectors for the different groups. A similar proximity operator analysis as above can be used to compute the proximity operator for this penalty. Where the lasso penalty has a proximity operator which is soft thresholding on each individual component, the proximity operator for the group lasso is soft thresholding on each group. For the group w_g we have that proximity operator of \lambda\gamma\left(\sum_{g=1}^G \|w_g\|_2\right)  is given by

\widetilde{S}_{\lambda\gamma }(w_g) =  \begin{cases}

w_g-\lambda\gamma \frac{w_g}{\|w_g\|_2}, & \|w_g\|_2>\lambda\gamma \\

0, & \|w_g\|_2\leq \lambda\gamma

\end{cases}

where w_g is the gth group.

In contrast to lasso, the derivation of the proximity operator for group lasso relies on the Moreau decomposition. Here the proximity operator of the conjugate of the group lasso penalty becomes a projection onto the ball of a dual norm. For overlapping groups one common approach is known as latent group lasso which introduces latent variables to account for overlap. Nested group structures are studied in hierarchical structure prediction and with directed acyclic graphs.

See also

Convex analysis

Proximal gradient method

Regularization

Statistical learning theory

References

First order methods

Category:Convex optimization

Category:Machine learning