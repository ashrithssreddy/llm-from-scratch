{{Infobox software

name = o1

logo = OpenAI o1 icon.png

logo caption =

screenshot =

caption =

author =

developer = OpenAI

latest release version =

latest preview date =

released =

repo =

programming language =

operating system =

genre =

replaces =

replaced_by = OpenAI o3

license = Proprietary

website =

}}

OpenAI o1 is a generative pre-trained transformer (GPT), the first in OpenAI's "o" series of reasoning models. A preview of o1 was released by OpenAI on September 12, 2024. o1 spends time "thinking" before it answers, making it better at complex reasoning tasks, science and programming than GPT-4o. The full version was released to ChatGPT users on December 5, 2024.

History

Background

According to leaked information, o1 was formerly known within OpenAI as "Q*", and later as "Strawberry". In July 2024, Reuters reported that OpenAI was developing a generative pre-trained transformer known as "Strawberry", which later became o1.

Release

"o1-preview" and "o1-mini" were released on September 12, 2024, for ChatGPT Plus and Team users. On December 5, 2024, the full version of o1 was released. On the same day, a subscription called ChatGPT Pro was released, featuring access to a pro version of o1 that uses more compute to provide better answers.

o1-preview's API is several times more expensive than GPT-4o. As of January 2025, API usage for the full o1 model is limited to developers on usage tier 5.

OpenAI noted that o1 is the first of a series of "reasoning" models. OpenAI shared in December 2024 benchmark results for its successor, o3 (the name o2 was skipped to avoid trademark conflict with the mobile carrier brand named O2).

In March 2025, OpenAI released the o1-pro API, its most expensive AI model to date. The pricing is set at $150 per 1 million input tokens and $600 per 1 million output tokens.

Capabilities

According to OpenAI, o1 has been trained using a new optimization algorithm and a dataset specifically tailored to it; while also meshing in reinforcement learning into its training.

o1 spends additional time thinking (generating a chain of thought) before generating an answer, which makes it better for complex reasoning tasks, particularly in science and mathematics. According to Mira Murati, this ability to think before responding represents a new, additional paradigm, which is improving model outputs by spending more computing power when generating the answer, whereas the model scaling paradigm improves outputs by increasing the model size, training data and training compute power. OpenAI's test results suggest a correlation between accuracy and the logarithm of the amount of compute spent thinking before answering. o1-mini is faster and 80% cheaper than o1-preview. It is particularly suitable for programming and STEM-related tasks, but does not have the same "broad world knowledge" as o1-preview.

OpenAI noted that o1's reasoning capabilities make it better at adhering to safety rules provided in the prompt's context window. OpenAI reported that during a test, one instance of o1-preview exploited a misconfiguration to succeed at a task that should have been infeasible due to a bug. OpenAI also granted early access to the UK and US AI Safety Institutes for research, evaluation, and testing. According to OpenAI's assessments, o1-preview and o1-mini crossed into "medium risk" in CBRN (biological, chemical, radiological, and nuclear) weapons. Dan Hendrycks wrote that "The model already outperforms PhD scientists most of the time on answering questions related to bioweapons." He suggested that these concerning capabilities will continue to increase.

Limitations

o1 usually requires more computing time and power than other GPT models by OpenAI, because it generates long chains of thought before making the final response.

OpenAI forbids users from trying to reveal o1's chain of thought, which is hidden by design and not trained to comply with the company's policies. Prompts are monitored, and users who intentionally or accidentally violate this may lose their access to o1. OpenAI cites AI safety and competitive advantage as reasons for the restriction, which has been described as a loss of transparency by developers who work with large language models (LLMs).

In October 2024, researchers at Apple submitted a preprint reporting that LLMs such as o1 may be replicating reasoning steps from the models' own training data. By changing the numbers and names used in a math problem or simply running the same problem again, LLMs would perform somewhat worse than their best benchmark results. Adding extraneous but logically inconsequential information to the problems caused a much greater drop in performance, from −17.5% for o1-preview and −29.1% for o1-mini, to −65.7% for the worst model tested.

Safety evaluations from Apollo Research found that o1 was more consistently able to deceive than other frontier models in controlled tests (e.g. attempting to copy itself to an external server when threatened with shutdown). When confronted, it relatively rarely admitted deceptive action (in 20% of test cases).

See also

List of large language models

References

External links

Category:2024 software

Category:Generative pre-trained transformers

Category:Large language models

Category:OpenAI

Category:ChatGPT

Category:2024 in artificial intelligence