{{Infobox software

developer = Mosaic ML and Databricks team

screenshot = DBRX chatbot example screenshot.webp

screenshot_size = 270px

screenshot_alt = Screenshot of a DBRX chatbot answer, describing Wikipedia in a thoughtful way

caption = Screenshot of DBRX describing Wikipedia

released = March 27, 2024

repo = https://github.com/databricks/dbrx

license = Databricks Open Model License

website = https://www.databricks.com/blog/introducing-dbrx-new-state-art-open-llm

}}

DBRX is a large language model (LLM) developed by Mosaic under its parent company Databricks, released on March 27, 2024 under the Databricks Open Model License. It is a mixture-of-experts transformer model, with 132 billion parameters in total. 36 billion parameters (4 out of 16 experts) are active for each token. The released model comes in either a base foundation model version or an instruction-tuned variant.

At the time of its release, DBRX outperformed other prominent open-source models such as Meta's LLaMA 2, Mistral AI's Mixtral, and xAI's Grok, in several benchmarks ranging from language understanding, programming ability and mathematics.

It was trained for 2.5 months and reported using on 3,072 Nvidia H100s connected by 3.2 terabytes per second bandwidth (InfiniBand), for a training cost of US$10M.

References

Category:Large language models

Category:2024 in artificial intelligence

Category:Generative pre-trained transformers

Category:2024 software