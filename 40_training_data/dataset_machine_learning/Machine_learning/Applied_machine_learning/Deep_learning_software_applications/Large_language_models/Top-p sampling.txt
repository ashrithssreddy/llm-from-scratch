Top-p sampling, also known as nucleus sampling, is a stochastic decoding strategy for generating sequences from autoregressive probabilistic models. It was originally proposed by Ari Holtzman, Yejin Choi and colleagues in 2019 for natural language generation to address the issue of repetitive and nonsensical text generated by other common decoding methods like beam search. The technique has since been applied in other scientific fields, such as protein engineering and geophysics.

In top-p sampling, a probability threshold p is set, and the next item in a sequence is sampled only from the smallest possible set of high-probability candidates whose cumulative probability exceeds p. This method adapts the size of the candidate pool based on the model's certainty, making it more flexible than top-k sampling, which samples from a fixed number of candidates. Due to its effectiveness, top-p sampling is a widely used technique in many large language model applications.

Technique

At each step of the text generation process, a language model calculates a probability distribution over its entire vocabulary for the next token. While simply picking the token with the highest probability (greedy search) or a limited set of high-probability sequences (beam search) is possible, these deterministic methods often produce text that is dull, repetitive, or nonsensical.

Drug and protein design

Top-p sampling is used in computational biology to generate novel molecular and protein sequences from specialized language models. In de novo drug design, chemical language models trained on molecular structures use nucleus sampling to generate focused libraries of new, valid drug candidates. By combining this generation with a predictive model for bioactivity, researchers have identified novel, potent kinase inhibitors. Similarly, in protein engineering, the technique is used to sample protein language models to explore the vast space of possible amino acid sequences to find novel, functional candidates for use in therapeutics or new materials.

A range of alternative sampling strategies have been proposed to address these limitations.

Factual-nucleus sampling was proposed to counter the tendency for the "uniform randomness" applied to the nucleus to harm the factuality of the generated text. It dynamically adapts the level of randomness to improve factual accuracy while maintaining text quality.

Locally typical sampling frames text generation in an information-theoretic light. Instead of selecting only the highest-probability tokens, it samples from a set of tokens that are "locally typical" in an information-theoretic sense, which has been shown to reduce repetition and improve quality.

See also

Beam search

References

Category:Natural language generation

Category:Machine learning algorithms

Category:Large language models

Category:2019 in artificial intelligence