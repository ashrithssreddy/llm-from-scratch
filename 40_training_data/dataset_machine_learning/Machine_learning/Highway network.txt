In machine learning, the Highway Network was the first working very deep feedforward neural network with hundreds of layers, much deeper than previous neural networks.

It uses skip connections modulated by learned gating mechanisms to regulate information flow, inspired by long short-term memory (LSTM) recurrent neural networks.

The advantage of the Highway Network over other deep learning architectures is its ability to overcome or partially prevent the vanishing gradient problem, thus improving its optimization. Gating mechanisms are used to facilitate information flow across the many layers ("information highways").

Highway Networks have found use in text sequence labeling and speech recognition tasks.

In 2014, the state of the art was training deep neural networks with 20 to 30 layers. Stacking too many layers led to a steep reduction in training accuracy, known as the "degradation" problem. In 2015, two techniques were developed to train such networks: the Highway Network (published in May), and the residual neural network, or ResNet (December). ResNet behaves like an open-gated Highway Net.

Model

The model has two gates in addition to the H(W_H,x) gate: the transform gate T(W_T,x) and the carry gate C(W_C,x). The latter two gates are non-linear transfer functions (specifically sigmoid by convention). The function H can be any desired transfer function.

The carry gate is defined as:

C(W_C,x)=1-T(W_T,x)

while the transform gate is just a gate with a sigmoid transfer function.

Structure

The structure of a hidden layer in the Highway Network follows the equation:

\begin{align}

y = H(x,W_{H}) \cdot T(x,W_{T}) + x \cdot C(x,W_{C}) \\

= H(x,W_{H}) \cdot T(x,W_{T}) + x \cdot (1 - T(x,W_{T}))

\end{align}

Related work

Sepp Hochreiter analyzed the vanishing gradient problem in 1991 and attributed to it the reason why deep learning did not work well.

It is like a 2000 LSTM with forget gates unfolded in time, and Lang & Witbrock (1988) which has the form  Here the randomly initialized weight matrix A does not have to be the identity mapping. Every residual connection is a skip connection, but almost all skip connections are not residual connections.

The original Highway Network paper not only introduced the basic principle for very deep feedforward networks, but also included experimental results with 20, 50, and 100 layers networks, and mentioned ongoing experiments with up to 900 layers. Networks with 50 or 100 layers had lower training error than their plain network counterparts, but no lower training error than their 20 layers counterpart (on the MNIST dataset, Figure 1 in  however, provided strong experimental evidence of the benefits of going deeper than 20 layers. It argued that the identity mapping without modulation is crucial and mentioned that modulation in the skip connection can still lead to vanishing signals in forward and backward propagation (Section 3 in  were initially opened through positive bias weights: as long as the gates are open, it behaves like the 1997 LSTM. Similarly, a Highway Net whose gates are opened through strongly positive bias weights behaves like a ResNet. The skip connections used in modern neural networks (e.g., Transformers) are dominantly identity mappings.

References

Category:Neural network architectures

Category:Machine learning

Category:2015 in artificial intelligence