In the context of decision trees in information theory and machine learning, information gain refers to the conditional expected value of the Kullback–Leibler divergence of the univariate probability distribution of one variable from the conditional distribution of this variable given the other one. (In broader contexts, information gain can also be used as a synonym for either Kullback–Leibler divergence or mutual information, but the focus of this article is on the more narrow meaning below.)

Explicitly, the information gain of a random variable X obtained from an observation of a random variable A taking value a is defined as:

\mathit{IG}(X, a) = D_\text{KL}\bigl(P_{X \mid a} \parallel P_X\bigr)

In other words, it is the Kullback–Leibler divergence of P_X(x) (the prior distribution for X) from P_{X \mid a}(x) (the posterior distribution for X given A = a).

The expected value of the information gain is the mutual information I(X; A):

\operatorname{E}_A[\mathit{IG}(X, A)] = I(X; A)

i.e. the reduction in the entropy of X achieved by learning the state of the random variable A.

In machine learning, this concept can be used to define a preferred sequence of attributes to investigate to most rapidly narrow down the state of X.  Such a sequence (which depends on the outcome of the investigation of previous attributes at each stage) is called a decision tree, and when applied in the area of machine learning is known as decision tree learning. Usually an attribute with high mutual information should be preferred to other attributes.

General definition

In general terms, the expected information gain is the reduction in information entropy  from a prior state to a state that takes some information as given:

IG(T,a) = \Eta{(T)} - \Eta{(T|a)},

where  \Eta{(T|a)}  is the conditional entropy of  T  given the value of attribute  a .

This is intuitively plausible when interpreting entropy  as a measure of uncertainty of a random variable T: by learning (or assuming)  a  about T, our uncertainty about T is reduced (i.e. IG(T,a) is positive), unless of course T is independent of  a , in which case \Eta(T|a) = \Eta(T), meaning IG(T,a) = 0.

Formal definition

Let  denote a set of training examples, each of the form (\textbf{x},y) = (x_1, x_2, x_3, ..., x_k, y) where x_a\in \mathrm{vals}(a) is the value of the a^{\text{th}}  attribute or feature of example \textbf{x} and  is the corresponding class label. The information gain for an attribute  is defined in terms of Shannon entropy \Eta( - ) as follows.  For a value  taken by attribute , let S_a{(v)} = \{\textbf{x}\in T|x_a=v\} be defined as the set of training inputs of  for which attribute  is equal to .  Then the information gain of  for attribute  is the difference between the a priori Shannon entropy \Eta(T) of the training set and the conditional entropy  \Eta{(T|a)}.

\Eta(T|a)= \sum_{v\in \mathrm{vals}(a)} {\frac

\cdot \Eta\left(S_a{\left(v\right)}\right)}.

IG(T,a) = \Eta(T) - \Eta(T|a)

The mutual information is equal to the total entropy for an attribute if for each of the attribute values a unique classification can be made for the result attribute.  In this case, the relative entropies subtracted from the total entropy are 0. In particular, the values v \in vals(a) defines a partition of the training set data  into mutually exclusive and all-inclusive subsets, inducing a categorical probability distribution P_{a}{(v)} on the values v \in vals(a) of attribute . The distribution is given P_{a}{(v)} := \frac. In this representation, the information gain of  given  can be defined as the difference between the unconditional Shannon entropy of  and the expected entropy of  conditioned on , where the expectation value is taken with respect to the induced distribution on the values of  .\begin{alignat}{2} IG(T,a) &= \Eta(T) -

\sum_{v\in \mathrm{vals}(a)} {P_a{(v)} \Eta\left(S_a{(v)}\right)} \\

&= \Eta(T) - \mathbb{E}_{P_a}{\left[\Eta{(S_a{(v)})}\right]} \\

&= \Eta(T) - \Eta{(T|a)}.

\end{alignat}

Another Take on Information Gain, with Example

For a better understanding of information gain, let us break it down. As we know, information gain is the reduction in information entropy, what is entropy? Basically, entropy is the measure of impurity or uncertainty in a group of observations. In engineering applications, information is analogous to signal, and entropy is analogous to noise. It determines how a decision tree chooses to split data. The leftmost figure below is very impure and has high entropy corresponding to higher disorder and lower information value. As we go to the right, the entropy decreases, and the information value increases.

Now, it is clear that information gain is the measure of how much information a feature provides about a class. Let's visualize information gain in a decision tree as shown in the right:

The node t is the parent node, and the sub-nodes tL and tR are child nodes. In this case, the parent node t has a collection of cancer and non-cancer samples denoted as C and NC respectively. We can use information gain to determine how good the splitting of nodes is in a decision tree. In terms of entropy, information gain is defined as:

{{NumBlk|1=:|2=Gain = (Entropy of the parent node) – (average entropy of the child nodes)

where,

probability of selecting a class ‘C’ sample at node t, pC,t = n(t, C) / n(t),

probability of selecting a class ‘NC’ sample at node t, pNC,t = n(t, NC) / n(t),

n(t), n(t, C), and n(t, NC) are the number of total samples, ‘C’ samples and ‘NC’ samples at node t respectively.Using this with the example training set, the process for finding information gain beginning with \Eta{(t)} for Mutation 1 is as follows:

pC, t = 4/7

pNC, t = 3/7

\Eta{(t)} = &minus;(4/7log2(4/7) + 3/7log2(3/7)) = 0.985

Note: \Eta{(t)} will be the same for all mutations at the root.

The relatively high value of entropy \Eta{(t)} = 0.985 (1 is the optimal value) suggests that the root node is highly impure and the constituents of the input at the root node would look like the leftmost figure in the above Entropy Diagram. However, such a set of data is good for learning the attributes of the mutations used to split the node. At a certain node, when the homogeneity of the constituents of the input occurs (as shown in the rightmost figure in the above Entropy Diagram), the dataset would no longer be good for learning.

Moving on, the entropy at left and right child nodes of the above decision tree is computed using the formulae:H(tL) = &minus;[pC,L log2(pC,L) + pNC,L log2(pNC,L)]

Drawbacks and Solutions

Although information gain is usually a good measure for deciding the relevance of an attribute, it is not perfect. A notable problem occurs when information gain is applied to attributes that can take on a large number of distinct values. For example, suppose that one is building a decision tree for some data describing the customers of a business. Information gain is often used to decide which of the attributes are the most relevant, so they can be tested near the root of the tree. One of the input attributes might be the customer's membership number, if they are a member of the business's membership program. This attribute has a high mutual information, because it uniquely identifies each customer, but we do not want to include it in the decision tree. Deciding how to treat a customer based on their membership number is unlikely to generalize to customers we haven't seen before (overfitting). This issue can also occur if the samples that are being tested have multiple attributes with many distinct values. In this case, it can cause the information gain of each of these attributes to be much higher than those without as many distinct values.

To counter this problem, Ross Quinlan proposed to instead choose the attribute with highest information gain ratio from among the attributes whose information gain is average or higher. This biases the decision tree against considering attributes with a large number of distinct values, while not giving an unfair advantage to attributes with very low information value, as the information value is higher or equal to the information gain.

See also

Information gain more broadly

Decision tree learning

Information content, the starting point of information theory and the basis of Shannon entropy

Information gain ratio

ID3 algorithm

C4.5 algorithm

Surprisal analysis

References

Further reading

Category:Decision trees

Category:Classification algorithms