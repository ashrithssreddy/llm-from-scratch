In machine learning, one-class classification (OCC), also known as unary classification or class-modelling, is an approach to the training of binary classifiers in which only examples of one of the two classes are used.

Examples include the monitoring of helicopter gearboxes, motor failure prediction, or assessing the operational status of a nuclear plant as 'normal': In such scenarios, there are few, if any, examples of the catastrophic system states – rare outliers – that comprise the second class. Alternatively, the class that is being focussed on may cover a small, coherent subset of the data and the training may rely on an information bottleneck approach.

In practice, counter-examples from the second class may be used in later rounds of training to further refine the algorithm.

Overview

The term one-class classification (OCC) was coined by Moya & Hush (1996) and many applications can be found in scientific literature, for example outlier detection, anomaly detection, novelty detection. A feature of OCC is that it uses only sample points from the assigned class, so that a representative sampling is not strictly required for non-target classes.

Introduction

SVM based one-class classification (OCC) relies on identifying the smallest hypersphere (with radius r, and center c) consisting of all the data points. This method is called Support Vector Data Description (SVDD). Formally, the problem can be defined in the following constrained optimization form,

\min_{r,c} r^2 \text{ subject to, } ||\Phi(x_i) - c||^2 \le r^2 \;\; \forall i = 1, 2, ..., n

However, the above formulation is highly restrictive, and is sensitive to the presence of outliers. Therefore, a flexible formulation, that allow for the presence of outliers is formulated as shown below,

\min_{r,c,\zeta} r^2 + \frac{1}{\nu n}\sum_{i=1}^{n}\zeta_i

\text{subject to, } ||\Phi(x_i) - c||^2 \le r^2 + \zeta_i \;\; \forall i = 1, 2, ..., n

From the Karush–Kuhn–Tucker conditions for optimality, we get

c = \sum_{i=1}^{n}\alpha_i\Phi(x_i),

where the \alpha_i's are the solution to the following optimization problem:

\max_\alpha \sum_{i=1}^{n}\alpha_i\kappa(x_i, x_i) - \sum_{i, j = 1}^{n}\alpha_i\alpha_j\kappa(x_i, x_j)

subject to, \sum_{i=1}^{n}\alpha_i = 1 \text{ and } 0 \le \alpha_i \le \frac{1}{\nu n} \text{for all } i = 1,2,...,n.

The introduction of kernel function provide additional flexibility to the One-class SVM (OSVM) algorithm.

PU (Positive Unlabeled) learning

A similar problem is PU learning, in which a binary classifier is constructed by semi-supervised learning from only positive and unlabeled sample points.

In PU learning, two sets of examples are assumed to be available for training: the positive set P and a mixed set U, which is assumed to contain both positive and negative samples, but without these being labeled as such. This contrasts with other forms of semisupervised learning, where it is assumed that a labeled set containing examples of both classes is available in addition to unlabeled samples. A variety of techniques exist to adapt supervised classifiers to the PU learning setting, including variants of the EM algorithm. PU learning has been successfully applied to text, time series, bioinformatics tasks, and remote sensing data.

Approaches

Several approaches have been proposed to solve one-class classification (OCC). The approaches can be distinguished into three main categories, density estimation, boundary methods, and reconstruction methods. is one of the simplest methods to create one-class classifiers. Due to Central Limit Theorem (CLT), these methods work best when large number of samples are present, and they are perturbed by small independent error values. The probability distribution for a d-dimensional object is given by:

p_{\mathcal{N}}(z;\mu;\Sigma) = \frac{1}{(2\pi)^{\frac{d}{2}} |\Sigma|^\frac{1}{2}}\exp\left\{-\frac{1}{2}(z-\mu)^T \Sigma^{-1} (z - \mu)\right\}

Where, \mu is the mean and \Sigma is the covariance matrix. Computing the inverse of covariance matrix (\Sigma^{-1}) is the costliest operation, and in the cases where the data is not scaled properly, or data has singular directions pseudo-inverse \Sigma^+ is used to approximate the inverse, and is calculated as \Sigma^T(\Sigma \Sigma^T)^{-1}.

Boundary methods

Boundary methods focus on setting boundaries around a few set of points, called target points. These methods attempt to optimize the volume. Boundary methods rely on distances, and hence are not robust to scale variance. K-centers method, NN-d, and SVDD are some of the key examples.

K-centers

In K-center algorithm, k small balls with equal radius are placed to minimize the maximum distance of all minimum distances between training objects and the centers. Formally, the following error is minimized,

\varepsilon_{k-center} = \max_i ( \min_k || x_i - \mu_k ||^2 )

The algorithm uses forward search method with random initialization, where the radius is determined by the maximum distance of the object, any given ball should capture. After the centers are determined, for any given test object z the distance can be calculated as,

d_{k-centr}(z) = \min_k || z - \mu_k ||^2

Reconstruction methods

Reconstruction methods use prior knowledge and generating process to build a generating model that best fits the data. New objects can be described in terms of a state of the generating model. Some examples of reconstruction methods for OCC are, k-means clustering, learning vector quantization, self-organizing maps, etc.

Applications

Document classification

The basic Support Vector Machine (SVM) paradigm is trained using both positive and negative examples, however studies have shown there are many valid reasons for using only positive examples. When the SVM algorithm is modified to only use positive examples, the process is considered one-class classification. One situation where this type of classification might prove useful to the SVM paradigm is in trying to identify a web browser's sites of interest based only off of the user's browsing history.

Biomedical studies

One-class classification can be particularly useful in biomedical studies where often data from other classes can be difficult or impossible to obtain. In studying biomedical data it can be difficult and/or expensive to obtain the set of labeled data from the second class that would be necessary to perform a two-class classification. A study from The Scientific World Journal found that the typicality approach is the most useful in analysing biomedical data because it can be applied to any type of dataset (continuous, discrete, or nominal). The typicality approach is based on the clustering of data by examining data and placing it into new or existing clusters. To apply typicality to one-class classification for biomedical studies, each new observation, y_0

, is compared to the target class, C, and identified as an outlier or a member of the target class. One-class classifiers are used for detecting concept drifts.

See also

Multiclass classification

Anomaly detection

Supervised learning

References

Category:Statistical classification

Category:Classification algorithms