Perceiver is a variant of the Transformer architecture, adapted for processing arbitrary forms of data, such as images, sounds and video, and spatial data. Unlike previous notable Transformer systems such as BERT and GPT-3, which were designed for text processing, the Perceiver is designed as a general architecture that can learn from large amounts of heterogeneous data. It accomplishes this with an asymmetric attention mechanism to distill inputs into a latent bottleneck.

Perceiver matches or outperforms specialized models on classification tasks.

Perceiver was introduced in June 2021 by DeepMind.

Outputs are produced by attending to the latent array using a specific output query associated with that particular output. For example to predict optical flow on one pixel a query would attend using the pixelâ€™s xy coordinates plus an optical flow task embedding to produce a single flow vector. It is a variation on the encoder/decoder architecture used in other designs.

Performance

Perceiver's performance is comparable to ResNet-50 and ViT on ImageNet without 2D convolutions. It attends to 50,000 pixels. It is competitive in all modalities in AudioSet.

See also

Convolutional neural network

Transformer (machine learning model)

References

External links

{{YouTube|WJWBq4NZfvY|DeepMind Perceiver and Perceiver IO  Paper Explained}}

, with the Fourier features explained in more detail

Category:Machine learning