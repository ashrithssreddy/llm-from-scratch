random numbers using different smoothing bandwidths.]]

In statistics, kernel density estimation (KDE) is the application of kernel smoothing for probability density estimation, i.e., a non-parametric method to estimate the probability density function of a random variable based on kernels as weights. KDE answers a fundamental data smoothing problem where inferences about the population are made based on a finite data sample. In some fields such as signal processing and econometrics it is also termed the Parzen–Rosenblatt window method,  after Emanuel Parzen and Murray Rosenblatt, who are usually credited with independently creating it in its current form. One of the famous applications of kernel density estimation is in estimating the class-conditional marginal densities of data when using a naive Bayes classifier, which can improve its prediction accuracy.

Definition

Let \mathbf{x} = \left ( x_1, x_2, x_3, ... \right ) be independent and identically distributed samples drawn from some univariate distribution with an unknown density  at any given point . We are interested in estimating the shape of this function . Its kernel density estimator is

\hat{f}_h(x) = \frac{1}{n}\sum_{i=1}^n K_h (x - x_i) = \frac{1}{nh} \sum_{i=1}^n K{\left(\frac{x-x_i}{h}\right)},

where  is the kernel — a non-negative function — and  is a smoothing parameter called the bandwidth or simply width. though the loss of efficiency is small for the kernels listed previously. Due to its convenient mathematical properties, the normal kernel is often used, which means , where  is the standard normal density function. The kernel density estimator then becomes

\hat{f}_h(x) = \frac{1}{nh\sigma} \frac{1}{\sqrt{2\pi}} \sum_{i=1}^n \exp\left(\frac{-(x-x_i)^2}{2h^2\sigma^2}\right),

where \sigma is the standard deviation of the sample \mathbf{x}.

The construction of a kernel density estimate finds interpretations in fields outside of density estimation. For example, in thermodynamics, this is equivalent to the amount of heat generated when heat kernels (the fundamental solution to the heat equation) are placed at each data point locations . Similar methods are used to construct discrete Laplace operators on point clouds for manifold learning (e.g. diffusion map).

Example

Kernel density estimates are closely related to histograms, but can be endowed with properties such as smoothness or continuity by using a suitable kernel. The diagram below based on these 6 data points illustrates this relationship:

For the histogram, first, the horizontal axis is divided into sub-intervals or bins which cover the range of the data: In this case, six bins each of width 2. Whenever a data point falls inside this interval, a box of height 1/12 is placed there. If more than one data point falls inside the same bin, the boxes are stacked on top of each other.

For the kernel density estimate, normal kernels with a standard deviation of 1.5 (indicated by the red dashed lines) are placed on each of the data points xi. The kernels are summed to make the kernel density estimate (solid blue curve). The smoothness of the kernel density estimate (compared to the discreteness of the histogram) illustrates how kernel density estimates converge faster to the true underlying density for continuous random variables.

on the horizontal axis.]]

Bandwidth selection

The bandwidth of the kernel is a free parameter which exhibits a strong influence on the resulting estimate. To illustrate its effect, we take a simulated random sample from the standard normal distribution (plotted at the blue spikes in the rug plot on the horizontal axis). The grey curve is the true density (a normal density with mean 0 and variance 1). In comparison, the red curve is undersmoothed since it contains too many spurious data artifacts arising from using a bandwidth h = 0.05, which is too small. The green curve is oversmoothed since using the bandwidth  obscures much of the underlying structure. The black curve with a bandwidth of h = 0.337 is considered to be optimally smoothed since its density estimate is close to the true density. An extreme situation is encountered in the limit h \to 0 (no smoothing), where the estimate is a sum of  delta functions centered at the coordinates of analyzed samples. In the other extreme limit h \to \infty the estimate retains the shape of the used kernel, centered on the mean of the samples (completely smooth).

The most common optimality criterion used to select this parameter is the expected L2 risk function, also termed the mean integrated squared error:

\operatorname{MISE} (h) = \operatorname{E}\!\left[ \int\! {\left(\hat{f}\!_h(x) - f(x)\right)}^2 dx \right]

Under weak assumptions on   and , ( is the, generally unknown, real density function), with the general consensus that the plug-in selectors and cross validation selectors are the most useful over a wide range of data sets.

Substituting any bandwidth  which has the same asymptotic order  as  into the AMISE gives that , where  is the big O notation. It can be shown that, under weak assumptions, there cannot exist a non-parametric estimator that converges at a faster rate than the kernel estimator. Note that the  rate is slower than the typical  convergence rate of parametric methods.

If the bandwidth is not held fixed, but is varied depending upon the location of either the estimate (balloon estimator) or the samples (pointwise estimator), this produces a particularly powerful method termed adaptive or variable bandwidth kernel density estimation.

Bandwidth selection for kernel density estimation of heavy-tailed distributions is relatively difficult.

A rule-of-thumb bandwidth estimator

If Gaussian basis functions are used to approximate univariate data, and the underlying density being estimated is Gaussian, the optimal choice for h (that is, the bandwidth that minimises the mean integrated squared error) is:

h = {\left(\frac{4\hat{\sigma}^5}{3n}\right)}^{1/5} \approx 1.06 \, \hat{\sigma} \, n^{-1/5},

An h value is considered more robust when it improves the fit for long-tailed and skewed distributions or for bimodal mixture distributions. This is often done empirically by replacing the standard deviation \hat{\sigma} by the parameter A below:

A = \min\left(\hat{\sigma}, \frac{\mathrm{IQR}}{1.34}\right) where IQR is the interquartile range.

Another modification that will improve the model is to reduce the factor from 1.06 to 0.9. Then the final formula would be:

h = 0.9\, \min\left(\hat{\sigma}, \frac{\mathrm{IQR}}{1.34}\right)\, n^{-1/5}

where n is the sample size.

This approximation is termed the normal distribution approximation, Gaussian approximation, or Silverman's rule of thumb. where g(x) and \lambda_1(x) are KDE version of g(x) and \lambda_1(x). Under mild assumptions, M_c is a consistent estimator of M. Note that one can use the mean shift algorithm to compute the estimator M_c numerically.

Statistical implementation

A non-exhaustive list of software implementations of kernel density estimators includes:

In Analytica release 4.4, the Smoothing option for PDF results uses KDE, and from expressions it is available via the built-in Pdf function.

In C/C++, FIGTree is a library that can be used to compute kernel density estimates using normal kernels. MATLAB interface available.

In C++, libagf is a library for variable kernel density estimation.

In C++, mlpack is a library that can compute KDE using many different kernels. It allows to set an error tolerance for faster computation. Python and R interfaces are available.

in C# and F#, Math.NET Numerics is an open source library for numerical computation which includes kernel density estimation

In CrimeStat, kernel density estimation is implemented using five different kernel functions – normal, uniform, quartic, negative exponential, and triangular. Both single- and dual-kernel density estimate routines are available.  Kernel density estimation is also used in interpolating a Head Bang routine, in estimating a two-dimensional Journey-to-crime density function, and in estimating a three-dimensional Bayesian Journey-to-crime estimate.

In ELKI, kernel density functions can be found in the package de.lmu.ifi.dbs.elki.math.statistics.kernelfunctions

In ESRI products, kernel density mapping is managed out of the Spatial Analyst toolbox and uses the Quartic(biweight) kernel.

In Excel, the Royal Society of Chemistry has created an add-in to run kernel density estimation based on their Analytical Methods Committee Technical Brief 4.

In gnuplot, kernel density estimation is implemented by the smooth kdensity option, the datafile can contain a weight and bandwidth for each point, or the bandwidth can be set automatically according to "Silverman's rule of thumb" (see above).

In Haskell, kernel density is implemented in the statistics package.

In IGOR Pro, kernel density estimation is implemented by the StatsKDE operation (added in Igor Pro 7.00).  Bandwidth can be user specified or estimated by means of Silverman, Scott or Bowmann and Azzalini. Kernel types are: Epanechnikov, Bi-weight, Tri-weight, Triangular, Gaussian and Rectangular.

In Java, the Weka machine learning package provides weka.estimators.KernelEstimator, among others.

In JavaScript, the visualization package D3.js offers a KDE package in its science.stats package.

In JMP, the Graph Builder platform utilizes kernel density estimation to provide contour plots and high density regions (HDRs) for bivariate densities, and violin plots and HDRs for univariate densities. Sliders allow the user to vary the bandwidth.  Bivariate and univariate kernel density estimates are also provided by the Fit Y by X and Distribution platforms, respectively.

In Julia, kernel density estimation is implemented in the KernelDensity.jl package.

In KNIME, 1D and 2D Kernel Density distributions can be generated and plotted using nodes from the Vernalis community contribution, e.g. 1D Kernel Density Plot, among others.  The underlying implementation is written in Java.

In MATLAB, kernel density estimation is implemented through the ksdensity function (Statistics Toolbox). As of the 2018a release of MATLAB, both the bandwidth and kernel smoother can be specified, including other options such as specifying the range of the kernel density. Alternatively, a free MATLAB software package which implements an automatic bandwidth selection method).

In Mathematica, numeric kernel density estimation is implemented by the function SmoothKernelDistribution and symbolic estimation is implemented using the function KernelMixtureDistribution both of which provide data-driven bandwidths.

In Minitab, the Royal Society of Chemistry has created a macro to run kernel density estimation based on their Analytical Methods Committee Technical Brief 4.

In the NAG Library, kernel density estimation is implemented via the g10ba routine (available in both the Fortran and the C versions of the Library).

In Nuklei, C++ kernel density methods focus on data from the Special Euclidean group SE(3).

In Octave, kernel density estimation is implemented by the kernel_density option (econometrics package).

In Origin, 2D kernel density plot can be made from its user interface, and two functions, Ksdensity for 1D and Ks2density for 2D can be used from its LabTalk , Python, or C code.

In Perl, an implementation can be found in the Statistics-KernelEstimation module

In PHP, an implementation can be found in the MathPHP library

In Python, many implementations exist:  pyqt_fit.kde Module in the PyQt-Fit package, SciPy (scipy.stats.gaussian_kde), Statsmodels (KDEUnivariate and KDEMultivariate), and scikit-learn (KernelDensity) (see comparison). KDEpy supports weighted data and its FFT implementation is orders of magnitude faster than the other implementations. The commonly used pandas library offers support for kde plotting through the plot method (df.plot(kind='kde')[https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.plot.kde.html). The getdist package for weighted and correlated MCMC samples supports optimized bandwidth, boundary correction and higher-order methods for 1D and 2D distributions. One newly used package for kernel density estimation is seaborn ( import seaborn as sns , sns.kdeplot() ). A GPU implementation of KDE also exists.

In R, it is implemented through density in the base distribution, and bw.nrd0 function is used in stats package, this function uses the optimized formula in Silverman's book. bkde in the KernSmooth library,  ParetoDensityEstimation in the DataVisualizations library (for pareto distribution density estimation), kde in the ks library, dkden and dbckden in the evmix library (latter for boundary corrected kernel density estimation for bounded support), npudens in the np library (numeric and categorical data), sm.density in the sm library. For an implementation of the kde.R function, which does not require installing any packages or libraries, see kde.R. The btb library, dedicated to urban analysis, implements kernel density estimation through kernel_smoothing.

In SAS, proc kde can be used to estimate univariate and bivariate kernel densities.

In Apache Spark, the KernelDensity() class

In Stata, it is implemented through kdensity; for example histogram x, kdensity. Alternatively a free Stata module KDENS is available allowing a user to estimate 1D or 2D density functions.

In Swift, it is implemented through SwiftStats.KernelDensityEstimation in the open-source statistics library SwiftStats.

See also

Kernel (statistics)

Kernel smoothing

Kernel regression

Density estimation (with presentation of other examples)

Mean-shift

Scale space: The triplets {(x, h, KDE with bandwidth h evaluated at x: all x, h > 0} form a scale space representation of the data.

Multivariate kernel density estimation

Variable kernel density estimation

Head/tail breaks

Further reading

References

External links

Introduction to kernel density estimation A short tutorial which motivates kernel density estimators as an improvement over histograms.

Kernel Bandwidth Optimization A free online tool that generates an optimized kernel density estimate.

Free Online Software (Calculator) computes the Kernel Density Estimation for a data series according to the following Kernels: Gaussian, Epanechnikov, Rectangular, Triangular, Biweight, Cosine, and Optcosine.

Kernel Density Estimation Applet An online interactive example of kernel density estimation. Requires .NET 3.0 or later.

Category:Estimation of densities

Category:Nonparametric statistics

Category:Machine learning