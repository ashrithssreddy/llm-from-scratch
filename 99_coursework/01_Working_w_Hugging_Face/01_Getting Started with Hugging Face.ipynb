{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1c158f40",
   "metadata": {},
   "source": [
    "## Introduction to Hugging Face\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfe6c30e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Access collaborate and stay informed\n",
    "# Models, Datasets and Apps (called huggingface spaces)\n",
    "\n",
    "## Python libraries avaiable\n",
    "Core ML libraries\n",
    "Transormers (built by hugging face) and Transormers.js \n",
    "Diffusers\n",
    "Datasets\n",
    "Tokenizers\n",
    "Evaluate\n",
    "timm - vision Models\n",
    "Sentense Trnasformers - Embeddidngs, retrieval and reranking\n",
    "\n",
    "## Training n Optimization\n",
    "    PEFT - Parameter effivcient fine tuning\n",
    "    Accelerate - Train pytorch models using multi-GPT, TPU effivcient\n",
    "    Optimum - Optimize HF transformers for faster training and inference\n",
    "    AWS trianium and inferentia\n",
    "    TRL - Train with reinforment learning\n",
    "    Safetensors - Safe way to store/transmit NN weights\n",
    "    bitsandbytes - Quantize models\n",
    "    Lighteval - evaluate LLMs across multiple backends\n",
    "\n",
    "## e.g. Text Generation Model (for custom enterprise data)\n",
    "Model Page\n",
    "    https://huggingface.co/zai-org/GLM-4.5\n",
    "Model Card\n",
    "\n",
    "Use the Model\n",
    "    Transformers python library to use models\n",
    "    Notebooks \n",
    "        Google Colab\n",
    "        Kaggle\n",
    "    Load Model\n",
    "        vLLM\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d38346be",
   "metadata": {},
   "source": [
    "## Running Hugging Face models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c34112cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "Run on personal local harware, or on the personal cloud\n",
    "    Good for small models\n",
    "Run on inference provider (partners who runs the computation) using hugging face API\n",
    "    For large GPU needs. Fast, todo: free trial \n",
    "\n",
    "## Transformers library -- local machine\n",
    "from transformers import pipeline\n",
    "\n",
    "todo: what is a task? same as Tasks on hugging face models page?\n",
    "gpt2_pipeline = pipeline(task = \"task-generation\", model = \"openai-community/gpt2\")\n",
    "command:\n",
    "gpt2_pipeline(\"what if AI\")\n",
    "output: list of dictinaries\n",
    "[\n",
    "    {\"generated_text\": \"what if AI would not be used, ...\"}\n",
    "]\n",
    "\n",
    "todo: all the params of pipeline object?\n",
    "max_new_tokens = output is 10 tokens max\n",
    "num_return_sequences === 2 texts outputs produced\n",
    "gpt2_pipeline(\"what if AI\", max_new_tokens=10, num_return_sequences=2)\n",
    "\n",
    "[\n",
    "    {\"generated_text\": \"what if AI would not be used, ...\"}\n",
    "    {\"generated_text\": \"what if AI was not smarter than humans ...\"}\n",
    "]\n",
    "\n",
    "## Inference providers\n",
    "examples: AWS (SageMaker, Bedrock), Google Cloud (Vertex AI), Azure (Azure ML)\n",
    "    Together AI, Replicate, Banana.dev, Modal, Runpod\n",
    "\n",
    "todo: setup an API key\n",
    "from huggingface_hub import InferenceClient\n",
    "import os\n",
    "\n",
    "client = InferenceClient(\n",
    "    provider = \"together\", # together-AI provider\n",
    "    api_key = os.environ[\"HF_TOKEN\"]\n",
    ")\n",
    "\n",
    "# an app with conversation interface\n",
    "completion = client.chat.completions.create(\n",
    "    model  = \"deepseek-ai/DeepSeek-V3\"\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"user\", # input to the model\n",
    "            \"context\": \"what is the capital city of France?\"\n",
    "        }\n",
    "    ]\n",
    ")\n",
    "print(completion.choices[0].message)\n",
    "\"The capital of France is Paris, known for iconic landmarks\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c077d84d",
   "metadata": {},
   "source": [
    "## Hugging Face Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03fdd7fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "e.g. italian text generation\n",
    "Text modalities\n",
    "Text Generation task\n",
    "Dataset card\n",
    "    meta data\n",
    "    comilation method\n",
    "    row count \n",
    "    licenses\n",
    "    Explore: Data Studio and data viewer. sql query\n",
    "\n",
    "pip install datasets --- developed by hugging face\n",
    "    Access, use, download\n",
    "\n",
    "from datasets import load_dataset\n",
    "# todo: how different from pandas dataframe?\n",
    "data = load_dataset(\"IVN-BIN/BioBERT_Italian\", split = \"train\") # \"train\", test and validate\n",
    "Common format: apache arrow (column based storage for faster querying)\n",
    "\n",
    "filtered = data.filter(lambda row: \"bella\" in row[\"text\"]) # datasets.arrow_dataset.Dataset\n",
    "Dataset({\n",
    "    features: ['id', 'url', 'title', 'text'],\n",
    "    num_rows: 93\n",
    "})\n",
    "\n",
    "sliced = data.select(range(20)) # first 20 rows\n",
    "Dataset({\n",
    "    features: ['id', 'url', 'title', 'text'],\n",
    "    num_rows: 20\n",
    "})\n",
    "\n",
    "sliced[0]\n",
    "{'id': '20519193',\n",
    " 'url': 'https://en.wikipedia.org/wiki/Gugu%20%28footballer%29',\n",
    " 'title': 'Gugu (footballer)',\n",
    " 'text': \"Luis Miguel Aparecido Alves (born May 25, 1985), known as Gugu, is a Brazilian ...\"\n",
    " }\n",
    "\n",
    "sliced[0]['text']\n",
    "\"Luis Miguel Aparecido Alves (born May 25, 1985), known as Gugu, is a Brazilian ...\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "263f10ec",
   "metadata": {},
   "source": [
    "## Scratch Notes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81e5a986",
   "metadata": {},
   "outputs": [],
   "source": [
    "#v todo: explanation for each of them\n",
    "available_tasks = [\n",
    "    'audio-classification', \n",
    "    'automatic-speech-recognition', \n",
    "    'depth-estimation', \n",
    "    'document-question-answering', \n",
    "    'feature-extraction', \n",
    "    'fill-mask', \n",
    "    'image-classification', \n",
    "    'image-feature-extraction', \n",
    "    'image-segmentation', \n",
    "    'image-text-to-text', \n",
    "    'image-to-image', \n",
    "    'image-to-text', \n",
    "    'mask-generation', \n",
    "    'ner', \n",
    "    'object-detection', \n",
    "    'question-answering', \n",
    "    'sentiment-analysis', \n",
    "    'summarization', \n",
    "    'table-question-answering', \n",
    "    'text-classification', \n",
    "    'text-generation', \n",
    "    'text-to-audio', \n",
    "    'text-to-speech', \n",
    "    'text2text-generation', \n",
    "    'token-classification', \n",
    "    'translation', \n",
    "    'video-classification', \n",
    "    'visual-question-answering', \n",
    "    'vqa', \n",
    "    'zero-shot-audio-classification', \n",
    "    'zero-shot-classification', \n",
    "    'zero-shot-image-classification', \n",
    "    'zero-shot-object-detection', \n",
    "    'translation_XX_to_YY']\n",
    "\n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
