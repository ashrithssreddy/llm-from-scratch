{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "94e2f57f",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a1a4892",
   "metadata": {},
   "source": [
    "# Enumerate Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "526689fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found models:\n",
      "C:\\Users\\Delulu Lemon\\.lmstudio\\models\\lmstudio-community\\gpt-oss-20b-GGUF\\gpt-oss-20b-MXFP4.gguf\n",
      "C:\\Users\\Delulu Lemon\\.lmstudio\\models\\dphn\\dolphin-2.9-llama3-8b-gguf\\dolphin-2.9-llama3-8b-q4_K_M.gguf\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "models_dir = Path(r\"C:\\Users\\Delulu Lemon\\.lmstudio\\models\")\n",
    "\n",
    "# Recursively find all .gguf files\n",
    "gguf_models = [f for f in models_dir.rglob(\"*.gguf\")]\n",
    "\n",
    "print(\"Found models:\")\n",
    "for model in gguf_models:\n",
    "    print(model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b64a5c3c",
   "metadata": {},
   "source": [
    "# Examine the models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f5aa447",
   "metadata": {},
   "source": [
    "#### Read metadata only\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3ccb5088",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'GGUF\\x03\\x00\\x00\\x00#\\x01\\x00\\x00\\x00\\x00\\x00\\x00\\x17\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x14\\x00\\x00\\x00\\x00\\x00\\x00\\x00general.architecture\\x08\\x00\\x00\\x00\\x05\\x00\\x00\\x00\\x00\\x00\\x00\\x00llama\\x0c\\x00\\x00\\x00\\x00\\x00\\x00\\x00general.name\\x08\\x00\\x00\\x00\\x07\\x00\\x00\\x00\\x00\\x00\\x00\\x00E:\\\\LLMs\\x10\\x00\\x00\\x00\\x00\\x00\\x00\\x00llama.vocab_size\\x04\\x00\\x00\\x00\\x02\\xf5\\x01\\x00\\x14\\x00\\x00\\x00\\x00\\x00\\x00\\x00llama.context_length\\x04\\x00\\x00\\x00\\x00 \\x00\\x00\\x16\\x00\\x00\\x00\\x00\\x00\\x00\\x00llama.embedding_'\n"
     ]
    }
   ],
   "source": [
    "file_path = r\"C:\\Users\\Delulu Lemon\\.lmstudio\\models\\dphn\\dolphin-2.9-llama3-8b-gguf\\dolphin-2.9-llama3-8b-q4_K_M.gguf\"\n",
    "\n",
    "with open(file_path, \"rb\") as f:\n",
    "    header = f.read(1024)\n",
    "    print(header[:200])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19ee3ec5",
   "metadata": {},
   "source": [
    "#### Convert GGUF to something readable\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d44de081",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'llama_cpp'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mllama_cpp\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Llama\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpathlib\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Path\n\u001b[0;32m      4\u001b[0m file_path \u001b[38;5;241m=\u001b[39m Path(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mC:\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mUsers\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mDelulu Lemon\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124m.lmstudio\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mmodels\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mdphn\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mdolphin-2.9-llama3-8b-gguf\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mdolphin-2.9-llama3-8b-q4_K_M.gguf\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'llama_cpp'"
     ]
    }
   ],
   "source": [
    "from llama_cpp import Llama\n",
    "from pathlib import Path\n",
    "\n",
    "file_path = Path(r\"C:\\Users\\Delulu Lemon\\.lmstudio\\models\\dphn\\dolphin-2.9-llama3-8b-gguf\\dolphin-2.9-llama3-8b-q4_K_M.gguf\")\n",
    "\n",
    "# Load the model (light load, no big memory allocation)\n",
    "llm = Llama(model_path=str(file_path), n_ctx=8)\n",
    "\n",
    "# Print basic model info\n",
    "print(\"Model config:\")\n",
    "print(llm.config)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00167b97",
   "metadata": {},
   "source": [
    "#### Inspect with hexdump style"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8864f47",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(file_path, \"rb\") as f:\n",
    "    data = f.read(512)  # first 512 bytes\n",
    "    print(\" \".join(f\"{b:02x}\" for b in data))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
