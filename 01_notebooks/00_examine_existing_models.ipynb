{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "91bccd93",
   "metadata": {},
   "source": [
    "##### 00_examine_existing_models.ipynb - Purpose\n",
    "This notebook is ONLY about examining existing pretrained models ‚Äî how they‚Äôre stored, what files they contain, and how they‚Äôre structured on disk.\n",
    "\n",
    "##### Goals\n",
    "- Download small pretrained models from Hugging Face.\n",
    "- Inspect the folder layout (tokenizer files, config, model weights).\n",
    "- Understand what each file does (.bin, .safetensors, config.json, tokenizer.json, etc.).\n",
    "- Load the model and print shapes of key components (embeddings, attention, MLP).\n",
    "- Get a practical feel for ‚Äúmodel anatomy‚Äù before building my own tiny version later.\n",
    "\n",
    "##### Scope (Important)\n",
    "- **NOT** learning how they're trained.  \n",
    "- **ONLY** file structure, weights, components, and practical inspection.\n",
    "\n",
    "##### Expected Outputs\n",
    "- Folder snapshots.\n",
    "- Breakdown of model components.\n",
    "- Parameter counts and shape summaries.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94e2f57f",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2273ba32",
   "metadata": {},
   "source": [
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e75f3ca4",
   "metadata": {},
   "source": [
    "# Download Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "882fa371",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Default download folder: C:\\Users\\Delulu Lemon\\.cache\\huggingface\n",
      "‚úì Model already downloaded at: C:\\Users\\Delulu Lemon\\.cache\\huggingface\\hub\\models--gpt2\\snapshots\\607a30d783dfa663caf39e06633721c8d4cfcd7e\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModel, AutoTokenizer, AutoConfig\n",
    "from pathlib import Path\n",
    "import os\n",
    "import json\n",
    "\n",
    "# Get default cache directory\n",
    "hf_home = os.environ.get('HF_HOME')\n",
    "default_download_path = Path(hf_home) if hf_home else Path.home() / \".cache\" / \"huggingface\"\n",
    "print(f\"Default download folder: {default_download_path.absolute()}\")\n",
    "\n",
    "# Model to download\n",
    "# Toy + small model examples (uncomment ONE to use)\n",
    "\n",
    "# --- TINY MODELS (<100MB weights) ---\n",
    "# model_name = \"sshleifer/tiny-gpt2\"                     # ~15M params,  ~70MB  (best tiny GPT-2 for inspection)\n",
    "# model_name = \"roneneldan/TinyStories-1M\"               # ~1M params,   ~6MB   (ultra tiny, educational)\n",
    "# model_name = \"roneneldan/TinyStories-10M\"              # ~10M params,  ~30MB  (tiny decoder-only transformer)\n",
    "# model_name = \"EleutherAI/pythia-14m\"                   # ~14M params,  ~55MB  (tiny pythia baseline)\n",
    "# model_name = \"google/flan-t5-small\"                    # ~60M params,  ~240MB (encoder-decoder; small but different arch)\n",
    "\n",
    "# --- SMALL MODELS (~100M‚Äì200M weights) ---\n",
    "# model_name = \"distilgpt2\"                              # ~82M params,  ~320MB (distilled GPT2; great size/quality balance)\n",
    "# model_name = \"EleutherAI/pythia-70m\"                   # ~70M params,  ~280MB (tiny GPT-NeoX style arch)\n",
    "# model_name = \"gpt2\"                                    # ~124M params, ~500MB (baseline GPT-2 small)\n",
    "# model_name = \"facebook/opt-125m\"                       # ~125M params, ~500MB (OPT architecture)\n",
    "\n",
    "# --- MID-SMALL MODELS (>200M weights) ---\n",
    "# model_name = \"microsoft/DialoGPT-small\"                # ~117M params, ~470MB (GPT2 tuned for dialogue)\n",
    "# model_name = \"EleutherAI/pythia-160m\"                  # ~160M params, ~640MB (larger Pythia variant)\n",
    "# model_name = \"gpt2-medium\"                             # ~355M params, ~1.4GB (too big for deep inspection but realistic)\n",
    "\n",
    "\n",
    "model_name = \"gpt2\"\n",
    "\n",
    "# Check if model already exists\n",
    "model_cache_subdir = default_download_path / \"hub\" / f\"models--{model_name.replace('/', '--')}\"\n",
    "snapshots_dir = model_cache_subdir / \"snapshots\"\n",
    "\n",
    "if snapshots_dir.exists():\n",
    "    snapshots = list(snapshots_dir.iterdir())\n",
    "    if snapshots:\n",
    "        print(f\"‚úì Model already downloaded at: {snapshots[0]}\")\n",
    "        os.startfile(snapshots[0])\n",
    "    else:\n",
    "        print(\"‚úó Model directory exists but empty\")\n",
    "else:\n",
    "    print(f\"‚úó Model not downloaded yet - will download in next cell\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e2883d99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Model downloaded and loaded successfully!\n",
      "\n",
      "Model type: GPT2Model\n",
      "Config type: GPT2Config\n",
      "Tokenizer type: GPT2TokenizerFast\n",
      "\n",
      "Opening model files folder: C:\\Users\\Delulu Lemon\\.cache\\huggingface\\hub\\models--gpt2\\snapshots\\607a30d783dfa663caf39e06633721c8d4cfcd7e\n"
     ]
    }
   ],
   "source": [
    "# Download and load the tokenizer, config, and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "config = AutoConfig.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name)\n",
    "\n",
    "print(\"‚úì Model downloaded and loaded successfully!\")\n",
    "print(f\"\\nModel type: {type(model).__name__}\")\n",
    "print(f\"Config type: {type(config).__name__}\")\n",
    "print(f\"Tokenizer type: {type(tokenizer).__name__}\")\n",
    "\n",
    "# Open the model folder to see the downloaded files\n",
    "# (Reconstruct paths in case cell 4 wasn't run - cell 4 should be run first)\n",
    "try:\n",
    "    # Try to use variables from cell 4\n",
    "    _ = default_download_path\n",
    "    _ = model_name\n",
    "except NameError:\n",
    "    # If not available, reconstruct them\n",
    "    import os\n",
    "    from pathlib import Path\n",
    "    hf_home = os.environ.get('HF_HOME')\n",
    "    default_download_path = Path(hf_home) if hf_home else Path.home() / \".cache\" / \"huggingface\"\n",
    "    model_name = \"gpt2\"\n",
    "\n",
    "model_cache_subdir = default_download_path / \"hub\" / f\"models--{model_name.replace('/', '--')}\"\n",
    "snapshots_dir = model_cache_subdir / \"snapshots\"\n",
    "if snapshots_dir.exists():\n",
    "    snapshots = list(snapshots_dir.iterdir())\n",
    "    if snapshots:\n",
    "        model_files_dir = snapshots[0]\n",
    "        print(f\"\\nOpening model files folder: {model_files_dir}\")\n",
    "        os.startfile(model_files_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b9c5f27",
   "metadata": {},
   "source": [
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8d44a39",
   "metadata": {},
   "source": [
    "# Enumerate Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b898a129",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2 downloaded model(s):\n",
      "\n",
      "1. gpt2\n",
      "   Snapshots: 1\n",
      "   Total size: 525.44 MB\n",
      "   Latest: 607a30d783df...\n",
      "\n",
      "2. tiiuae/falcon-7b-instruct\n",
      "   Snapshots: 1\n",
      "   Total size: 13768.35 MB\n",
      "   Latest: 8782b5c5d8c9...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Enumerate all downloaded models\n",
    "try:\n",
    "    _ = default_download_path\n",
    "except NameError:\n",
    "    import os\n",
    "    from pathlib import Path\n",
    "    hf_home = os.environ.get('HF_HOME')\n",
    "    default_download_path = Path(hf_home) if hf_home else Path.home() / \".cache\" / \"huggingface\"\n",
    "\n",
    "hub_dir = default_download_path / \"hub\"\n",
    "\n",
    "if hub_dir.exists():\n",
    "    # Find all model directories (they start with \"models--\")\n",
    "    model_dirs = [d for d in hub_dir.iterdir() if d.is_dir() and d.name.startswith(\"models--\")]\n",
    "    \n",
    "    if model_dirs:\n",
    "        print(f\"Found {len(model_dirs)} downloaded model(s):\\n\")\n",
    "        \n",
    "        for i, model_dir in enumerate(sorted(model_dirs), 1):\n",
    "            # Extract model name (convert \"models--gpt2\" back to \"gpt2\")\n",
    "            model_name = model_dir.name.replace(\"models--\", \"\").replace(\"--\", \"/\")\n",
    "            \n",
    "            # Get snapshots\n",
    "            snapshots_dir = model_dir / \"snapshots\"\n",
    "            snapshots = []\n",
    "            if snapshots_dir.exists():\n",
    "                snapshots = list(snapshots_dir.iterdir())\n",
    "            \n",
    "            # Calculate total size\n",
    "            total_size = 0\n",
    "            if snapshots:\n",
    "                for snapshot in snapshots:\n",
    "                    for file in snapshot.rglob(\"*\"):\n",
    "                        if file.is_file():\n",
    "                            total_size += file.stat().st_size\n",
    "            \n",
    "            total_size_mb = total_size / (1024 * 1024)\n",
    "            \n",
    "            print(f\"{i}. {model_name}\")\n",
    "            print(f\"   Snapshots: {len(snapshots)}\")\n",
    "            print(f\"   Total size: {total_size_mb:.2f} MB\")\n",
    "            if snapshots:\n",
    "                print(f\"   Latest: {snapshots[0].name[:12]}...\")\n",
    "            print()\n",
    "    else:\n",
    "        print(\"No models found in cache directory\")\n",
    "else:\n",
    "    print(\"Cache directory not found or empty\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2590470",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fedc8ae",
   "metadata": {},
   "source": [
    "# Use Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "16141d9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What is capital of kansas?\n",
      "\n",
      "Answer:\n",
      "What is capital of kansas?\n",
      "\n",
      "Capital of kansas is defined as a capital amount in the local currency. Capital of kansas is calculated based on the number of people under the age of 18 years and the amount of cash in circulation.\n",
      "\n",
      "Where is the capital of kansas located?\n",
      "\n",
      "Capital of kansas is located in the town or region where the property is located.\n",
      "\n",
      "How much money has been deposited into the bank accounts of the bank?\n",
      "\n",
      "The total\n"
     ]
    }
   ],
   "source": [
    "# Load the language model (with language modeling head) for text generation\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# Always load the correct model type for text generation (AutoModelForCausalLM)\n",
    "# Cell 5 loads AutoModel (base model), but we need AutoModelForCausalLM for generation\n",
    "try:\n",
    "    _ = tokenizer\n",
    "except NameError:\n",
    "    model_name = \"gpt2\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Load the language model (with LM head) for text generation\n",
    "model_name = \"gpt2\"\n",
    "lm_model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "# Ask a question\n",
    "question = \"What is capital of kansas?\"\n",
    "\n",
    "print(f\"Question: {question}\\n\")\n",
    "print(\"Answer:\")\n",
    "\n",
    "# Tokenize the input\n",
    "inputs = tokenizer.encode(question, return_tensors=\"pt\")\n",
    "\n",
    "# Generate response\n",
    "outputs = lm_model.generate(\n",
    "    inputs,\n",
    "    max_length=100,\n",
    "    num_return_sequences=1,\n",
    "    temperature=0.7,\n",
    "    do_sample=True,\n",
    "    pad_token_id=tokenizer.eos_token_id\n",
    ")\n",
    "\n",
    "# Decode and print the response\n",
    "response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a1a4892",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e283bb00",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5454634b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e7a3e73",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f77efc04",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3026884d",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Understand which neurons got triggered"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d5679d5",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cddd2a16",
   "metadata": {},
   "source": [
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b64a5c3c",
   "metadata": {},
   "source": [
    "# Examine Model Files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4a3e91ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model folder hierarchy:\n",
      "\n",
      "‚îî‚îÄ‚îÄ üìÅ models--gpt2                                      <-- Model cache folder - contains all files for this model\n",
      "    ‚îú‚îÄ‚îÄ üìÅ .no_exist                                     <-- Symlink fallback folder - used when symlinks aren't supported on Windows\n",
      "    ‚îÇ   ‚îî‚îÄ‚îÄ üìÅ 607a30d783dfa663caf39e06633721c8d4cfcd7e  <-- Model snapshot (identified by commit hash)\n",
      "    ‚îÇ       ‚îú‚îÄ‚îÄ üìÑ added_tokens.json (0.00 MB)           <-- Added tokens - custom tokens added to the tokenizer\n",
      "    ‚îÇ       ‚îú‚îÄ‚îÄ üìÑ chat_template.jinja (0.00 MB)         <-- Chat template - template for formatting chat conversations\n",
      "    ‚îÇ       ‚îî‚îÄ‚îÄ üìÑ special_tokens_map.json (0.00 MB)     <-- Special tokens - mappings for special tokens (BOS, EOS, PAD, etc.)\n",
      "    ‚îú‚îÄ‚îÄ üìÅ blobs                                         <-- Blobs folder - stores deduplicated file content (Hugging Face cache optimization)\n",
      "    ‚îú‚îÄ‚îÄ üìÅ refs                                          <-- References folder - contains pointers to specific model versions (like git refs)\n",
      "    ‚îÇ   ‚îî‚îÄ‚îÄ üìÑ main (0.00 MB)                            <-- Reference file - points to the main/default model version\n",
      "    ‚îî‚îÄ‚îÄ üìÅ snapshots                                     <-- Snapshots folder - contains versioned model files (identified by commit hash)\n",
      "        ‚îî‚îÄ‚îÄ üìÅ 607a30d783dfa663caf39e06633721c8d4cfcd7e  <-- Model snapshot (identified by commit hash)\n",
      "            ‚îú‚îÄ‚îÄ üìÑ config.json (0.00 MB)                 <-- Model configuration - architecture, hyperparameters, and model settings\n",
      "            ‚îú‚îÄ‚îÄ üìÑ merges.txt (0.44 MB)                  <-- BPE merges - Byte Pair Encoding merge rules for tokenization\n",
      "            ‚îú‚îÄ‚îÄ üìÑ model.safetensors (522.71 MB)         <-- Model weights - the actual neural network parameters (SafeTensors format)\n",
      "            ‚îú‚îÄ‚îÄ üìÑ tokenizer.json (1.29 MB)              <-- Tokenizer data - complete tokenizer configuration and vocabulary\n",
      "            ‚îú‚îÄ‚îÄ üìÑ tokenizer_config.json (0.00 MB)       <-- Tokenizer config - tokenizer settings and special tokens\n",
      "            ‚îî‚îÄ‚îÄ üìÑ vocab.json (0.99 MB)                  <-- Vocabulary mapping - word/token to ID mappings\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    _ = default_download_path\n",
    "    _ = model_name\n",
    "except NameError:\n",
    "    import os\n",
    "    from pathlib import Path\n",
    "    hf_home = os.environ.get('HF_HOME')\n",
    "    default_download_path = Path(hf_home) if hf_home else Path.home() / \".cache\" / \"huggingface\"\n",
    "    model_name = \"gpt2\"\n",
    "\n",
    "# Descriptions for files and folders\n",
    "descriptions = {\n",
    "    # Folders\n",
    "    \"models--gpt2\": \"Model cache folder - contains all files for this model\",\n",
    "    \"snapshots\": \"Snapshots folder - contains versioned model files (identified by commit hash)\",\n",
    "    \"blobs\": \"Blobs folder - stores deduplicated file content (Hugging Face cache optimization)\",\n",
    "    \"refs\": \"References folder - contains pointers to specific model versions (like git refs)\",\n",
    "    \".no_exist\": \"Symlink fallback folder - used when symlinks aren't supported on Windows\",\n",
    "    \n",
    "    # Files\n",
    "    \"config.json\": \"Model configuration - architecture, hyperparameters, and model settings\",\n",
    "    \"model.safetensors\": \"Model weights - the actual neural network parameters (SafeTensors format)\",\n",
    "    \"pytorch_model.bin\": \"Model weights - alternative format (PyTorch binary, older format)\",\n",
    "    \"tokenizer.json\": \"Tokenizer data - complete tokenizer configuration and vocabulary\",\n",
    "    \"tokenizer_config.json\": \"Tokenizer config - tokenizer settings and special tokens\",\n",
    "    \"vocab.json\": \"Vocabulary mapping - word/token to ID mappings\",\n",
    "    \"merges.txt\": \"BPE merges - Byte Pair Encoding merge rules for tokenization\",\n",
    "    \"special_tokens_map.json\": \"Special tokens - mappings for special tokens (BOS, EOS, PAD, etc.)\",\n",
    "    \"added_tokens.json\": \"Added tokens - custom tokens added to the tokenizer\",\n",
    "    \"chat_template.jinja\": \"Chat template - template for formatting chat conversations\",\n",
    "    \"main\": \"Reference file - points to the main/default model version\",\n",
    "}\n",
    "\n",
    "def get_description(name):\n",
    "    \"\"\"Get description for a file or folder\"\"\"\n",
    "    # Check exact match first\n",
    "    if name in descriptions:\n",
    "        return f\" <-- {descriptions[name]}\"\n",
    "    # Check if it's a commit hash (long hex string)\n",
    "    if len(name) == 40 and all(c in '0123456789abcdef' for c in name.lower()):\n",
    "        return \" <-- Model snapshot (identified by commit hash)\"\n",
    "    return \"\"\n",
    "\n",
    "def collect_tree_items(path, prefix=\"\", is_last=True, items=None):\n",
    "    \"\"\"Collect all tree items with their display info\"\"\"\n",
    "    if items is None:\n",
    "        items = []\n",
    "    \n",
    "    name = path.name if path.name else str(path)\n",
    "    \n",
    "    if path.is_file():\n",
    "        size = path.stat().st_size\n",
    "        size_mb = size / (1024 * 1024)\n",
    "        size_str = f\"({size_mb:.2f} MB)\"\n",
    "        icon = \"üìÑ\"\n",
    "    else:\n",
    "        size_str = \"\"\n",
    "        icon = \"üìÅ\"\n",
    "    \n",
    "    description = get_description(name)\n",
    "    connector = \"‚îî‚îÄ‚îÄ \" if is_last else \"‚îú‚îÄ‚îÄ \"\n",
    "    \n",
    "    # Calculate the base display string (without description)\n",
    "    base_str = f\"{prefix}{connector}{icon} {name} {size_str}\"\n",
    "    items.append((base_str, description, path))\n",
    "    \n",
    "    if path.is_dir():\n",
    "        children = sorted(path.iterdir(), key=lambda x: (x.is_file(), x.name))\n",
    "        extension = \"    \" if is_last else \"‚îÇ   \"\n",
    "        new_prefix = prefix + extension\n",
    "        \n",
    "        for i, child in enumerate(children):\n",
    "            is_last_child = (i == len(children) - 1)\n",
    "            collect_tree_items(child, new_prefix, is_last_child, items)\n",
    "    \n",
    "    return items\n",
    "\n",
    "def print_tree(path, prefix=\"\", is_last=True):\n",
    "    \"\"\"Print directory tree with file sizes and descriptions (aligned arrows)\"\"\"\n",
    "    # First pass: collect all items\n",
    "    items = collect_tree_items(path, prefix, is_last)\n",
    "    \n",
    "    # Find maximum width for alignment\n",
    "    max_width = max(len(base_str) for base_str, _, _ in items)\n",
    "    \n",
    "    # Second pass: print with aligned arrows\n",
    "    for base_str, description, _ in items:\n",
    "        padded_base = base_str.ljust(max_width)\n",
    "        print(f\"{padded_base}{description}\")\n",
    "\n",
    "model_cache_path = default_download_path / \"hub\" / f\"models--{model_name.replace('/', '--')}\"\n",
    "snapshots_dir = model_cache_path / \"snapshots\"\n",
    "\n",
    "if snapshots_dir.exists():\n",
    "    snapshots = list(snapshots_dir.iterdir())\n",
    "    if snapshots:\n",
    "        print(f\"Model folder hierarchy:\\n\")\n",
    "        print_tree(model_cache_path)\n",
    "    else:\n",
    "        print(\"‚úó No snapshots found\")\n",
    "else:\n",
    "    print(\"‚úó Model not downloaded yet\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dac7226",
   "metadata": {},
   "source": [
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76ebe6d1",
   "metadata": {},
   "source": [
    "# Example Model Object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "528f14df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "MODEL CONFIGURATION\n",
      "============================================================\n",
      "{\n",
      "  \"vocab_size\": 50257,\n",
      "  \"n_positions\": 1024,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_layer\": 12,\n",
      "  \"n_head\": 12,\n",
      "  \"n_inner\": null,\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"scale_attn_weights\": true,\n",
      "  \"use_cache\": true,\n",
      "  \"scale_attn_by_inverse_layer_idx\": false,\n",
      "  \"reorder_and_upcast_attn\": false,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"return_dict\": true,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"torchscript\": false,\n",
      "  \"dtype\": null,\n",
      "  \"pruned_heads\": {},\n",
      "  \"tie_word_embeddings\": true,\n",
      "  \"chunk_size_feed_forward\": 0,\n",
      "  \"is_encoder_decoder\": false,\n",
      "  \"is_decoder\": false,\n",
      "  \"cross_attention_hidden_size\": null,\n",
      "  \"add_cross_attention\": false,\n",
      "  \"tie_encoder_decoder\": false,\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"finetuning_task\": null,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\"\n",
      "  },\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1\n",
      "  },\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 50\n",
      "    }\n",
      "  },\n",
      "  \"problem_type\": null,\n",
      "  \"tokenizer_class\": null,\n",
      "  \"prefix\": null,\n",
      "  \"pad_token_id\": null,\n",
      "  \"sep_token_id\": null,\n",
      "  \"decoder_start_token_id\": null,\n",
      "  \"max_length\": 20,\n",
      "  \"min_length\": 0,\n",
      "  \"do_sample\": false,\n",
      "  \"early_stopping\": false,\n",
      "  \"num_beams\": 1,\n",
      "  \"num_beam_groups\": 1,\n",
      "  \"diversity_penalty\": 0.0,\n",
      "  \"temperature\": 1.0,\n",
      "  \"top_k\": 50,\n",
      "  \"top_p\": 1.0,\n",
      "  \"typical_p\": 1.0,\n",
      "  \"repetition_penalty\": 1.0,\n",
      "  \"length_penalty\": 1.0,\n",
      "  \"no_repeat_ngram_size\": 0,\n",
      "  \"encoder_no_repeat_ngram_size\": 0,\n",
      "  \"bad_words_ids\": null,\n",
      "  \"num_return_sequences\": 1,\n",
      "  \"output_scores\": false,\n",
      "  \"return_dict_in_generate\": false,\n",
      "  \"forced_bos_token_id\": null,\n",
      "  \"forced_eos_token_id\": null,\n",
      "  \"remove_invalid_values\": false,\n",
      "  \"exponential_decay_length_penalty\": null,\n",
      "  \"suppress_tokens\": null,\n",
      "  \"begin_suppress_tokens\": null,\n",
      "  \"_name_or_path\": \"gpt2\",\n",
      "  \"transformers_version\": \"4.56.2\",\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 1024,\n",
      "  \"tf_legacy_loss\": false,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"output_attentions\": false\n",
      "}\n",
      "\n",
      "============================================================\n",
      "KEY MODEL PARAMETERS\n",
      "============================================================\n",
      "Vocabulary size: 50,257\n",
      "Hidden size: 768\n",
      "Number of attention heads: 12\n",
      "Number of layers: 12\n",
      "Max position embeddings: 1,024\n",
      "\n",
      "Total parameters: 124,439,808\n",
      "Trainable parameters: 124,439,808\n",
      "Model size (approx): 474.70 MB (assuming float32)\n"
     ]
    }
   ],
   "source": [
    "# Display model configuration\n",
    "print(\"=\" * 60)\n",
    "print(\"MODEL CONFIGURATION\")\n",
    "print(\"=\" * 60)\n",
    "print(json.dumps(config.to_dict(), indent=2))\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"KEY MODEL PARAMETERS\")\n",
    "print(\"=\" * 60)\n",
    "if hasattr(config, 'vocab_size'):\n",
    "    print(f\"Vocabulary size: {config.vocab_size:,}\")\n",
    "if hasattr(config, 'hidden_size'):\n",
    "    print(f\"Hidden size: {config.hidden_size:,}\")\n",
    "if hasattr(config, 'num_attention_heads'):\n",
    "    print(f\"Number of attention heads: {config.num_attention_heads}\")\n",
    "if hasattr(config, 'num_hidden_layers'):\n",
    "    print(f\"Number of layers: {config.num_hidden_layers}\")\n",
    "if hasattr(config, 'max_position_embeddings'):\n",
    "    print(f\"Max position embeddings: {config.max_position_embeddings:,}\")\n",
    "\n",
    "# Count total parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"\\nTotal parameters: {total_params:,}\")\n",
    "print(f\"Trainable parameters: {trainable_params:,}\")\n",
    "print(f\"Model size (approx): {total_params * 4 / (1024**2):.2f} MB (assuming float32)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "679678c0",
   "metadata": {},
   "source": [
    "---\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
