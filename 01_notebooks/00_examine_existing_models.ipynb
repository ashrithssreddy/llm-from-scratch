{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "91bccd93",
   "metadata": {},
   "source": [
    "##### 00_examine_existing_models.ipynb - Purpose\n",
    "This notebook is ONLY about examining existing pretrained models ‚Äî how they‚Äôre stored, what files they contain, and how they‚Äôre structured on disk.\n",
    "\n",
    "##### Goals\n",
    "- Download small pretrained models from Hugging Face.\n",
    "- Inspect the folder layout (tokenizer files, config, model weights).\n",
    "- Understand what each file does (.bin, .safetensors, config.json, tokenizer.json, etc.).\n",
    "- Load the model and print shapes of key components (embeddings, attention, MLP).\n",
    "- Get a practical feel for ‚Äúmodel anatomy‚Äù before building my own tiny version later.\n",
    "\n",
    "##### Scope (Important)\n",
    "- **NOT** learning how they're trained.  \n",
    "- **ONLY** file structure, weights, components, and practical inspection.\n",
    "\n",
    "##### Expected Outputs\n",
    "- Folder snapshots.\n",
    "- Breakdown of model components.\n",
    "- Parameter counts and shape summaries.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94e2f57f",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2273ba32",
   "metadata": {},
   "source": [
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e75f3ca4",
   "metadata": {},
   "source": [
    "# Download Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "882fa371",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Default download folder: C:\\Users\\Delulu Lemon\\.cache\\huggingface\n",
      "‚úì Model already downloaded at: C:\\Users\\Delulu Lemon\\.cache\\huggingface\\hub\\models--roneneldan--TinyStories-1M\\snapshots\\2ad005d9b1e727d4a7ed107816c44aff9f78935f\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModel, AutoTokenizer, AutoConfig\n",
    "from pathlib import Path\n",
    "import os\n",
    "import json\n",
    "\n",
    "# Get default cache directory\n",
    "hf_home = os.environ.get('HF_HOME')\n",
    "default_download_path = Path(hf_home) if hf_home else Path.home() / \".cache\" / \"huggingface\"\n",
    "print(f\"Default download folder: {default_download_path.absolute()}\")\n",
    "\n",
    "# Model to download\n",
    "# Toy + small model examples (uncomment ONE to use)\n",
    "\n",
    "# --- TINY MODELS (<100MB weights) ---\n",
    "# model_name = \"sshleifer/tiny-gpt2\"                     # ~15M params,  ~70MB  (best tiny GPT-2 for inspection)\n",
    "# model_name = \"roneneldan/TinyStories-1M\"               # ~1M params,   ~6MB   (ultra tiny, educational)\n",
    "# model_name = \"roneneldan/TinyStories-10M\"              # ~10M params,  ~30MB  (tiny decoder-only transformer)\n",
    "# model_name = \"EleutherAI/pythia-14m\"                   # ~14M params,  ~55MB  (tiny pythia baseline)\n",
    "# model_name = \"google/flan-t5-small\"                    # ~60M params,  ~240MB (encoder-decoder; small but different arch)\n",
    "\n",
    "# --- SMALL MODELS (~100M‚Äì200M weights) ---\n",
    "# model_name = \"distilgpt2\"                              # ~82M params,  ~320MB (distilled GPT2; great size/quality balance)\n",
    "# model_name = \"EleutherAI/pythia-70m\"                   # ~70M params,  ~280MB (tiny GPT-NeoX style arch)\n",
    "# model_name = \"gpt2\"                                    # ~124M params, ~500MB (baseline GPT-2 small)\n",
    "# model_name = \"facebook/opt-125m\"                       # ~125M params, ~500MB (OPT architecture)\n",
    "\n",
    "# --- MID-SMALL MODELS (>200M weights) ---\n",
    "# model_name = \"microsoft/DialoGPT-small\"                # ~117M params, ~470MB (GPT2 tuned for dialogue)\n",
    "# model_name = \"EleutherAI/pythia-160m\"                  # ~160M params, ~640MB (larger Pythia variant)\n",
    "# model_name = \"gpt2-medium\"                             # ~355M params, ~1.4GB (too big for deep inspection but realistic)\n",
    "\n",
    "\n",
    "model_name = \"roneneldan/TinyStories-1M\"\n",
    "\n",
    "# Check if model already exists\n",
    "model_cache_subdir = default_download_path / \"hub\" / f\"models--{model_name.replace('/', '--')}\"\n",
    "snapshots_dir = model_cache_subdir / \"snapshots\"\n",
    "\n",
    "if snapshots_dir.exists():\n",
    "    snapshots = list(snapshots_dir.iterdir())\n",
    "    if snapshots:\n",
    "        print(f\"‚úì Model already downloaded at: {snapshots[0]}\")\n",
    "        os.startfile(snapshots[0])\n",
    "    else:\n",
    "        print(\"‚úó Model directory exists but empty\")\n",
    "else:\n",
    "    print(f\"‚úó Model not downloaded yet - will download in next cell\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e2883d99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Model downloaded and loaded successfully!\n",
      "\n",
      "Model type: GPTNeoModel\n",
      "Config type: GPTNeoConfig\n",
      "Tokenizer type: GPT2TokenizerFast\n",
      "\n",
      "Opening model files folder: C:\\Users\\Delulu Lemon\\.cache\\huggingface\\hub\\models--roneneldan--TinyStories-1M\\snapshots\\2ad005d9b1e727d4a7ed107816c44aff9f78935f\n"
     ]
    }
   ],
   "source": [
    "# Download and load the tokenizer, config, and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "config = AutoConfig.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name)\n",
    "\n",
    "print(\"‚úì Model downloaded and loaded successfully!\")\n",
    "print(f\"\\nModel type: {type(model).__name__}\")\n",
    "print(f\"Config type: {type(config).__name__}\")\n",
    "print(f\"Tokenizer type: {type(tokenizer).__name__}\")\n",
    "\n",
    "# Open the model folder to see the downloaded files\n",
    "# (Reconstruct paths in case cell 4 wasn't run - cell 4 should be run first)\n",
    "try:\n",
    "    _ = default_download_path\n",
    "except NameError:\n",
    "    import os\n",
    "    from pathlib import Path\n",
    "    hf_home = os.environ.get('HF_HOME')\n",
    "    default_download_path = Path(hf_home) if hf_home else Path.home() / \".cache\" / \"huggingface\"\n",
    "\n",
    "try:\n",
    "    _ = model_name\n",
    "except NameError:\n",
    "    raise NameError(\"model_name not set. Please run cell 4 first or set model_name manually.\")\n",
    "\n",
    "model_cache_subdir = default_download_path / \"hub\" / f\"models--{model_name.replace('/', '--')}\"\n",
    "snapshots_dir = model_cache_subdir / \"snapshots\"\n",
    "if snapshots_dir.exists():\n",
    "    snapshots = list(snapshots_dir.iterdir())\n",
    "    if snapshots:\n",
    "        model_files_dir = snapshots[0]\n",
    "        print(f\"\\nOpening model files folder: {model_files_dir}\")\n",
    "        os.startfile(model_files_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b9c5f27",
   "metadata": {},
   "source": [
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8d44a39",
   "metadata": {},
   "source": [
    "# Enumerate Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b898a129",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3 downloaded model(s):\n",
      "\n",
      "1. gpt2\n",
      "   Snapshots: 1\n",
      "   Total size: 525.44 MB\n",
      "   Latest: 607a30d783df...\n",
      "\n",
      "2. roneneldan/TinyStories-1M\n",
      "   Snapshots: 2\n",
      "   Total size: 95.84 MB\n",
      "   Latest: 2ad005d9b1e7...\n",
      "\n",
      "3. tiiuae/falcon-7b-instruct\n",
      "   Snapshots: 1\n",
      "   Total size: 13768.35 MB\n",
      "   Latest: 8782b5c5d8c9...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Enumerate all downloaded models\n",
    "try:\n",
    "    _ = default_download_path\n",
    "except NameError:\n",
    "    import os\n",
    "    from pathlib import Path\n",
    "    hf_home = os.environ.get('HF_HOME')\n",
    "    default_download_path = Path(hf_home) if hf_home else Path.home() / \".cache\" / \"huggingface\"\n",
    "\n",
    "hub_dir = default_download_path / \"hub\"\n",
    "\n",
    "if hub_dir.exists():\n",
    "    # Find all model directories (they start with \"models--\")\n",
    "    model_dirs = [d for d in hub_dir.iterdir() if d.is_dir() and d.name.startswith(\"models--\")]\n",
    "    \n",
    "    if model_dirs:\n",
    "        print(f\"Found {len(model_dirs)} downloaded model(s):\\n\")\n",
    "        \n",
    "        for i, model_dir in enumerate(sorted(model_dirs), 1):\n",
    "            # Extract model name (convert \"models--gpt2\" back to \"gpt2\")\n",
    "            model_name = model_dir.name.replace(\"models--\", \"\").replace(\"--\", \"/\")\n",
    "            \n",
    "            # Get snapshots\n",
    "            snapshots_dir = model_dir / \"snapshots\"\n",
    "            snapshots = []\n",
    "            if snapshots_dir.exists():\n",
    "                snapshots = list(snapshots_dir.iterdir())\n",
    "            \n",
    "            # Calculate total size\n",
    "            total_size = 0\n",
    "            if snapshots:\n",
    "                for snapshot in snapshots:\n",
    "                    for file in snapshot.rglob(\"*\"):\n",
    "                        if file.is_file():\n",
    "                            total_size += file.stat().st_size\n",
    "            \n",
    "            total_size_mb = total_size / (1024 * 1024)\n",
    "            \n",
    "            print(f\"{i}. {model_name}\")\n",
    "            print(f\"   Snapshots: {len(snapshots)}\")\n",
    "            print(f\"   Total size: {total_size_mb:.2f} MB\")\n",
    "            if snapshots:\n",
    "                print(f\"   Latest: {snapshots[0].name[:12]}...\")\n",
    "            print()\n",
    "    else:\n",
    "        print(\"No models found in cache directory\")\n",
    "else:\n",
    "    print(\"Cache directory not found or empty\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2590470",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fedc8ae",
   "metadata": {},
   "source": [
    "# Use Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16141d9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2cc9e65402884e679b39d268ee5e3d36",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What is capital of kansas?\n",
      "\n",
      "Answer:\n"
     ]
    }
   ],
   "source": [
    "# Load the language model (with language modeling head) for text generation\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# Always load the correct model type for text generation (AutoModelForCausalLM)\n",
    "# Cell 5 loads AutoModel (base model), but we need AutoModelForCausalLM for generation\n",
    "try:\n",
    "    _ = model_name\n",
    "except NameError:\n",
    "    raise NameError(\"model_name not set. Please run cell 4 first or set model_name manually.\")\n",
    "\n",
    "try:\n",
    "    _ = tokenizer\n",
    "except NameError:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Load the language model (with LM head) for text generation\n",
    "lm_model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "# Ask a question\n",
    "question = \"What is capital of kansas?\"\n",
    "\n",
    "print(f\"Question: {question}\\n\")\n",
    "print(\"Answer:\")\n",
    "\n",
    "# Tokenize the input\n",
    "inputs = tokenizer.encode(question, return_tensors=\"pt\")\n",
    "\n",
    "# Generate response\n",
    "outputs = lm_model.generate(\n",
    "    inputs,\n",
    "    max_length=100,\n",
    "    num_return_sequences=1,\n",
    "    temperature=0.7,\n",
    "    do_sample=True,\n",
    "    pad_token_id=tokenizer.eos_token_id\n",
    ")\n",
    "\n",
    "# Decode and print the response\n",
    "response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a1a4892",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e283bb00",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5454634b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e7a3e73",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f77efc04",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3026884d",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Understand which neurons got triggered"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d5679d5",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cddd2a16",
   "metadata": {},
   "source": [
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b64a5c3c",
   "metadata": {},
   "source": [
    "# Examine Model Files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a3e91ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    _ = default_download_path\n",
    "except NameError:\n",
    "    import os\n",
    "    from pathlib import Path\n",
    "    hf_home = os.environ.get('HF_HOME')\n",
    "    default_download_path = Path(hf_home) if hf_home else Path.home() / \".cache\" / \"huggingface\"\n",
    "\n",
    "try:\n",
    "    _ = model_name\n",
    "except NameError:\n",
    "    raise NameError(\"model_name not set. Please run cell 4 first or set model_name manually.\")\n",
    "\n",
    "# Descriptions for files and folders\n",
    "descriptions = {\n",
    "    # Folders (generic - will match any model folder)\n",
    "    \"snapshots\": \"Snapshots folder - contains versioned model files (identified by commit hash)\",\n",
    "    \"blobs\": \"Blobs folder - stores deduplicated file content (Hugging Face cache optimization)\",\n",
    "    \"refs\": \"References folder - contains pointers to specific model versions (like git refs)\",\n",
    "    \".no_exist\": \"Symlink fallback folder - used when symlinks aren't supported on Windows\",\n",
    "    \n",
    "    # Files\n",
    "    \"config.json\": \"Model configuration - architecture, hyperparameters, and model settings\",\n",
    "    \"model.safetensors\": \"Model weights - the actual neural network parameters (SafeTensors format)\",\n",
    "    \"pytorch_model.bin\": \"Model weights - alternative format (PyTorch binary, older format)\",\n",
    "    \"tokenizer.json\": \"Tokenizer data - complete tokenizer configuration and vocabulary\",\n",
    "    \"tokenizer_config.json\": \"Tokenizer config - tokenizer settings and special tokens\",\n",
    "    \"vocab.json\": \"Vocabulary mapping - word/token to ID mappings\",\n",
    "    \"merges.txt\": \"BPE merges - Byte Pair Encoding merge rules for tokenization\",\n",
    "    \"special_tokens_map.json\": \"Special tokens - mappings for special tokens (BOS, EOS, PAD, etc.)\",\n",
    "    \"added_tokens.json\": \"Added tokens - custom tokens added to the tokenizer\",\n",
    "    \"chat_template.jinja\": \"Chat template - template for formatting chat conversations\",\n",
    "    \"main\": \"Reference file - points to the main/default model version\",\n",
    "}\n",
    "\n",
    "def get_description(name):\n",
    "    \"\"\"Get description for a file or folder\"\"\"\n",
    "    # Check if it's a model folder (starts with \"models--\")\n",
    "    if name.startswith(\"models--\"):\n",
    "        model_display_name = name.replace(\"models--\", \"\").replace(\"--\", \"/\")\n",
    "        return f\" <-- Model cache folder for '{model_display_name}' - contains all files for this model\"\n",
    "    \n",
    "    # Check exact match first\n",
    "    if name in descriptions:\n",
    "        return f\" <-- {descriptions[name]}\"\n",
    "    \n",
    "    # Check if it's a commit hash (long hex string)\n",
    "    if len(name) == 40 and all(c in '0123456789abcdef' for c in name.lower()):\n",
    "        return \" <-- Model snapshot (identified by commit hash)\"\n",
    "    \n",
    "    return \"\"\n",
    "\n",
    "def collect_tree_items(path, prefix=\"\", is_last=True, items=None):\n",
    "    \"\"\"Collect all tree items with their display info\"\"\"\n",
    "    if items is None:\n",
    "        items = []\n",
    "    \n",
    "    name = path.name if path.name else str(path)\n",
    "    \n",
    "    if path.is_file():\n",
    "        size = path.stat().st_size\n",
    "        size_mb = size / (1024 * 1024)\n",
    "        size_str = f\"({size_mb:.2f} MB)\"\n",
    "        icon = \"üìÑ\"\n",
    "    else:\n",
    "        size_str = \"\"\n",
    "        icon = \"üìÅ\"\n",
    "    \n",
    "    description = get_description(name)\n",
    "    connector = \"‚îî‚îÄ‚îÄ \" if is_last else \"‚îú‚îÄ‚îÄ \"\n",
    "    \n",
    "    # Calculate the base display string (without description)\n",
    "    base_str = f\"{prefix}{connector}{icon} {name} {size_str}\"\n",
    "    items.append((base_str, description, path))\n",
    "    \n",
    "    if path.is_dir():\n",
    "        children = sorted(path.iterdir(), key=lambda x: (x.is_file(), x.name))\n",
    "        extension = \"    \" if is_last else \"‚îÇ   \"\n",
    "        new_prefix = prefix + extension\n",
    "        \n",
    "        for i, child in enumerate(children):\n",
    "            is_last_child = (i == len(children) - 1)\n",
    "            collect_tree_items(child, new_prefix, is_last_child, items)\n",
    "    \n",
    "    return items\n",
    "\n",
    "def print_tree(path, prefix=\"\", is_last=True):\n",
    "    \"\"\"Print directory tree with file sizes and descriptions (aligned arrows)\"\"\"\n",
    "    # First pass: collect all items\n",
    "    items = collect_tree_items(path, prefix, is_last)\n",
    "    \n",
    "    # Find maximum width for alignment\n",
    "    max_width = max(len(base_str) for base_str, _, _ in items)\n",
    "    \n",
    "    # Second pass: print with aligned arrows\n",
    "    for base_str, description, _ in items:\n",
    "        padded_base = base_str.ljust(max_width)\n",
    "        print(f\"{padded_base}{description}\")\n",
    "\n",
    "model_cache_path = default_download_path / \"hub\" / f\"models--{model_name.replace('/', '--')}\"\n",
    "snapshots_dir = model_cache_path / \"snapshots\"\n",
    "\n",
    "if snapshots_dir.exists():\n",
    "    snapshots = list(snapshots_dir.iterdir())\n",
    "    if snapshots:\n",
    "        print(f\"Model folder hierarchy:\\n\")\n",
    "        print_tree(model_cache_path)\n",
    "    else:\n",
    "        print(\"‚úó No snapshots found\")\n",
    "else:\n",
    "    print(\"‚úó Model not downloaded yet\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dac7226",
   "metadata": {},
   "source": [
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76ebe6d1",
   "metadata": {},
   "source": [
    "# Example Model Object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "528f14df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display model configuration\n",
    "print(\"=\" * 60)\n",
    "print(\"MODEL CONFIGURATION\")\n",
    "print(\"=\" * 60)\n",
    "print(json.dumps(config.to_dict(), indent=2))\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"KEY MODEL PARAMETERS\")\n",
    "print(\"=\" * 60)\n",
    "if hasattr(config, 'vocab_size'):\n",
    "    print(f\"Vocabulary size: {config.vocab_size:,}\")\n",
    "if hasattr(config, 'hidden_size'):\n",
    "    print(f\"Hidden size: {config.hidden_size:,}\")\n",
    "if hasattr(config, 'num_attention_heads'):\n",
    "    print(f\"Number of attention heads: {config.num_attention_heads}\")\n",
    "if hasattr(config, 'num_hidden_layers'):\n",
    "    print(f\"Number of layers: {config.num_hidden_layers}\")\n",
    "if hasattr(config, 'max_position_embeddings'):\n",
    "    print(f\"Max position embeddings: {config.max_position_embeddings:,}\")\n",
    "\n",
    "# Count total parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"\\nTotal parameters: {total_params:,}\")\n",
    "print(f\"Trainable parameters: {trainable_params:,}\")\n",
    "print(f\"Model size (approx): {total_params * 4 / (1024**2):.2f} MB (assuming float32)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "679678c0",
   "metadata": {},
   "source": [
    "---\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
