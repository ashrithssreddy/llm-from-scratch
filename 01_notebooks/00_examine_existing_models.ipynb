{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "91bccd93",
   "metadata": {},
   "source": [
    "##### 00_examine_existing_models.ipynb - Purpose\n",
    "This notebook is ONLY about examining existing pretrained models ‚Äî how they‚Äôre stored, what files they contain, and how they‚Äôre structured on disk.\n",
    "\n",
    "##### Goals\n",
    "- Download small pretrained models from Hugging Face.\n",
    "- Inspect the folder layout (tokenizer files, config, model weights).\n",
    "- Understand what each file does (.bin, .safetensors, config.json, tokenizer.json, etc.).\n",
    "- Load the model and print shapes of key components (embeddings, attention, MLP).\n",
    "- Get a practical feel for ‚Äúmodel anatomy‚Äù before building my own tiny version later.\n",
    "\n",
    "##### Scope (Important)\n",
    "- **NOT** learning how they're trained.  \n",
    "- **ONLY** file structure, weights, components, and practical inspection.\n",
    "\n",
    "##### Expected Outputs\n",
    "- Folder snapshots.\n",
    "- Breakdown of model components.\n",
    "- Parameter counts and shape summaries.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94e2f57f",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2273ba32",
   "metadata": {},
   "source": [
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e75f3ca4",
   "metadata": {},
   "source": [
    "# Download Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "882fa371",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Default download folder: C:\\Users\\Delulu Lemon\\.cache\\huggingface\n",
      "‚úì Model already downloaded at: C:\\Users\\Delulu Lemon\\.cache\\huggingface\\hub\\models--gpt2\\snapshots\\607a30d783dfa663caf39e06633721c8d4cfcd7e\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModel, AutoTokenizer, AutoConfig\n",
    "from pathlib import Path\n",
    "import os\n",
    "import json\n",
    "\n",
    "# Get default cache directory\n",
    "hf_home = os.environ.get('HF_HOME')\n",
    "default_download_path = Path(hf_home) if hf_home else Path.home() / \".cache\" / \"huggingface\"\n",
    "print(f\"Default download folder: {default_download_path.absolute()}\")\n",
    "\n",
    "# Model to download\n",
    "model_name = \"gpt2\"\n",
    "\n",
    "# Check if model already exists\n",
    "model_cache_subdir = default_download_path / \"hub\" / f\"models--{model_name.replace('/', '--')}\"\n",
    "snapshots_dir = model_cache_subdir / \"snapshots\"\n",
    "\n",
    "if snapshots_dir.exists():\n",
    "    snapshots = list(snapshots_dir.iterdir())\n",
    "    if snapshots:\n",
    "        print(f\"‚úì Model already downloaded at: {snapshots[0]}\")\n",
    "        os.startfile(snapshots[0])\n",
    "    else:\n",
    "        print(\"‚úó Model directory exists but empty\")\n",
    "else:\n",
    "    print(f\"‚úó Model not downloaded yet - will download in next cell\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e2883d99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Model downloaded and loaded successfully!\n",
      "\n",
      "Model type: GPT2Model\n",
      "Config type: GPT2Config\n",
      "Tokenizer type: GPT2TokenizerFast\n",
      "\n",
      "Opening model files folder: C:\\Users\\Delulu Lemon\\.cache\\huggingface\\hub\\models--gpt2\\snapshots\\607a30d783dfa663caf39e06633721c8d4cfcd7e\n"
     ]
    }
   ],
   "source": [
    "# Download and load the tokenizer, config, and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "config = AutoConfig.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name)\n",
    "\n",
    "print(\"‚úì Model downloaded and loaded successfully!\")\n",
    "print(f\"\\nModel type: {type(model).__name__}\")\n",
    "print(f\"Config type: {type(config).__name__}\")\n",
    "print(f\"Tokenizer type: {type(tokenizer).__name__}\")\n",
    "\n",
    "# Open the model folder to see the downloaded files\n",
    "# (Reconstruct paths in case cell 4 wasn't run - cell 4 should be run first)\n",
    "try:\n",
    "    # Try to use variables from cell 4\n",
    "    _ = default_download_path\n",
    "    _ = model_name\n",
    "except NameError:\n",
    "    # If not available, reconstruct them\n",
    "    import os\n",
    "    from pathlib import Path\n",
    "    hf_home = os.environ.get('HF_HOME')\n",
    "    default_download_path = Path(hf_home) if hf_home else Path.home() / \".cache\" / \"huggingface\"\n",
    "    model_name = \"gpt2\"\n",
    "\n",
    "model_cache_subdir = default_download_path / \"hub\" / f\"models--{model_name.replace('/', '--')}\"\n",
    "snapshots_dir = model_cache_subdir / \"snapshots\"\n",
    "if snapshots_dir.exists():\n",
    "    snapshots = list(snapshots_dir.iterdir())\n",
    "    if snapshots:\n",
    "        model_files_dir = snapshots[0]\n",
    "        print(f\"\\nOpening model files folder: {model_files_dir}\")\n",
    "        os.startfile(model_files_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b9c5f27",
   "metadata": {},
   "source": [
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fedc8ae",
   "metadata": {},
   "source": [
    "# Use Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d5679d5",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a1a4892",
   "metadata": {},
   "source": [
    "# Enumerate Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cddd2a16",
   "metadata": {},
   "source": [
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b64a5c3c",
   "metadata": {},
   "source": [
    "# Examine Model Files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4a3e91ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model folder hierarchy:\n",
      "\n",
      "‚îî‚îÄ‚îÄ üìÅ models--gpt2 \n",
      "    ‚îú‚îÄ‚îÄ üìÅ .no_exist \n",
      "    ‚îÇ   ‚îî‚îÄ‚îÄ üìÅ 607a30d783dfa663caf39e06633721c8d4cfcd7e \n",
      "    ‚îÇ       ‚îú‚îÄ‚îÄ üìÑ added_tokens.json (0.00 MB)\n",
      "    ‚îÇ       ‚îú‚îÄ‚îÄ üìÑ chat_template.jinja (0.00 MB)\n",
      "    ‚îÇ       ‚îî‚îÄ‚îÄ üìÑ special_tokens_map.json (0.00 MB)\n",
      "    ‚îú‚îÄ‚îÄ üìÅ blobs \n",
      "    ‚îú‚îÄ‚îÄ üìÅ refs \n",
      "    ‚îÇ   ‚îî‚îÄ‚îÄ üìÑ main (0.00 MB)\n",
      "    ‚îî‚îÄ‚îÄ üìÅ snapshots \n",
      "        ‚îî‚îÄ‚îÄ üìÅ 607a30d783dfa663caf39e06633721c8d4cfcd7e \n",
      "            ‚îú‚îÄ‚îÄ üìÑ config.json (0.00 MB)\n",
      "            ‚îú‚îÄ‚îÄ üìÑ merges.txt (0.44 MB)\n",
      "            ‚îú‚îÄ‚îÄ üìÑ model.safetensors (522.71 MB)\n",
      "            ‚îú‚îÄ‚îÄ üìÑ tokenizer.json (1.29 MB)\n",
      "            ‚îú‚îÄ‚îÄ üìÑ tokenizer_config.json (0.00 MB)\n",
      "            ‚îî‚îÄ‚îÄ üìÑ vocab.json (0.99 MB)\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    _ = default_download_path\n",
    "    _ = model_name\n",
    "except NameError:\n",
    "    import os\n",
    "    from pathlib import Path\n",
    "    hf_home = os.environ.get('HF_HOME')\n",
    "    default_download_path = Path(hf_home) if hf_home else Path.home() / \".cache\" / \"huggingface\"\n",
    "    model_name = \"gpt2\"\n",
    "\n",
    "def print_tree(path, prefix=\"\", is_last=True):\n",
    "    \"\"\"Print directory tree with file sizes\"\"\"\n",
    "    name = path.name if path.name else str(path)\n",
    "    \n",
    "    if path.is_file():\n",
    "        size = path.stat().st_size\n",
    "        size_mb = size / (1024 * 1024)\n",
    "        size_str = f\"({size_mb:.2f} MB)\"\n",
    "        icon = \"üìÑ\"\n",
    "    else:\n",
    "        size_str = \"\"\n",
    "        icon = \"üìÅ\"\n",
    "    \n",
    "    connector = \"‚îî‚îÄ‚îÄ \" if is_last else \"‚îú‚îÄ‚îÄ \"\n",
    "    print(f\"{prefix}{connector}{icon} {name} {size_str}\")\n",
    "    \n",
    "    if path.is_dir():\n",
    "        children = sorted(path.iterdir(), key=lambda x: (x.is_file(), x.name))\n",
    "        extension = \"    \" if is_last else \"‚îÇ   \"\n",
    "        new_prefix = prefix + extension\n",
    "        \n",
    "        for i, child in enumerate(children):\n",
    "            is_last_child = (i == len(children) - 1)\n",
    "            print_tree(child, new_prefix, is_last_child)\n",
    "\n",
    "model_cache_path = default_download_path / \"hub\" / f\"models--{model_name.replace('/', '--')}\"\n",
    "snapshots_dir = model_cache_path / \"snapshots\"\n",
    "\n",
    "if snapshots_dir.exists():\n",
    "    snapshots = list(snapshots_dir.iterdir())\n",
    "    if snapshots:\n",
    "        print(f\"Model folder hierarchy:\\n\")\n",
    "        print_tree(model_cache_path)\n",
    "    else:\n",
    "        print(\"‚úó No snapshots found\")\n",
    "else:\n",
    "    print(\"‚úó Model not downloaded yet\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dac7226",
   "metadata": {},
   "source": [
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76ebe6d1",
   "metadata": {},
   "source": [
    "# Example Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "528f14df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "MODEL CONFIGURATION\n",
      "============================================================\n",
      "{\n",
      "  \"vocab_size\": 50257,\n",
      "  \"n_positions\": 1024,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_layer\": 12,\n",
      "  \"n_head\": 12,\n",
      "  \"n_inner\": null,\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"scale_attn_weights\": true,\n",
      "  \"use_cache\": true,\n",
      "  \"scale_attn_by_inverse_layer_idx\": false,\n",
      "  \"reorder_and_upcast_attn\": false,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"return_dict\": true,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"torchscript\": false,\n",
      "  \"dtype\": null,\n",
      "  \"pruned_heads\": {},\n",
      "  \"tie_word_embeddings\": true,\n",
      "  \"chunk_size_feed_forward\": 0,\n",
      "  \"is_encoder_decoder\": false,\n",
      "  \"is_decoder\": false,\n",
      "  \"cross_attention_hidden_size\": null,\n",
      "  \"add_cross_attention\": false,\n",
      "  \"tie_encoder_decoder\": false,\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"finetuning_task\": null,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\"\n",
      "  },\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1\n",
      "  },\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 50\n",
      "    }\n",
      "  },\n",
      "  \"problem_type\": null,\n",
      "  \"tokenizer_class\": null,\n",
      "  \"prefix\": null,\n",
      "  \"pad_token_id\": null,\n",
      "  \"sep_token_id\": null,\n",
      "  \"decoder_start_token_id\": null,\n",
      "  \"max_length\": 20,\n",
      "  \"min_length\": 0,\n",
      "  \"do_sample\": false,\n",
      "  \"early_stopping\": false,\n",
      "  \"num_beams\": 1,\n",
      "  \"num_beam_groups\": 1,\n",
      "  \"diversity_penalty\": 0.0,\n",
      "  \"temperature\": 1.0,\n",
      "  \"top_k\": 50,\n",
      "  \"top_p\": 1.0,\n",
      "  \"typical_p\": 1.0,\n",
      "  \"repetition_penalty\": 1.0,\n",
      "  \"length_penalty\": 1.0,\n",
      "  \"no_repeat_ngram_size\": 0,\n",
      "  \"encoder_no_repeat_ngram_size\": 0,\n",
      "  \"bad_words_ids\": null,\n",
      "  \"num_return_sequences\": 1,\n",
      "  \"output_scores\": false,\n",
      "  \"return_dict_in_generate\": false,\n",
      "  \"forced_bos_token_id\": null,\n",
      "  \"forced_eos_token_id\": null,\n",
      "  \"remove_invalid_values\": false,\n",
      "  \"exponential_decay_length_penalty\": null,\n",
      "  \"suppress_tokens\": null,\n",
      "  \"begin_suppress_tokens\": null,\n",
      "  \"_name_or_path\": \"gpt2\",\n",
      "  \"transformers_version\": \"4.56.2\",\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 1024,\n",
      "  \"tf_legacy_loss\": false,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"output_attentions\": false\n",
      "}\n",
      "\n",
      "============================================================\n",
      "KEY MODEL PARAMETERS\n",
      "============================================================\n",
      "Vocabulary size: 50,257\n",
      "Hidden size: 768\n",
      "Number of attention heads: 12\n",
      "Number of layers: 12\n",
      "Max position embeddings: 1,024\n",
      "\n",
      "Total parameters: 124,439,808\n",
      "Trainable parameters: 124,439,808\n",
      "Model size (approx): 474.70 MB (assuming float32)\n"
     ]
    }
   ],
   "source": [
    "# Display model configuration\n",
    "print(\"=\" * 60)\n",
    "print(\"MODEL CONFIGURATION\")\n",
    "print(\"=\" * 60)\n",
    "print(json.dumps(config.to_dict(), indent=2))\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"KEY MODEL PARAMETERS\")\n",
    "print(\"=\" * 60)\n",
    "if hasattr(config, 'vocab_size'):\n",
    "    print(f\"Vocabulary size: {config.vocab_size:,}\")\n",
    "if hasattr(config, 'hidden_size'):\n",
    "    print(f\"Hidden size: {config.hidden_size:,}\")\n",
    "if hasattr(config, 'num_attention_heads'):\n",
    "    print(f\"Number of attention heads: {config.num_attention_heads}\")\n",
    "if hasattr(config, 'num_hidden_layers'):\n",
    "    print(f\"Number of layers: {config.num_hidden_layers}\")\n",
    "if hasattr(config, 'max_position_embeddings'):\n",
    "    print(f\"Max position embeddings: {config.max_position_embeddings:,}\")\n",
    "\n",
    "# Count total parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"\\nTotal parameters: {total_params:,}\")\n",
    "print(f\"Trainable parameters: {trainable_params:,}\")\n",
    "print(f\"Model size (approx): {total_params * 4 / (1024**2):.2f} MB (assuming float32)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "679678c0",
   "metadata": {},
   "source": [
    "---\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
