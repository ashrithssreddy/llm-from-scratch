# Encoding (stoi/itos):

Encoding is the step that follows tokenization in the LLM pipeline. After text is split into tokens, those tokens need to be converted into a numerical format so the model can understand and process them. This transformation is called **encoding**.

---

### ðŸ”µ What Is Encoding?

Encoding is the process of **mapping each token to an integer ID**. Neural networks cannot operate on text directlyâ€”they only work with numbersâ€”so encoding is essential.

In practice, encoding uses two key mappings:

* **stoi** â†’ stringâ€‘toâ€‘index
* **itos** â†’ indexâ€‘toâ€‘string (reverse)

These mappings together form the vocabulary of your tokenizer.

---

### ðŸŸ© stoi: String â†’ Index

This mapping assigns each token a unique integer.

**Example:** Suppose your tokenizer produced these tokens:

```
["h", "e", "l", "l", "o"]
```

You might create this mapping:

```
stoi = {
  "h": 0,
  "e": 1,
  "l": 2,
  "o": 3
}
```

Now encoding becomes:

```
"hello" â†’ [0, 1, 2, 2, 3]
```

---

### ðŸŸ§ itos: Index â†’ String

This is the reverse mapping.

```
itos = {
  0: "h",
  1: "e",
  2: "l",
  3: "o"
}
```

Decoding works like this:

```
[0, 1, 2, 2, 3] â†’ "hello"
```

---

### ðŸ›  A Full Practical Example

Imagine the input text is:

```
"I love pizza!"
```

##### Step 1: Tokenization

Wordâ€‘level example:

```
["I", "love", "pizza", "!"]
```

##### Step 2: Build stoi

```
stoi = {
  "I": 0,
  "love": 1,
  "pizza": 2,
  "!": 3
}
```

##### Step 3: Encode

```
[0, 1, 2, 3]
```

##### Step 4: Decode (using itos)

```
"I love pizza!"
```

---

### ðŸ”¢ Encoding in Real LLMs

Modern LLMs do not manually craft these dictionaries. Instead:

* GPT models use **BPE tokenizers** with prebuilt vocabularies
* Each subword piece has a stable, published integer ID
Example (GPTâ€‘2):

```
"playing" â†’ ["play", "ing"] â†’ [1327, 352]
```

Below are the standard publicly available vocabularies and merge rules used by wellâ€‘known models:

* **GPTâ€‘2 Vocabulary (vocab.json)** â€” [https://huggingface.co/openai-community/gpt2/blob/main/vocab.json](https://huggingface.co/openai-community/gpt2/blob/main/vocab.json)
* **GPTâ€‘2 BPE Merge Rules (merges.txt)** â€” [https://huggingface.co/openai-community/gpt2/blob/main/merges.txt](https://huggingface.co/openai-community/gpt2/blob/main/merges.txt)
* **GPTâ€‘Neo Vocabulary** â€” [https://huggingface.co/EleutherAI/gpt-neo-1.3B/blob/main/vocab.json](https://huggingface.co/EleutherAI/gpt-neo-1.3B/blob/main/vocab.json)
* **GPTâ€‘Neo Merge Rules** â€” [https://huggingface.co/EleutherAI/gpt-neo-1.3B/blob/main/merges.txt](https://huggingface.co/EleutherAI/gpt-neo-1.3B/blob/main/merges.txt)
* **LLaMA SentencePiece Model (tokenizer.model)** â€” [https://huggingface.co/meta-llama/Llama-2-7b-hf/blob/main/tokenizer.model](https://huggingface.co/meta-llama/Llama-2-7b-hf/blob/main/tokenizer.model)
* **LLaMA Vocabulary (tokenizer.json)** â€” [https://huggingface.co/meta-llama/Llama-2-7b-hf/blob/main/tokenizer.json](https://huggingface.co/meta-llama/Llama-2-7b-hf/blob/main/tokenizer.json)

These are the exact files used during training of those models. All IDs and tokens match the model weights.

---

### ðŸŸ¦ Why Encoding Matters

Encoding:

* Translates humanâ€‘readable text into modelâ€‘readable numbers
* Ensures consistency in how tokens are represented
* Allows fast lookup during training
* Defines the size of the embedding layer and logits output

---
