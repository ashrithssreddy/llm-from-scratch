2025-11-30 13:06:23 - INFO - 
2025-11-30 13:06:23 - INFO - ================================================================================
2025-11-30 13:06:23 - INFO - TRAINING SESSION STARTED
2025-11-30 13:06:23 - INFO - ================================================================================
2025-11-30 13:06:23 - INFO - 
2025-11-30 13:06:23 - INFO - Log file: 97_logs\training_20251130_130623.log
2025-11-30 13:06:23 - INFO - Timestamp: 20251130_130623
2025-11-30 13:06:23 - INFO - 
2025-11-30 13:06:23 - INFO - Command line arguments parsed
2025-11-30 13:06:23 - INFO - 
2025-11-30 13:06:23 - INFO - 
2025-11-30 13:06:23 - INFO - ================================================================================
2025-11-30 13:06:23 - INFO - TRAINING CONFIGURATION
2025-11-30 13:06:23 - INFO - ================================================================================
2025-11-30 13:06:23 - INFO - 
2025-11-30 13:06:23 - INFO - Training Parameters:
2025-11-30 13:06:23 - INFO -   - Dataset folder: 40_training_data/dataset_toy/
2025-11-30 13:06:23 - INFO -   - Total epochs: 10
2025-11-30 13:06:23 - INFO -   - Batch size: 32
2025-11-30 13:06:23 - INFO -   - Sequence length (block_size): 128
2025-11-30 13:06:23 - INFO -   - Embedding dimension: 128
2025-11-30 13:06:23 - INFO -   - Attention heads: 4
2025-11-30 13:06:23 - INFO -   - Transformer layers: 3
2025-11-30 13:06:23 - INFO -   - Learning rate: 0.001
2025-11-30 13:06:23 - INFO - 
2025-11-30 13:06:23 - INFO - ================================================================================
2025-11-30 13:06:23 - INFO - STEP 1: DEVICE SELECTION
2025-11-30 13:06:23 - INFO - ================================================================================
2025-11-30 13:06:23 - INFO - 
2025-11-30 13:06:23 - INFO - Selected device: cpu
2025-11-30 13:06:23 - INFO -   - Using CPU (no CUDA available)
2025-11-30 13:06:23 - INFO -   - Time taken: 0.001 seconds
2025-11-30 13:06:23 - INFO - 
2025-11-30 13:06:23 - INFO - ================================================================================
2025-11-30 13:06:23 - INFO - STEP 2: DATA LOADING
2025-11-30 13:06:23 - INFO - ================================================================================
2025-11-30 13:06:23 - INFO - 
2025-11-30 13:06:23 - INFO - Loading all .txt files from: 40_training_data/dataset_toy/
2025-11-30 13:06:23 - INFO - 
2025-11-30 13:06:23 - INFO - Data loaded successfully:
2025-11-30 13:06:23 - INFO -   - Total characters: 5,178
2025-11-30 13:06:23 - INFO -   - Total lines: 43
2025-11-30 13:06:23 - INFO -   - Time taken: 0.002 seconds
2025-11-30 13:06:23 - INFO - 
2025-11-30 13:06:23 - INFO - ================================================================================
2025-11-30 13:06:23 - INFO - STEP 3: DATASET PREPARATION
2025-11-30 13:06:23 - INFO - ================================================================================
2025-11-30 13:06:23 - INFO - 
2025-11-30 13:06:23 - INFO - Creating character-level dataset...
2025-11-30 13:06:23 - INFO -   - Processing text into sequences of length 128
2025-11-30 13:06:23 - DEBUG - Creating character-level dataset...
2025-11-30 13:06:23 - DEBUG -   - Input text length: 5178 characters
2025-11-30 13:06:23 - DEBUG -   - Block size: 128
2025-11-30 13:06:23 - DEBUG -   - Vocabulary size: 48
2025-11-30 13:06:23 - DEBUG -   - Unique characters: 
 ',-.ABCDEFHILMNPRSTWabcdefghijklmnopqrstuvwxyz
2025-11-30 13:06:23 - DEBUG -   - Encoding text to integer sequences...
2025-11-30 13:06:23 - DEBUG -   - Encoded data length: 5178 tokens
2025-11-30 13:06:23 - INFO - 
2025-11-30 13:06:23 - INFO - Dataset created:
2025-11-30 13:06:23 - INFO -   - Vocabulary size: 48 unique characters
2025-11-30 13:06:23 - INFO -   - Total training sequences: 5,050
2025-11-30 13:06:23 - INFO -   - Total tokens: 5,178
2025-11-30 13:06:23 - INFO -   - Time taken: 0.003 seconds
2025-11-30 13:06:23 - INFO - 
2025-11-30 13:06:23 - INFO - ================================================================================
2025-11-30 13:06:23 - INFO - STEP 4: DATALOADER SETUP
2025-11-30 13:06:23 - INFO - ================================================================================
2025-11-30 13:06:23 - INFO - 
2025-11-30 13:06:23 - INFO - Creating DataLoader with batch_size=32 (shuffling enabled)...
2025-11-30 13:06:23 - INFO -   - Total batches per epoch: 158
2025-11-30 13:06:23 - INFO -   - Samples per batch: 32
2025-11-30 13:06:23 - INFO -   - Time taken: 0.001 seconds
2025-11-30 13:06:23 - INFO - 
2025-11-30 13:06:23 - INFO - ================================================================================
2025-11-30 13:06:23 - INFO - STEP 5: MODEL INITIALIZATION
2025-11-30 13:06:23 - INFO - ================================================================================
2025-11-30 13:06:23 - INFO - 
2025-11-30 13:06:23 - INFO - Building transformer-based language model...
2025-11-30 13:06:23 - INFO -   - Architecture: 3 transformer layers
2025-11-30 13:06:23 - INFO -   - Embedding dimension: 128
2025-11-30 13:06:23 - INFO -   - Attention heads per layer: 4
2025-11-30 13:06:23 - INFO -   - Vocabulary size: 48
2025-11-30 13:06:23 - INFO -   - Context window: 128 tokens
2025-11-30 13:06:23 - DEBUG - Initializing transformer model components...
2025-11-30 13:06:23 - DEBUG -   - Vocab size: 48, Embed dim: 128
2025-11-30 13:06:23 - DEBUG -   - Heads: 4, Layers: 3, Block size: 128
2025-11-30 13:06:23 - DEBUG -   - Created token embedding: 48 x 128
2025-11-30 13:06:23 - DEBUG -   - Created positional embedding: 128 x 128
2025-11-30 13:06:23 - DEBUG -   - Creating 3 transformer encoder layers...
2025-11-30 13:06:23 - DEBUG -   - Transformer blocks created: 3 layers
2025-11-30 13:06:23 - DEBUG -   - Output layer: Linear(128 -> 48)
2025-11-30 13:06:23 - INFO - 
2025-11-30 13:06:23 - INFO - Model initialized:
2025-11-30 13:06:23 - INFO -   - Total parameters: 623,792
2025-11-30 13:06:23 - INFO -   - Trainable parameters: 623,792
2025-11-30 13:06:23 - INFO -   - Model location: cpu
2025-11-30 13:06:23 - INFO -   - Time taken: 0.011 seconds
2025-11-30 13:06:23 - INFO - 
2025-11-30 13:06:23 - INFO - ================================================================================
2025-11-30 13:06:23 - INFO - STEP 6: TRAINING SETUP
2025-11-30 13:06:23 - INFO - ================================================================================
2025-11-30 13:06:23 - INFO - 
2025-11-30 13:06:23 - INFO - Configuring training components:
2025-11-30 13:06:23 - INFO -   - Loss function: CrossEntropyLoss
2025-11-30 13:06:23 - INFO -   - Optimizer: AdamW (learning rate: 0.001)
2025-11-30 13:06:24 - INFO -   - Time taken: 1.019 seconds
2025-11-30 13:06:24 - INFO - 
2025-11-30 13:06:24 - INFO - ================================================================================
2025-11-30 13:06:24 - INFO - STEP 7: TRAINING LOOP
2025-11-30 13:06:24 - INFO - ================================================================================
2025-11-30 13:06:24 - INFO - 
2025-11-30 13:06:24 - INFO - Starting training: 10 epochs, 158 batches per epoch
2025-11-30 13:06:24 - INFO - 
2025-11-30 13:06:24 - INFO - 
2025-11-30 13:06:24 - INFO - --------------------------------------------------------------------------------
2025-11-30 13:06:24 - INFO - EPOCH 1/10
2025-11-30 13:06:24 - INFO - --------------------------------------------------------------------------------
2025-11-30 13:06:24 - INFO - 
2025-11-30 13:06:24 - INFO - Processing 158 batches...
2025-11-30 13:06:25 - DEBUG -   First batch gradient norm: 1.6961
2025-11-30 13:06:28 - DEBUG -   Batch 10/158: Current loss = 2.9090, Running average = 3.2030
2025-11-30 13:06:32 - DEBUG -   Batch 20/158: Current loss = 2.6492, Running average = 2.9815
2025-11-30 13:06:36 - DEBUG -   Batch 30/158: Current loss = 2.5715, Running average = 2.8545
2025-11-30 13:06:39 - DEBUG -   Batch 40/158: Current loss = 2.5007, Running average = 2.7748
2025-11-30 13:06:42 - DEBUG -   Batch 50/158: Current loss = 2.4572, Running average = 2.7118
2025-11-30 13:06:46 - DEBUG -   Batch 60/158: Current loss = 2.4316, Running average = 2.6633
2025-11-30 13:06:49 - DEBUG -   Batch 70/158: Current loss = 2.3772, Running average = 2.6280
2025-11-30 13:06:53 - DEBUG -   Batch 80/158: Current loss = 2.3643, Running average = 2.5969
2025-11-30 13:06:56 - DEBUG -   Batch 90/158: Current loss = 2.3483, Running average = 2.5723
2025-11-30 13:06:59 - DEBUG -   Batch 100/158: Current loss = 2.3507, Running average = 2.5515
2025-11-30 13:07:02 - DEBUG -   Batch 110/158: Current loss = 2.3422, Running average = 2.5322
2025-11-30 13:07:06 - DEBUG -   Batch 120/158: Current loss = 2.3470, Running average = 2.5159
2025-11-30 13:07:09 - DEBUG -   Batch 130/158: Current loss = 2.3331, Running average = 2.5012
2025-11-30 13:07:12 - DEBUG -   Batch 140/158: Current loss = 2.2973, Running average = 2.4879
2025-11-30 13:07:16 - DEBUG -   Batch 150/158: Current loss = 2.2738, Running average = 2.4756
2025-11-30 13:07:18 - INFO - Epoch statistics:
2025-11-30 13:07:18 - INFO -   - Average loss: 2.4660
2025-11-30 13:07:18 - INFO -   - Min batch loss: 2.2516
2025-11-30 13:07:18 - INFO -   - Max batch loss: 4.0623
2025-11-30 13:07:18 - INFO - 
2025-11-30 13:07:18 - INFO - Epoch 1/10 completed - Average Loss: 2.4660
2025-11-30 13:07:18 - INFO -   - Time taken: 53.744 seconds
2025-11-30 13:07:18 - INFO - 
2025-11-30 13:07:18 - INFO - 
2025-11-30 13:07:18 - INFO - --------------------------------------------------------------------------------
2025-11-30 13:07:18 - INFO - EPOCH 2/10
2025-11-30 13:07:18 - INFO - --------------------------------------------------------------------------------
2025-11-30 13:07:18 - INFO - 
2025-11-30 13:07:18 - INFO - Processing 158 batches...
2025-11-30 13:07:19 - DEBUG -   First batch gradient norm: 0.2994
2025-11-30 13:07:21 - DEBUG -   Batch 10/158: Current loss = 2.2593, Running average = 2.2673
2025-11-30 13:07:25 - DEBUG -   Batch 20/158: Current loss = 2.1791, Running average = 2.2467
2025-11-30 13:07:28 - DEBUG -   Batch 30/158: Current loss = 2.0754, Running average = 2.2071
2025-11-30 13:07:31 - DEBUG -   Batch 40/158: Current loss = 1.6320, Running average = 2.1126
2025-11-30 13:07:35 - DEBUG -   Batch 50/158: Current loss = 1.1319, Running average = 1.9599
2025-11-30 13:07:38 - DEBUG -   Batch 60/158: Current loss = 0.6311, Running average = 1.7744
2025-11-30 13:07:42 - DEBUG -   Batch 70/158: Current loss = 0.4121, Running average = 1.5918
2025-11-30 13:07:45 - DEBUG -   Batch 80/158: Current loss = 0.2546, Running average = 1.4304
2025-11-30 13:07:48 - DEBUG -   Batch 90/158: Current loss = 0.1869, Running average = 1.2947
2025-11-30 13:07:51 - DEBUG -   Batch 100/158: Current loss = 0.1579, Running average = 1.1826
2025-11-30 13:07:55 - DEBUG -   Batch 110/158: Current loss = 0.1478, Running average = 1.0878
2025-11-30 13:07:58 - DEBUG -   Batch 120/158: Current loss = 0.1078, Running average = 1.0072
2025-11-30 13:08:01 - DEBUG -   Batch 130/158: Current loss = 0.0891, Running average = 0.9376
2025-11-30 13:08:05 - DEBUG -   Batch 140/158: Current loss = 0.0761, Running average = 0.8768
2025-11-30 13:08:08 - DEBUG -   Batch 150/158: Current loss = 0.0762, Running average = 0.8237
2025-11-30 13:08:11 - INFO - Epoch statistics:
2025-11-30 13:08:11 - INFO -   - Average loss: 0.7855
2025-11-30 13:08:11 - INFO -   - Min batch loss: 0.0606
2025-11-30 13:08:11 - INFO -   - Max batch loss: 2.3227
2025-11-30 13:08:11 - INFO - 
2025-11-30 13:08:11 - INFO - Epoch 2/10 completed - Average Loss: 0.7855
2025-11-30 13:08:11 - INFO -   - Time taken: 52.581 seconds
2025-11-30 13:08:11 - INFO - 
2025-11-30 13:08:11 - INFO - 
2025-11-30 13:08:11 - INFO - --------------------------------------------------------------------------------
2025-11-30 13:08:11 - INFO - EPOCH 3/10
2025-11-30 13:08:11 - INFO - --------------------------------------------------------------------------------
2025-11-30 13:08:11 - INFO - 
2025-11-30 13:08:11 - INFO - Processing 158 batches...
2025-11-30 13:08:11 - DEBUG -   First batch gradient norm: 0.0974
2025-11-30 13:08:14 - DEBUG -   Batch 10/158: Current loss = 0.0589, Running average = 0.0629
2025-11-30 13:08:17 - DEBUG -   Batch 20/158: Current loss = 0.0654, Running average = 0.0595
2025-11-30 13:08:21 - DEBUG -   Batch 30/158: Current loss = 0.0465, Running average = 0.0572
2025-11-30 13:08:24 - DEBUG -   Batch 40/158: Current loss = 0.0544, Running average = 0.0546
2025-11-30 13:08:27 - DEBUG -   Batch 50/158: Current loss = 0.0417, Running average = 0.0524
2025-11-30 13:08:31 - DEBUG -   Batch 60/158: Current loss = 0.0448, Running average = 0.0514
2025-11-30 13:08:34 - DEBUG -   Batch 70/158: Current loss = 0.0342, Running average = 0.0503
2025-11-30 13:08:37 - DEBUG -   Batch 80/158: Current loss = 0.0441, Running average = 0.0492
2025-11-30 13:08:40 - DEBUG -   Batch 90/158: Current loss = 0.0335, Running average = 0.0480
2025-11-30 13:08:43 - DEBUG -   Batch 100/158: Current loss = 0.0301, Running average = 0.0470
2025-11-30 13:08:47 - DEBUG -   Batch 110/158: Current loss = 0.0396, Running average = 0.0463
2025-11-30 13:08:50 - DEBUG -   Batch 120/158: Current loss = 0.0357, Running average = 0.0457
2025-11-30 13:08:54 - DEBUG -   Batch 130/158: Current loss = 0.0375, Running average = 0.0450
2025-11-30 13:08:57 - DEBUG -   Batch 140/158: Current loss = 0.0386, Running average = 0.0444
2025-11-30 13:09:00 - DEBUG -   Batch 150/158: Current loss = 0.0303, Running average = 0.0437
2025-11-30 13:09:02 - INFO - Epoch statistics:
2025-11-30 13:09:02 - INFO -   - Average loss: 0.0434
2025-11-30 13:09:02 - INFO -   - Min batch loss: 0.0293
2025-11-30 13:09:02 - INFO -   - Max batch loss: 0.0696
2025-11-30 13:09:02 - INFO - 
2025-11-30 13:09:02 - INFO - Epoch 3/10 completed - Average Loss: 0.0434
2025-11-30 13:09:02 - INFO -   - Time taken: 51.669 seconds
2025-11-30 13:09:02 - INFO - 
2025-11-30 13:09:02 - INFO - 
2025-11-30 13:09:02 - INFO - --------------------------------------------------------------------------------
2025-11-30 13:09:02 - INFO - EPOCH 4/10
2025-11-30 13:09:02 - INFO - --------------------------------------------------------------------------------
2025-11-30 13:09:02 - INFO - 
2025-11-30 13:09:02 - INFO - Processing 158 batches...
2025-11-30 13:09:03 - DEBUG -   First batch gradient norm: 0.0600
2025-11-30 13:09:06 - DEBUG -   Batch 10/158: Current loss = 0.0289, Running average = 0.0343
2025-11-30 13:09:09 - DEBUG -   Batch 20/158: Current loss = 0.0317, Running average = 0.0339
2025-11-30 13:09:13 - DEBUG -   Batch 30/158: Current loss = 0.0338, Running average = 0.0339
2025-11-30 13:09:16 - DEBUG -   Batch 40/158: Current loss = 0.0310, Running average = 0.0337
2025-11-30 13:09:19 - DEBUG -   Batch 50/158: Current loss = 0.0286, Running average = 0.0334
2025-11-30 13:09:22 - DEBUG -   Batch 60/158: Current loss = 0.0317, Running average = 0.0331
2025-11-30 13:09:25 - DEBUG -   Batch 70/158: Current loss = 0.0287, Running average = 0.0330
2025-11-30 13:09:29 - DEBUG -   Batch 80/158: Current loss = 0.0286, Running average = 0.0326
2025-11-30 13:09:32 - DEBUG -   Batch 90/158: Current loss = 0.0306, Running average = 0.0322
2025-11-30 13:09:35 - DEBUG -   Batch 100/158: Current loss = 0.0330, Running average = 0.0320
2025-11-30 13:09:38 - DEBUG -   Batch 110/158: Current loss = 0.0308, Running average = 0.0318
2025-11-30 13:09:42 - DEBUG -   Batch 120/158: Current loss = 0.0306, Running average = 0.0317
2025-11-30 13:09:45 - DEBUG -   Batch 130/158: Current loss = 0.0291, Running average = 0.0317
2025-11-30 13:09:48 - DEBUG -   Batch 140/158: Current loss = 0.0350, Running average = 0.0317
2025-11-30 13:09:51 - DEBUG -   Batch 150/158: Current loss = 0.0319, Running average = 0.0316
2025-11-30 13:09:54 - INFO - Epoch statistics:
2025-11-30 13:09:54 - INFO -   - Average loss: 0.0316
2025-11-30 13:09:54 - INFO -   - Min batch loss: 0.0234
2025-11-30 13:09:54 - INFO -   - Max batch loss: 0.0396
2025-11-30 13:09:54 - INFO - 
2025-11-30 13:09:54 - INFO - Epoch 4/10 completed - Average Loss: 0.0316
2025-11-30 13:09:54 - INFO -   - Time taken: 51.407 seconds
2025-11-30 13:09:54 - INFO - 
2025-11-30 13:09:54 - INFO - 
2025-11-30 13:09:54 - INFO - --------------------------------------------------------------------------------
2025-11-30 13:09:54 - INFO - EPOCH 5/10
2025-11-30 13:09:54 - INFO - --------------------------------------------------------------------------------
2025-11-30 13:09:54 - INFO - 
2025-11-30 13:09:54 - INFO - Processing 158 batches...
2025-11-30 13:09:54 - DEBUG -   First batch gradient norm: 0.0505
2025-11-30 13:09:57 - DEBUG -   Batch 10/158: Current loss = 0.0278, Running average = 0.0285
2025-11-30 13:10:00 - DEBUG -   Batch 20/158: Current loss = 0.0252, Running average = 0.0280
2025-11-30 13:10:04 - DEBUG -   Batch 30/158: Current loss = 0.0327, Running average = 0.0288
2025-11-30 13:10:07 - DEBUG -   Batch 40/158: Current loss = 0.0225, Running average = 0.0283
2025-11-30 13:10:10 - DEBUG -   Batch 50/158: Current loss = 0.0234, Running average = 0.0281
2025-11-30 13:10:14 - DEBUG -   Batch 60/158: Current loss = 0.0247, Running average = 0.0279
2025-11-30 13:10:17 - DEBUG -   Batch 70/158: Current loss = 0.0227, Running average = 0.0278
2025-11-30 13:10:20 - DEBUG -   Batch 80/158: Current loss = 0.0292, Running average = 0.0280
2025-11-30 13:10:23 - DEBUG -   Batch 90/158: Current loss = 0.0287, Running average = 0.0281
2025-11-30 13:10:27 - DEBUG -   Batch 100/158: Current loss = 0.0268, Running average = 0.0281
2025-11-30 13:10:30 - DEBUG -   Batch 110/158: Current loss = 0.0256, Running average = 0.0278
2025-11-30 13:10:33 - DEBUG -   Batch 120/158: Current loss = 0.0243, Running average = 0.0276
2025-11-30 13:10:37 - DEBUG -   Batch 130/158: Current loss = 0.0218, Running average = 0.0274
2025-11-30 13:10:40 - DEBUG -   Batch 140/158: Current loss = 0.0272, Running average = 0.0273
2025-11-30 13:10:43 - DEBUG -   Batch 150/158: Current loss = 0.0288, Running average = 0.0273
2025-11-30 13:10:46 - INFO - Epoch statistics:
2025-11-30 13:10:46 - INFO -   - Average loss: 0.0272
2025-11-30 13:10:46 - INFO -   - Min batch loss: 0.0205
2025-11-30 13:10:46 - INFO -   - Max batch loss: 0.0378
2025-11-30 13:10:46 - INFO - 
2025-11-30 13:10:46 - INFO - Epoch 5/10 completed - Average Loss: 0.0272
2025-11-30 13:10:46 - INFO -   - Time taken: 51.984 seconds
2025-11-30 13:10:46 - INFO - 
2025-11-30 13:10:46 - INFO - 
2025-11-30 13:10:46 - INFO - --------------------------------------------------------------------------------
2025-11-30 13:10:46 - INFO - EPOCH 6/10
2025-11-30 13:10:46 - INFO - --------------------------------------------------------------------------------
2025-11-30 13:10:46 - INFO - 
2025-11-30 13:10:46 - INFO - Processing 158 batches...
2025-11-30 13:10:46 - DEBUG -   First batch gradient norm: 0.0451
2025-11-30 13:10:49 - DEBUG -   Batch 10/158: Current loss = 0.0235, Running average = 0.0263
2025-11-30 13:10:52 - DEBUG -   Batch 20/158: Current loss = 0.0309, Running average = 0.0258
2025-11-30 13:10:56 - DEBUG -   Batch 30/158: Current loss = 0.0276, Running average = 0.0260
2025-11-30 13:10:59 - DEBUG -   Batch 40/158: Current loss = 0.0260, Running average = 0.0257
2025-11-30 13:11:02 - DEBUG -   Batch 50/158: Current loss = 0.0260, Running average = 0.0256
2025-11-30 13:11:05 - DEBUG -   Batch 60/158: Current loss = 0.0203, Running average = 0.0253
2025-11-30 13:11:09 - DEBUG -   Batch 70/158: Current loss = 0.0268, Running average = 0.0252
2025-11-30 13:11:12 - DEBUG -   Batch 80/158: Current loss = 0.0235, Running average = 0.0250
2025-11-30 13:11:15 - DEBUG -   Batch 90/158: Current loss = 0.0262, Running average = 0.0251
2025-11-30 13:11:18 - DEBUG -   Batch 100/158: Current loss = 0.0234, Running average = 0.0250
2025-11-30 13:11:22 - DEBUG -   Batch 110/158: Current loss = 0.0248, Running average = 0.0250
2025-11-30 13:11:25 - DEBUG -   Batch 120/158: Current loss = 0.0234, Running average = 0.0249
2025-11-30 13:11:28 - DEBUG -   Batch 130/158: Current loss = 0.0212, Running average = 0.0247
2025-11-30 13:11:32 - DEBUG -   Batch 140/158: Current loss = 0.0226, Running average = 0.0247
2025-11-30 13:11:35 - DEBUG -   Batch 150/158: Current loss = 0.0205, Running average = 0.0247
2025-11-30 13:11:37 - INFO - Epoch statistics:
2025-11-30 13:11:37 - INFO -   - Average loss: 0.0246
2025-11-30 13:11:37 - INFO -   - Min batch loss: 0.0197
2025-11-30 13:11:37 - INFO -   - Max batch loss: 0.0316
2025-11-30 13:11:37 - INFO - 
2025-11-30 13:11:37 - INFO - Epoch 6/10 completed - Average Loss: 0.0246
2025-11-30 13:11:37 - INFO -   - Time taken: 51.462 seconds
2025-11-30 13:11:37 - INFO - 
2025-11-30 13:11:37 - INFO - 
2025-11-30 13:11:37 - INFO - --------------------------------------------------------------------------------
2025-11-30 13:11:37 - INFO - EPOCH 7/10
2025-11-30 13:11:37 - INFO - --------------------------------------------------------------------------------
2025-11-30 13:11:37 - INFO - 
2025-11-30 13:11:37 - INFO - Processing 158 batches...
2025-11-30 13:11:38 - DEBUG -   First batch gradient norm: 0.0562
2025-11-30 13:11:41 - DEBUG -   Batch 10/158: Current loss = 0.0220, Running average = 0.0248
2025-11-30 13:11:44 - DEBUG -   Batch 20/158: Current loss = 0.0256, Running average = 0.0235
2025-11-30 13:11:47 - DEBUG -   Batch 30/158: Current loss = 0.0273, Running average = 0.0237
2025-11-30 13:11:50 - DEBUG -   Batch 40/158: Current loss = 0.0249, Running average = 0.0237
2025-11-30 13:11:53 - DEBUG -   Batch 50/158: Current loss = 0.0275, Running average = 0.0239
2025-11-30 13:11:57 - DEBUG -   Batch 60/158: Current loss = 0.0243, Running average = 0.0239
2025-11-30 13:12:01 - DEBUG -   Batch 70/158: Current loss = 0.0254, Running average = 0.0238
2025-11-30 13:12:05 - DEBUG -   Batch 80/158: Current loss = 0.0229, Running average = 0.0237
2025-11-30 13:12:09 - DEBUG -   Batch 90/158: Current loss = 0.0259, Running average = 0.0237
2025-11-30 13:12:12 - DEBUG -   Batch 100/158: Current loss = 0.0291, Running average = 0.0238
2025-11-30 13:12:16 - DEBUG -   Batch 110/158: Current loss = 0.0226, Running average = 0.0237
2025-11-30 13:12:20 - DEBUG -   Batch 120/158: Current loss = 0.0261, Running average = 0.0237
2025-11-30 13:12:24 - DEBUG -   Batch 130/158: Current loss = 0.0280, Running average = 0.0236
2025-11-30 13:12:28 - DEBUG -   Batch 140/158: Current loss = 0.0257, Running average = 0.0237
2025-11-30 13:12:31 - DEBUG -   Batch 150/158: Current loss = 0.0236, Running average = 0.0236
2025-11-30 13:12:34 - INFO - Epoch statistics:
2025-11-30 13:12:34 - INFO -   - Average loss: 0.0236
2025-11-30 13:12:34 - INFO -   - Min batch loss: 0.0169
2025-11-30 13:12:34 - INFO -   - Max batch loss: 0.0333
2025-11-30 13:12:34 - INFO - 
2025-11-30 13:12:34 - INFO - Epoch 7/10 completed - Average Loss: 0.0236
2025-11-30 13:12:34 - INFO -   - Time taken: 57.101 seconds
2025-11-30 13:12:34 - INFO - 
2025-11-30 13:12:34 - INFO - 
2025-11-30 13:12:34 - INFO - --------------------------------------------------------------------------------
2025-11-30 13:12:34 - INFO - EPOCH 8/10
2025-11-30 13:12:34 - INFO - --------------------------------------------------------------------------------
2025-11-30 13:12:34 - INFO - 
2025-11-30 13:12:34 - INFO - Processing 158 batches...
2025-11-30 13:12:35 - DEBUG -   First batch gradient norm: 0.0354
2025-11-30 13:12:38 - DEBUG -   Batch 10/158: Current loss = 0.0245, Running average = 0.0219
2025-11-30 13:12:42 - DEBUG -   Batch 20/158: Current loss = 0.0196, Running average = 0.0213
2025-11-30 13:12:46 - DEBUG -   Batch 30/158: Current loss = 0.0243, Running average = 0.0214
2025-11-30 13:12:50 - DEBUG -   Batch 40/158: Current loss = 0.0211, Running average = 0.0216
2025-11-30 13:12:53 - DEBUG -   Batch 50/158: Current loss = 0.0217, Running average = 0.0217
2025-11-30 13:12:57 - DEBUG -   Batch 60/158: Current loss = 0.0209, Running average = 0.0218
2025-11-30 13:13:01 - DEBUG -   Batch 70/158: Current loss = 0.0245, Running average = 0.0218
2025-11-30 13:13:05 - DEBUG -   Batch 80/158: Current loss = 0.0208, Running average = 0.0219
2025-11-30 13:13:09 - DEBUG -   Batch 90/158: Current loss = 0.0187, Running average = 0.0218
2025-11-30 13:13:12 - DEBUG -   Batch 100/158: Current loss = 0.0270, Running average = 0.0219
2025-11-30 13:13:16 - DEBUG -   Batch 110/158: Current loss = 0.0251, Running average = 0.0221
2025-11-30 13:13:20 - DEBUG -   Batch 120/158: Current loss = 0.0260, Running average = 0.0222
2025-11-30 13:13:24 - DEBUG -   Batch 130/158: Current loss = 0.0232, Running average = 0.0222
2025-11-30 13:13:28 - DEBUG -   Batch 140/158: Current loss = 0.0207, Running average = 0.0221
2025-11-30 13:13:31 - DEBUG -   Batch 150/158: Current loss = 0.0201, Running average = 0.0221
2025-11-30 13:13:34 - INFO - Epoch statistics:
2025-11-30 13:13:34 - INFO -   - Average loss: 0.0221
2025-11-30 13:13:34 - INFO -   - Min batch loss: 0.0153
2025-11-30 13:13:34 - INFO -   - Max batch loss: 0.0286
2025-11-30 13:13:34 - INFO - 
2025-11-30 13:13:34 - INFO - Epoch 8/10 completed - Average Loss: 0.0221
2025-11-30 13:13:34 - INFO -   - Time taken: 59.826 seconds
2025-11-30 13:13:34 - INFO - 
2025-11-30 13:13:34 - INFO - 
2025-11-30 13:13:34 - INFO - --------------------------------------------------------------------------------
2025-11-30 13:13:34 - INFO - EPOCH 9/10
2025-11-30 13:13:34 - INFO - --------------------------------------------------------------------------------
2025-11-30 13:13:34 - INFO - 
2025-11-30 13:13:34 - INFO - Processing 158 batches...
2025-11-30 13:13:35 - DEBUG -   First batch gradient norm: 0.0427
2025-11-30 13:13:38 - DEBUG -   Batch 10/158: Current loss = 0.0216, Running average = 0.0225
2025-11-30 13:13:42 - DEBUG -   Batch 20/158: Current loss = 0.0193, Running average = 0.0222
2025-11-30 13:13:46 - DEBUG -   Batch 30/158: Current loss = 0.0193, Running average = 0.0215
2025-11-30 13:13:49 - DEBUG -   Batch 40/158: Current loss = 0.0226, Running average = 0.0219
2025-11-30 13:13:53 - DEBUG -   Batch 50/158: Current loss = 0.0220, Running average = 0.0217
2025-11-30 13:13:57 - DEBUG -   Batch 60/158: Current loss = 0.0219, Running average = 0.0218
2025-11-30 13:14:01 - DEBUG -   Batch 70/158: Current loss = 0.0224, Running average = 0.0218
2025-11-30 13:14:05 - DEBUG -   Batch 80/158: Current loss = 0.0232, Running average = 0.0217
2025-11-30 13:14:09 - DEBUG -   Batch 90/158: Current loss = 0.0202, Running average = 0.0216
2025-11-30 13:14:12 - DEBUG -   Batch 100/158: Current loss = 0.0213, Running average = 0.0215
2025-11-30 13:14:16 - DEBUG -   Batch 110/158: Current loss = 0.0206, Running average = 0.0214
2025-11-30 13:14:20 - DEBUG -   Batch 120/158: Current loss = 0.0201, Running average = 0.0214
2025-11-30 13:14:24 - DEBUG -   Batch 130/158: Current loss = 0.0233, Running average = 0.0214
2025-11-30 13:14:28 - DEBUG -   Batch 140/158: Current loss = 0.0222, Running average = 0.0214
2025-11-30 13:14:31 - DEBUG -   Batch 150/158: Current loss = 0.0246, Running average = 0.0214
2025-11-30 13:14:34 - INFO - Epoch statistics:
2025-11-30 13:14:34 - INFO -   - Average loss: 0.0214
2025-11-30 13:14:34 - INFO -   - Min batch loss: 0.0158
2025-11-30 13:14:34 - INFO -   - Max batch loss: 0.0285
2025-11-30 13:14:34 - INFO - 
2025-11-30 13:14:34 - INFO - Epoch 9/10 completed - Average Loss: 0.0214
2025-11-30 13:14:34 - INFO -   - Time taken: 60.141 seconds
2025-11-30 13:14:34 - INFO - 
2025-11-30 13:14:34 - INFO - 
2025-11-30 13:14:34 - INFO - --------------------------------------------------------------------------------
2025-11-30 13:14:34 - INFO - EPOCH 10/10
2025-11-30 13:14:34 - INFO - --------------------------------------------------------------------------------
2025-11-30 13:14:34 - INFO - 
2025-11-30 13:14:34 - INFO - Processing 158 batches...
2025-11-30 13:14:35 - DEBUG -   First batch gradient norm: 0.0386
2025-11-30 13:14:38 - DEBUG -   Batch 10/158: Current loss = 0.0196, Running average = 0.0203
2025-11-30 13:14:42 - DEBUG -   Batch 20/158: Current loss = 0.0184, Running average = 0.0203
2025-11-30 13:14:46 - DEBUG -   Batch 30/158: Current loss = 0.0221, Running average = 0.0202
2025-11-30 13:14:50 - DEBUG -   Batch 40/158: Current loss = 0.0214, Running average = 0.0203
2025-11-30 13:14:53 - DEBUG -   Batch 50/158: Current loss = 0.0213, Running average = 0.0203
2025-11-30 13:14:57 - DEBUG -   Batch 60/158: Current loss = 0.0206, Running average = 0.0204
2025-11-30 13:15:01 - DEBUG -   Batch 70/158: Current loss = 0.0168, Running average = 0.0204
2025-11-30 13:15:05 - DEBUG -   Batch 80/158: Current loss = 0.0228, Running average = 0.0206
2025-11-30 13:15:09 - DEBUG -   Batch 90/158: Current loss = 0.0158, Running average = 0.0206
2025-11-30 13:15:13 - DEBUG -   Batch 100/158: Current loss = 0.0221, Running average = 0.0206
2025-11-30 13:15:17 - DEBUG -   Batch 110/158: Current loss = 0.0229, Running average = 0.0207
2025-11-30 13:15:20 - DEBUG -   Batch 120/158: Current loss = 0.0205, Running average = 0.0207
2025-11-30 13:15:24 - DEBUG -   Batch 130/158: Current loss = 0.0200, Running average = 0.0207
2025-11-30 13:15:28 - DEBUG -   Batch 140/158: Current loss = 0.0216, Running average = 0.0207
2025-11-30 13:15:32 - DEBUG -   Batch 150/158: Current loss = 0.0210, Running average = 0.0208
2025-11-30 13:15:35 - INFO - Epoch statistics:
2025-11-30 13:15:35 - INFO -   - Average loss: 0.0207
2025-11-30 13:15:35 - INFO -   - Min batch loss: 0.0140
2025-11-30 13:15:35 - INFO -   - Max batch loss: 0.0260
2025-11-30 13:15:35 - INFO - 
2025-11-30 13:15:35 - INFO - Epoch 10/10 completed - Average Loss: 0.0207
2025-11-30 13:15:35 - INFO -   - Time taken: 60.638 seconds
2025-11-30 13:15:35 - INFO - 
2025-11-30 13:15:35 - INFO - ================================================================================
2025-11-30 13:15:35 - INFO - TRAINING COMPLETE
2025-11-30 13:15:35 - INFO - ================================================================================
2025-11-30 13:15:35 - INFO - 
2025-11-30 13:15:35 - INFO - Training duration: 00:09:10 (550.56 seconds)
2025-11-30 13:15:35 - INFO - 
2025-11-30 13:15:35 - INFO - ================================================================================
2025-11-30 13:15:35 - INFO - STEP 8: MODEL SAVING
2025-11-30 13:15:35 - INFO - ================================================================================
2025-11-30 13:15:35 - INFO - 
2025-11-30 13:15:35 - INFO - Saving trained model to: 50_models\dataset_toy\embed128_layers3_heads4_epochs10.pt
2025-11-30 13:15:35 - INFO - 
2025-11-30 13:15:35 - INFO - Saving model components:
2025-11-30 13:15:35 - INFO -   - Model weights (state_dict)
2025-11-30 13:15:35 - INFO -   - Vocabulary mappings (character to index)
2025-11-30 13:15:35 - INFO -   - Model hyperparameters
2025-11-30 13:15:35 - INFO - 
2025-11-30 13:15:35 - INFO - Model saved successfully!
2025-11-30 13:15:35 - INFO -   - File path: 50_models\dataset_toy\embed128_layers3_heads4_epochs10.pt
2025-11-30 13:15:35 - INFO -   - File size: 2.40 MB
2025-11-30 13:15:35 - INFO -   - Time taken: 0.013 seconds
2025-11-30 13:15:35 - INFO - 
2025-11-30 13:15:35 - INFO - ================================================================================
2025-11-30 13:15:35 - INFO - TRAINING SESSION ENDED
2025-11-30 13:15:35 - INFO - ================================================================================
2025-11-30 13:15:35 - INFO - 
