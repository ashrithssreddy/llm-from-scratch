2025-11-30 12:20:37 - INFO - 
2025-11-30 12:20:37 - INFO - ================================================================================
2025-11-30 12:20:37 - INFO - TRAINING SESSION STARTED
2025-11-30 12:20:37 - INFO - ================================================================================
2025-11-30 12:20:37 - INFO - 
2025-11-30 12:20:37 - INFO - Log file: 97_logs\training_20251130_122037.log
2025-11-30 12:20:37 - INFO - Timestamp: 20251130_122037
2025-11-30 12:20:37 - INFO - 
2025-11-30 12:20:37 - INFO - Command line arguments parsed
2025-11-30 12:20:37 - INFO - 
2025-11-30 12:20:37 - INFO - 
2025-11-30 12:20:37 - INFO - ================================================================================
2025-11-30 12:20:37 - INFO - TRAINING CONFIGURATION
2025-11-30 12:20:37 - INFO - ================================================================================
2025-11-30 12:20:37 - INFO - 
2025-11-30 12:20:37 - INFO - Training Parameters:
2025-11-30 12:20:37 - INFO -   - Dataset folder: 40_training_data/dataset_toy/
2025-11-30 12:20:37 - INFO -   - Total epochs: 10
2025-11-30 12:20:37 - INFO -   - Batch size: 32
2025-11-30 12:20:37 - INFO -   - Sequence length (block_size): 128
2025-11-30 12:20:37 - INFO -   - Embedding dimension: 128
2025-11-30 12:20:37 - INFO -   - Attention heads: 4
2025-11-30 12:20:37 - INFO -   - Transformer layers: 3
2025-11-30 12:20:37 - INFO -   - Learning rate: 0.001
2025-11-30 12:20:37 - INFO - 
2025-11-30 12:20:37 - INFO - ================================================================================
2025-11-30 12:20:37 - INFO - STEP 1: DEVICE SELECTION
2025-11-30 12:20:37 - INFO - ================================================================================
2025-11-30 12:20:37 - INFO - 
2025-11-30 12:20:37 - INFO - Selected device: cpu
2025-11-30 12:20:37 - INFO -   - Using CPU (no CUDA available)
2025-11-30 12:20:37 - INFO -   - Time taken: 0.001 seconds
2025-11-30 12:20:37 - INFO - 
2025-11-30 12:20:37 - INFO - ================================================================================
2025-11-30 12:20:37 - INFO - STEP 2: DATA LOADING
2025-11-30 12:20:37 - INFO - ================================================================================
2025-11-30 12:20:37 - INFO - 
2025-11-30 12:20:37 - INFO - Loading all .txt files from: 40_training_data/dataset_toy/
2025-11-30 12:20:37 - INFO - 
2025-11-30 12:20:37 - INFO - Data loaded successfully:
2025-11-30 12:20:37 - INFO -   - Total characters: 5,178
2025-11-30 12:20:37 - INFO -   - Total lines: 43
2025-11-30 12:20:37 - INFO -   - Time taken: 0.002 seconds
2025-11-30 12:20:37 - INFO - 
2025-11-30 12:20:37 - INFO - ================================================================================
2025-11-30 12:20:37 - INFO - STEP 3: DATASET PREPARATION
2025-11-30 12:20:37 - INFO - ================================================================================
2025-11-30 12:20:37 - INFO - 
2025-11-30 12:20:37 - INFO - Creating character-level dataset...
2025-11-30 12:20:37 - INFO -   - Processing text into sequences of length 128
2025-11-30 12:20:37 - DEBUG - Creating character-level dataset...
2025-11-30 12:20:37 - DEBUG -   - Input text length: 5178 characters
2025-11-30 12:20:37 - DEBUG -   - Block size: 128
2025-11-30 12:20:37 - DEBUG -   - Vocabulary size: 48
2025-11-30 12:20:37 - DEBUG -   - Unique characters: 
 ',-.ABCDEFHILMNPRSTWabcdefghijklmnopqrstuvwxyz
2025-11-30 12:20:37 - DEBUG -   - Encoding text to integer sequences...
2025-11-30 12:20:37 - DEBUG -   - Encoded data length: 5178 tokens
2025-11-30 12:20:37 - INFO - 
2025-11-30 12:20:37 - INFO - Dataset created:
2025-11-30 12:20:37 - INFO -   - Vocabulary size: 48 unique characters
2025-11-30 12:20:37 - INFO -   - Total training sequences: 5,050
2025-11-30 12:20:37 - INFO -   - Total tokens: 5,178
2025-11-30 12:20:37 - INFO -   - Time taken: 0.003 seconds
2025-11-30 12:20:37 - INFO - 
2025-11-30 12:20:37 - INFO - ================================================================================
2025-11-30 12:20:37 - INFO - STEP 4: DATALOADER SETUP
2025-11-30 12:20:37 - INFO - ================================================================================
2025-11-30 12:20:37 - INFO - 
2025-11-30 12:20:37 - INFO - Creating DataLoader with batch_size=32 (shuffling enabled)...
2025-11-30 12:20:37 - INFO -   - Total batches per epoch: 158
2025-11-30 12:20:37 - INFO -   - Samples per batch: 32
2025-11-30 12:20:37 - INFO -   - Time taken: 0.001 seconds
2025-11-30 12:20:37 - INFO - 
2025-11-30 12:20:37 - INFO - ================================================================================
2025-11-30 12:20:37 - INFO - STEP 5: MODEL INITIALIZATION
2025-11-30 12:20:37 - INFO - ================================================================================
2025-11-30 12:20:37 - INFO - 
2025-11-30 12:20:37 - INFO - Building transformer-based language model...
2025-11-30 12:20:37 - INFO -   - Architecture: 3 transformer layers
2025-11-30 12:20:37 - INFO -   - Embedding dimension: 128
2025-11-30 12:20:37 - INFO -   - Attention heads per layer: 4
2025-11-30 12:20:37 - INFO -   - Vocabulary size: 48
2025-11-30 12:20:37 - INFO -   - Context window: 128 tokens
2025-11-30 12:20:37 - DEBUG - Initializing transformer model components...
2025-11-30 12:20:37 - DEBUG -   - Vocab size: 48, Embed dim: 128
2025-11-30 12:20:37 - DEBUG -   - Heads: 4, Layers: 3, Block size: 128
2025-11-30 12:20:37 - DEBUG -   - Created token embedding: 48 x 128
2025-11-30 12:20:37 - DEBUG -   - Created positional embedding: 128 x 128
2025-11-30 12:20:37 - DEBUG -   - Creating 3 transformer encoder layers...
2025-11-30 12:20:37 - DEBUG -   - Transformer blocks created: 3 layers
2025-11-30 12:20:37 - DEBUG -   - Output layer: Linear(128 -> 48)
2025-11-30 12:20:37 - INFO - 
2025-11-30 12:20:37 - INFO - Model initialized:
2025-11-30 12:20:37 - INFO -   - Total parameters: 623,792
2025-11-30 12:20:37 - INFO -   - Trainable parameters: 623,792
2025-11-30 12:20:37 - INFO -   - Model location: cpu
2025-11-30 12:20:37 - INFO -   - Time taken: 0.011 seconds
2025-11-30 12:20:37 - INFO - 
2025-11-30 12:20:37 - INFO - ================================================================================
2025-11-30 12:20:37 - INFO - STEP 6: TRAINING SETUP
2025-11-30 12:20:37 - INFO - ================================================================================
2025-11-30 12:20:37 - INFO - 
2025-11-30 12:20:37 - INFO - Configuring training components:
2025-11-30 12:20:37 - INFO -   - Loss function: CrossEntropyLoss
2025-11-30 12:20:37 - INFO -   - Optimizer: AdamW (learning rate: 0.001)
2025-11-30 12:20:38 - INFO -   - Time taken: 1.226 seconds
2025-11-30 12:20:38 - INFO - 
2025-11-30 12:20:38 - INFO - ================================================================================
2025-11-30 12:20:38 - INFO - STEP 7: TRAINING LOOP
2025-11-30 12:20:38 - INFO - ================================================================================
2025-11-30 12:20:38 - INFO - 
2025-11-30 12:20:38 - INFO - Starting training: 10 epochs, 158 batches per epoch
2025-11-30 12:20:38 - INFO - 
2025-11-30 12:20:38 - INFO - 
2025-11-30 12:20:38 - INFO - --------------------------------------------------------------------------------
2025-11-30 12:20:38 - INFO - EPOCH 1/10
2025-11-30 12:20:38 - INFO - --------------------------------------------------------------------------------
2025-11-30 12:20:38 - INFO - 
2025-11-30 12:20:38 - INFO - Processing 158 batches...
2025-11-30 12:20:39 - DEBUG -   First batch gradient norm: 1.9560
2025-11-30 12:20:43 - DEBUG -   Batch 10/158: Current loss = 2.9098, Running average = 3.1840
2025-11-30 12:20:47 - DEBUG -   Batch 20/158: Current loss = 2.6559, Running average = 2.9677
2025-11-30 12:20:50 - DEBUG -   Batch 30/158: Current loss = 2.5545, Running average = 2.8449
2025-11-30 12:20:54 - DEBUG -   Batch 40/158: Current loss = 2.4902, Running average = 2.7629
2025-11-30 12:20:57 - DEBUG -   Batch 50/158: Current loss = 2.4504, Running average = 2.7033
2025-11-30 12:21:00 - DEBUG -   Batch 60/158: Current loss = 2.4138, Running average = 2.6559
2025-11-30 12:21:03 - DEBUG -   Batch 70/158: Current loss = 2.3753, Running average = 2.6214
2025-11-30 12:21:07 - DEBUG -   Batch 80/158: Current loss = 2.3772, Running average = 2.5937
2025-11-30 12:21:10 - DEBUG -   Batch 90/158: Current loss = 2.4076, Running average = 2.5706
2025-11-30 12:21:13 - DEBUG -   Batch 100/158: Current loss = 2.3630, Running average = 2.5497
2025-11-30 12:21:17 - DEBUG -   Batch 110/158: Current loss = 2.3200, Running average = 2.5310
2025-11-30 12:21:20 - DEBUG -   Batch 120/158: Current loss = 2.3101, Running average = 2.5139
2025-11-30 12:21:23 - DEBUG -   Batch 130/158: Current loss = 2.2929, Running average = 2.4964
2025-11-30 12:21:27 - DEBUG -   Batch 140/158: Current loss = 2.1741, Running average = 2.4775
2025-11-30 12:21:30 - DEBUG -   Batch 150/158: Current loss = 1.9053, Running average = 2.4494
2025-11-30 12:21:32 - INFO - Epoch statistics:
2025-11-30 12:21:32 - INFO -   - Average loss: 2.4144
2025-11-30 12:21:32 - INFO -   - Min batch loss: 1.5854
2025-11-30 12:21:32 - INFO -   - Max batch loss: 4.0973
2025-11-30 12:21:32 - INFO - 
2025-11-30 12:21:32 - INFO - Epoch 1/10 completed - Average Loss: 2.4144
2025-11-30 12:21:32 - INFO - 
2025-11-30 12:21:32 - INFO - 
2025-11-30 12:21:32 - INFO - --------------------------------------------------------------------------------
2025-11-30 12:21:32 - INFO - EPOCH 2/10
2025-11-30 12:21:32 - INFO - --------------------------------------------------------------------------------
2025-11-30 12:21:32 - INFO - 
2025-11-30 12:21:32 - INFO - Processing 158 batches...
2025-11-30 12:21:32 - DEBUG -   First batch gradient norm: 0.5593
2025-11-30 12:21:35 - DEBUG -   Batch 10/158: Current loss = 1.1039, Running average = 1.3283
2025-11-30 12:21:39 - DEBUG -   Batch 20/158: Current loss = 0.7014, Running average = 1.1012
2025-11-30 12:21:42 - DEBUG -   Batch 30/158: Current loss = 0.4518, Running average = 0.9180
2025-11-30 12:21:45 - DEBUG -   Batch 40/158: Current loss = 0.2744, Running average = 0.7728
2025-11-30 12:21:48 - DEBUG -   Batch 50/158: Current loss = 0.2101, Running average = 0.6640
2025-11-30 12:21:52 - DEBUG -   Batch 60/158: Current loss = 0.1636, Running average = 0.5830
2025-11-30 12:21:55 - DEBUG -   Batch 70/158: Current loss = 0.1308, Running average = 0.5212
2025-11-30 12:21:58 - DEBUG -   Batch 80/158: Current loss = 0.1150, Running average = 0.4717
2025-11-30 12:22:01 - DEBUG -   Batch 90/158: Current loss = 0.1171, Running average = 0.4313
2025-11-30 12:22:04 - DEBUG -   Batch 100/158: Current loss = 0.0911, Running average = 0.3978
2025-11-30 12:22:08 - DEBUG -   Batch 110/158: Current loss = 0.0825, Running average = 0.3691
2025-11-30 12:22:11 - DEBUG -   Batch 120/158: Current loss = 0.0622, Running average = 0.3445
2025-11-30 12:22:14 - DEBUG -   Batch 130/158: Current loss = 0.0586, Running average = 0.3233
2025-11-30 12:22:18 - DEBUG -   Batch 140/158: Current loss = 0.0647, Running average = 0.3048
2025-11-30 12:22:21 - DEBUG -   Batch 150/158: Current loss = 0.0524, Running average = 0.2881
2025-11-30 12:22:23 - INFO - Epoch statistics:
2025-11-30 12:22:23 - INFO -   - Average loss: 0.2763
2025-11-30 12:22:23 - INFO -   - Min batch loss: 0.0458
2025-11-30 12:22:23 - INFO -   - Max batch loss: 1.5462
2025-11-30 12:22:23 - INFO - 
2025-11-30 12:22:23 - INFO - Epoch 2/10 completed - Average Loss: 0.2763
2025-11-30 12:22:23 - INFO - 
2025-11-30 12:22:23 - INFO - 
2025-11-30 12:22:23 - INFO - --------------------------------------------------------------------------------
2025-11-30 12:22:23 - INFO - EPOCH 3/10
2025-11-30 12:22:23 - INFO - --------------------------------------------------------------------------------
2025-11-30 12:22:23 - INFO - 
2025-11-30 12:22:23 - INFO - Processing 158 batches...
2025-11-30 12:22:24 - DEBUG -   First batch gradient norm: 0.0780
2025-11-30 12:22:27 - DEBUG -   Batch 10/158: Current loss = 0.0454, Running average = 0.0469
2025-11-30 12:22:30 - DEBUG -   Batch 20/158: Current loss = 0.0443, Running average = 0.0463
2025-11-30 12:22:33 - DEBUG -   Batch 30/158: Current loss = 0.0406, Running average = 0.0448
2025-11-30 12:22:37 - DEBUG -   Batch 40/158: Current loss = 0.0376, Running average = 0.0437
2025-11-30 12:22:40 - DEBUG -   Batch 50/158: Current loss = 0.0382, Running average = 0.0432
2025-11-30 12:22:43 - DEBUG -   Batch 60/158: Current loss = 0.0398, Running average = 0.0427
2025-11-30 12:22:46 - DEBUG -   Batch 70/158: Current loss = 0.0410, Running average = 0.0419
2025-11-30 12:22:50 - DEBUG -   Batch 80/158: Current loss = 0.0391, Running average = 0.0413
2025-11-30 12:22:53 - DEBUG -   Batch 90/158: Current loss = 0.0375, Running average = 0.0408
2025-11-30 12:22:56 - DEBUG -   Batch 100/158: Current loss = 0.0417, Running average = 0.0402
2025-11-30 12:22:59 - DEBUG -   Batch 110/158: Current loss = 0.0304, Running average = 0.0399
2025-11-30 12:23:03 - DEBUG -   Batch 120/158: Current loss = 0.0313, Running average = 0.0394
2025-11-30 12:23:06 - DEBUG -   Batch 130/158: Current loss = 0.0373, Running average = 0.0391
2025-11-30 12:23:09 - DEBUG -   Batch 140/158: Current loss = 0.0349, Running average = 0.0387
2025-11-30 12:23:12 - DEBUG -   Batch 150/158: Current loss = 0.0326, Running average = 0.0382
2025-11-30 12:23:15 - INFO - Epoch statistics:
2025-11-30 12:23:15 - INFO -   - Average loss: 0.0380
2025-11-30 12:23:15 - INFO -   - Min batch loss: 0.0272
2025-11-30 12:23:15 - INFO -   - Max batch loss: 0.0551
2025-11-30 12:23:15 - INFO - 
2025-11-30 12:23:15 - INFO - Epoch 3/10 completed - Average Loss: 0.0380
2025-11-30 12:23:15 - INFO - 
2025-11-30 12:23:15 - INFO - 
2025-11-30 12:23:15 - INFO - --------------------------------------------------------------------------------
2025-11-30 12:23:15 - INFO - EPOCH 4/10
2025-11-30 12:23:15 - INFO - --------------------------------------------------------------------------------
2025-11-30 12:23:15 - INFO - 
2025-11-30 12:23:15 - INFO - Processing 158 batches...
2025-11-30 12:23:15 - DEBUG -   First batch gradient norm: 0.0536
2025-11-30 12:23:18 - DEBUG -   Batch 10/158: Current loss = 0.0343, Running average = 0.0323
2025-11-30 12:23:22 - DEBUG -   Batch 20/158: Current loss = 0.0286, Running average = 0.0317
2025-11-30 12:23:25 - DEBUG -   Batch 30/158: Current loss = 0.0308, Running average = 0.0314
2025-11-30 12:23:28 - DEBUG -   Batch 40/158: Current loss = 0.0321, Running average = 0.0310
2025-11-30 12:23:31 - DEBUG -   Batch 50/158: Current loss = 0.0308, Running average = 0.0307
2025-11-30 12:23:35 - DEBUG -   Batch 60/158: Current loss = 0.0340, Running average = 0.0307
2025-11-30 12:23:38 - DEBUG -   Batch 70/158: Current loss = 0.0275, Running average = 0.0304
2025-11-30 12:23:41 - DEBUG -   Batch 80/158: Current loss = 0.0304, Running average = 0.0302
2025-11-30 12:23:45 - DEBUG -   Batch 90/158: Current loss = 0.0265, Running average = 0.0303
2025-11-30 12:23:48 - DEBUG -   Batch 100/158: Current loss = 0.0310, Running average = 0.0301
2025-11-30 12:23:51 - DEBUG -   Batch 110/158: Current loss = 0.0208, Running average = 0.0299
2025-11-30 12:23:55 - DEBUG -   Batch 120/158: Current loss = 0.0311, Running average = 0.0297
2025-11-30 12:23:58 - DEBUG -   Batch 130/158: Current loss = 0.0309, Running average = 0.0295
2025-11-30 12:24:01 - DEBUG -   Batch 140/158: Current loss = 0.0266, Running average = 0.0295
2025-11-30 12:24:04 - DEBUG -   Batch 150/158: Current loss = 0.0245, Running average = 0.0294
2025-11-30 12:24:07 - INFO - Epoch statistics:
2025-11-30 12:24:07 - INFO -   - Average loss: 0.0293
2025-11-30 12:24:07 - INFO -   - Min batch loss: 0.0199
2025-11-30 12:24:07 - INFO -   - Max batch loss: 0.0381
2025-11-30 12:24:07 - INFO - 
2025-11-30 12:24:07 - INFO - Epoch 4/10 completed - Average Loss: 0.0293
2025-11-30 12:24:07 - INFO - 
2025-11-30 12:24:07 - INFO - 
2025-11-30 12:24:07 - INFO - --------------------------------------------------------------------------------
2025-11-30 12:24:07 - INFO - EPOCH 5/10
2025-11-30 12:24:07 - INFO - --------------------------------------------------------------------------------
2025-11-30 12:24:07 - INFO - 
2025-11-30 12:24:07 - INFO - Processing 158 batches...
2025-11-30 12:24:07 - DEBUG -   First batch gradient norm: 0.0460
2025-11-30 12:24:10 - DEBUG -   Batch 10/158: Current loss = 0.0234, Running average = 0.0268
2025-11-30 12:24:14 - DEBUG -   Batch 20/158: Current loss = 0.0258, Running average = 0.0267
2025-11-30 12:24:17 - DEBUG -   Batch 30/158: Current loss = 0.0250, Running average = 0.0259
2025-11-30 12:24:20 - DEBUG -   Batch 40/158: Current loss = 0.0229, Running average = 0.0253
2025-11-30 12:24:23 - DEBUG -   Batch 50/158: Current loss = 0.0245, Running average = 0.0250
2025-11-30 12:24:27 - DEBUG -   Batch 60/158: Current loss = 0.0201, Running average = 0.0249
2025-11-30 12:24:30 - DEBUG -   Batch 70/158: Current loss = 0.0252, Running average = 0.0250
2025-11-30 12:24:34 - DEBUG -   Batch 80/158: Current loss = 0.0257, Running average = 0.0250
2025-11-30 12:24:38 - DEBUG -   Batch 90/158: Current loss = 0.0290, Running average = 0.0250
2025-11-30 12:24:42 - DEBUG -   Batch 100/158: Current loss = 0.0243, Running average = 0.0250
2025-11-30 12:24:46 - DEBUG -   Batch 110/158: Current loss = 0.0256, Running average = 0.0250
2025-11-30 12:24:50 - DEBUG -   Batch 120/158: Current loss = 0.0223, Running average = 0.0250
2025-11-30 12:24:54 - DEBUG -   Batch 130/158: Current loss = 0.0271, Running average = 0.0251
2025-11-30 12:24:57 - DEBUG -   Batch 140/158: Current loss = 0.0267, Running average = 0.0251
2025-11-30 12:25:01 - DEBUG -   Batch 150/158: Current loss = 0.0301, Running average = 0.0251
2025-11-30 12:25:04 - INFO - Epoch statistics:
2025-11-30 12:25:04 - INFO -   - Average loss: 0.0249
2025-11-30 12:25:04 - INFO -   - Min batch loss: 0.0184
2025-11-30 12:25:04 - INFO -   - Max batch loss: 0.0313
2025-11-30 12:25:04 - INFO - 
2025-11-30 12:25:04 - INFO - Epoch 5/10 completed - Average Loss: 0.0249
2025-11-30 12:25:04 - INFO - 
2025-11-30 12:25:04 - INFO - 
2025-11-30 12:25:04 - INFO - --------------------------------------------------------------------------------
2025-11-30 12:25:04 - INFO - EPOCH 6/10
2025-11-30 12:25:04 - INFO - --------------------------------------------------------------------------------
2025-11-30 12:25:04 - INFO - 
2025-11-30 12:25:04 - INFO - Processing 158 batches...
2025-11-30 12:25:05 - DEBUG -   First batch gradient norm: 0.0374
2025-11-30 12:25:08 - DEBUG -   Batch 10/158: Current loss = 0.0225, Running average = 0.0239
2025-11-30 12:25:12 - DEBUG -   Batch 20/158: Current loss = 0.0226, Running average = 0.0239
2025-11-30 12:25:16 - DEBUG -   Batch 30/158: Current loss = 0.0236, Running average = 0.0242
2025-11-30 12:25:20 - DEBUG -   Batch 40/158: Current loss = 0.0198, Running average = 0.0237
2025-11-30 12:25:24 - DEBUG -   Batch 50/158: Current loss = 0.0207, Running average = 0.0236
2025-11-30 12:25:28 - DEBUG -   Batch 60/158: Current loss = 0.0277, Running average = 0.0237
2025-11-30 12:25:32 - DEBUG -   Batch 70/158: Current loss = 0.0222, Running average = 0.0237
2025-11-30 12:25:35 - DEBUG -   Batch 80/158: Current loss = 0.0254, Running average = 0.0235
2025-11-30 12:25:39 - DEBUG -   Batch 90/158: Current loss = 0.0248, Running average = 0.0235
2025-11-30 12:25:43 - DEBUG -   Batch 100/158: Current loss = 0.0218, Running average = 0.0236
2025-11-30 12:25:47 - DEBUG -   Batch 110/158: Current loss = 0.0229, Running average = 0.0236
2025-11-30 12:25:51 - DEBUG -   Batch 120/158: Current loss = 0.0202, Running average = 0.0235
2025-11-30 12:25:55 - DEBUG -   Batch 130/158: Current loss = 0.0210, Running average = 0.0234
2025-11-30 12:25:59 - DEBUG -   Batch 140/158: Current loss = 0.0256, Running average = 0.0234
2025-11-30 12:26:03 - DEBUG -   Batch 150/158: Current loss = 0.0235, Running average = 0.0233
2025-11-30 12:26:06 - INFO - Epoch statistics:
2025-11-30 12:26:06 - INFO -   - Average loss: 0.0233
2025-11-30 12:26:06 - INFO -   - Min batch loss: 0.0155
2025-11-30 12:26:06 - INFO -   - Max batch loss: 0.0319
2025-11-30 12:26:06 - INFO - 
2025-11-30 12:26:06 - INFO - Epoch 6/10 completed - Average Loss: 0.0233
2025-11-30 12:26:06 - INFO - 
2025-11-30 12:26:06 - INFO - 
2025-11-30 12:26:06 - INFO - --------------------------------------------------------------------------------
2025-11-30 12:26:06 - INFO - EPOCH 7/10
2025-11-30 12:26:06 - INFO - --------------------------------------------------------------------------------
2025-11-30 12:26:06 - INFO - 
2025-11-30 12:26:06 - INFO - Processing 158 batches...
2025-11-30 12:26:06 - DEBUG -   First batch gradient norm: 0.0370
2025-11-30 12:26:10 - DEBUG -   Batch 10/158: Current loss = 0.0196, Running average = 0.0216
2025-11-30 12:26:13 - DEBUG -   Batch 20/158: Current loss = 0.0198, Running average = 0.0218
2025-11-30 12:26:17 - DEBUG -   Batch 30/158: Current loss = 0.0244, Running average = 0.0217
2025-11-30 12:26:21 - DEBUG -   Batch 40/158: Current loss = 0.0216, Running average = 0.0218
2025-11-30 12:26:25 - DEBUG -   Batch 50/158: Current loss = 0.0187, Running average = 0.0216
2025-11-30 12:26:29 - DEBUG -   Batch 60/158: Current loss = 0.0229, Running average = 0.0217
2025-11-30 12:26:33 - DEBUG -   Batch 70/158: Current loss = 0.0204, Running average = 0.0217
2025-11-30 12:26:37 - DEBUG -   Batch 80/158: Current loss = 0.0231, Running average = 0.0217
2025-11-30 12:26:41 - DEBUG -   Batch 90/158: Current loss = 0.0232, Running average = 0.0218
2025-11-30 12:26:45 - DEBUG -   Batch 100/158: Current loss = 0.0215, Running average = 0.0218
2025-11-30 12:26:48 - DEBUG -   Batch 110/158: Current loss = 0.0247, Running average = 0.0218
2025-11-30 12:26:52 - DEBUG -   Batch 120/158: Current loss = 0.0246, Running average = 0.0218
2025-11-30 12:26:56 - DEBUG -   Batch 130/158: Current loss = 0.0237, Running average = 0.0219
2025-11-30 12:27:00 - DEBUG -   Batch 140/158: Current loss = 0.0267, Running average = 0.0219
2025-11-30 12:27:04 - DEBUG -   Batch 150/158: Current loss = 0.0209, Running average = 0.0219
2025-11-30 12:27:07 - INFO - Epoch statistics:
2025-11-30 12:27:07 - INFO -   - Average loss: 0.0219
2025-11-30 12:27:07 - INFO -   - Min batch loss: 0.0177
2025-11-30 12:27:07 - INFO -   - Max batch loss: 0.0280
2025-11-30 12:27:07 - INFO - 
2025-11-30 12:27:07 - INFO - Epoch 7/10 completed - Average Loss: 0.0219
2025-11-30 12:27:07 - INFO - 
2025-11-30 12:27:07 - INFO - 
2025-11-30 12:27:07 - INFO - --------------------------------------------------------------------------------
2025-11-30 12:27:07 - INFO - EPOCH 8/10
2025-11-30 12:27:07 - INFO - --------------------------------------------------------------------------------
2025-11-30 12:27:07 - INFO - 
2025-11-30 12:27:07 - INFO - Processing 158 batches...
2025-11-30 12:27:07 - DEBUG -   First batch gradient norm: 0.0366
2025-11-30 12:27:11 - DEBUG -   Batch 10/158: Current loss = 0.0221, Running average = 0.0217
2025-11-30 12:27:15 - DEBUG -   Batch 20/158: Current loss = 0.0224, Running average = 0.0213
2025-11-30 12:27:18 - DEBUG -   Batch 30/158: Current loss = 0.0227, Running average = 0.0210
2025-11-30 12:27:22 - DEBUG -   Batch 40/158: Current loss = 0.0192, Running average = 0.0212
2025-11-30 12:27:26 - DEBUG -   Batch 50/158: Current loss = 0.0201, Running average = 0.0211
2025-11-30 12:27:30 - DEBUG -   Batch 60/158: Current loss = 0.0231, Running average = 0.0213
2025-11-30 12:27:34 - DEBUG -   Batch 70/158: Current loss = 0.0221, Running average = 0.0213
2025-11-30 12:27:38 - DEBUG -   Batch 80/158: Current loss = 0.0202, Running average = 0.0213
2025-11-30 12:27:42 - DEBUG -   Batch 90/158: Current loss = 0.0215, Running average = 0.0212
2025-11-30 12:27:46 - DEBUG -   Batch 100/158: Current loss = 0.0236, Running average = 0.0213
2025-11-30 12:27:50 - DEBUG -   Batch 110/158: Current loss = 0.0206, Running average = 0.0212
2025-11-30 12:27:53 - DEBUG -   Batch 120/158: Current loss = 0.0187, Running average = 0.0212
2025-11-30 12:27:57 - DEBUG -   Batch 130/158: Current loss = 0.0227, Running average = 0.0212
2025-11-30 12:28:01 - DEBUG -   Batch 140/158: Current loss = 0.0251, Running average = 0.0213
2025-11-30 12:28:05 - DEBUG -   Batch 150/158: Current loss = 0.0239, Running average = 0.0213
2025-11-30 12:28:08 - INFO - Epoch statistics:
2025-11-30 12:28:08 - INFO -   - Average loss: 0.0212
2025-11-30 12:28:08 - INFO -   - Min batch loss: 0.0162
2025-11-30 12:28:08 - INFO -   - Max batch loss: 0.0276
2025-11-30 12:28:08 - INFO - 
2025-11-30 12:28:08 - INFO - Epoch 8/10 completed - Average Loss: 0.0212
2025-11-30 12:28:08 - INFO - 
2025-11-30 12:28:08 - INFO - 
2025-11-30 12:28:08 - INFO - --------------------------------------------------------------------------------
2025-11-30 12:28:08 - INFO - EPOCH 9/10
2025-11-30 12:28:08 - INFO - --------------------------------------------------------------------------------
2025-11-30 12:28:08 - INFO - 
2025-11-30 12:28:08 - INFO - Processing 158 batches...
2025-11-30 12:28:08 - DEBUG -   First batch gradient norm: 0.0311
2025-11-30 12:28:12 - DEBUG -   Batch 10/158: Current loss = 0.0233, Running average = 0.0196
2025-11-30 12:28:16 - DEBUG -   Batch 20/158: Current loss = 0.0174, Running average = 0.0196
2025-11-30 12:28:20 - DEBUG -   Batch 30/158: Current loss = 0.0241, Running average = 0.0202
2025-11-30 12:28:23 - DEBUG -   Batch 40/158: Current loss = 0.0243, Running average = 0.0204
2025-11-30 12:28:27 - DEBUG -   Batch 50/158: Current loss = 0.0211, Running average = 0.0204
2025-11-30 12:28:31 - DEBUG -   Batch 60/158: Current loss = 0.0244, Running average = 0.0206
2025-11-30 12:28:35 - DEBUG -   Batch 70/158: Current loss = 0.0201, Running average = 0.0207
2025-11-30 12:28:39 - DEBUG -   Batch 80/158: Current loss = 0.0197, Running average = 0.0207
2025-11-30 12:28:43 - DEBUG -   Batch 90/158: Current loss = 0.0199, Running average = 0.0207
2025-11-30 12:28:47 - DEBUG -   Batch 100/158: Current loss = 0.0210, Running average = 0.0208
2025-11-30 12:28:50 - DEBUG -   Batch 110/158: Current loss = 0.0211, Running average = 0.0209
2025-11-30 12:28:54 - DEBUG -   Batch 120/158: Current loss = 0.0219, Running average = 0.0209
2025-11-30 12:28:58 - DEBUG -   Batch 130/158: Current loss = 0.0193, Running average = 0.0208
2025-11-30 12:29:02 - DEBUG -   Batch 140/158: Current loss = 0.0168, Running average = 0.0208
2025-11-30 12:29:06 - DEBUG -   Batch 150/158: Current loss = 0.0221, Running average = 0.0208
2025-11-30 12:29:09 - INFO - Epoch statistics:
2025-11-30 12:29:09 - INFO -   - Average loss: 0.0207
2025-11-30 12:29:09 - INFO -   - Min batch loss: 0.0149
2025-11-30 12:29:09 - INFO -   - Max batch loss: 0.0260
2025-11-30 12:29:09 - INFO - 
2025-11-30 12:29:09 - INFO - Epoch 9/10 completed - Average Loss: 0.0207
2025-11-30 12:29:09 - INFO - 
2025-11-30 12:29:09 - INFO - 
2025-11-30 12:29:09 - INFO - --------------------------------------------------------------------------------
2025-11-30 12:29:09 - INFO - EPOCH 10/10
2025-11-30 12:29:09 - INFO - --------------------------------------------------------------------------------
2025-11-30 12:29:09 - INFO - 
2025-11-30 12:29:09 - INFO - Processing 158 batches...
2025-11-30 12:29:09 - DEBUG -   First batch gradient norm: 0.0350
2025-11-30 12:29:13 - DEBUG -   Batch 10/158: Current loss = 0.0196, Running average = 0.0197
2025-11-30 12:29:17 - DEBUG -   Batch 20/158: Current loss = 0.0199, Running average = 0.0199
2025-11-30 12:29:20 - DEBUG -   Batch 30/158: Current loss = 0.0206, Running average = 0.0202
2025-11-30 12:29:24 - DEBUG -   Batch 40/158: Current loss = 0.0185, Running average = 0.0201
2025-11-30 12:29:28 - DEBUG -   Batch 50/158: Current loss = 0.0214, Running average = 0.0202
2025-11-30 12:29:32 - DEBUG -   Batch 60/158: Current loss = 0.0210, Running average = 0.0201
2025-11-30 12:29:36 - DEBUG -   Batch 70/158: Current loss = 0.0178, Running average = 0.0200
2025-11-30 12:29:40 - DEBUG -   Batch 80/158: Current loss = 0.0190, Running average = 0.0201
2025-11-30 12:29:44 - DEBUG -   Batch 90/158: Current loss = 0.0217, Running average = 0.0201
2025-11-30 12:29:48 - DEBUG -   Batch 100/158: Current loss = 0.0202, Running average = 0.0202
2025-11-30 12:29:52 - DEBUG -   Batch 110/158: Current loss = 0.0232, Running average = 0.0202
2025-11-30 12:29:56 - DEBUG -   Batch 120/158: Current loss = 0.0180, Running average = 0.0202
2025-11-30 12:30:00 - DEBUG -   Batch 130/158: Current loss = 0.0191, Running average = 0.0203
2025-11-30 12:30:04 - DEBUG -   Batch 140/158: Current loss = 0.0168, Running average = 0.0202
2025-11-30 12:30:08 - DEBUG -   Batch 150/158: Current loss = 0.0217, Running average = 0.0203
2025-11-30 12:30:11 - INFO - Epoch statistics:
2025-11-30 12:30:11 - INFO -   - Average loss: 0.0202
2025-11-30 12:30:11 - INFO -   - Min batch loss: 0.0151
2025-11-30 12:30:11 - INFO -   - Max batch loss: 0.0252
2025-11-30 12:30:11 - INFO - 
2025-11-30 12:30:11 - INFO - Epoch 10/10 completed - Average Loss: 0.0202
2025-11-30 12:30:11 - INFO - 
2025-11-30 12:30:11 - INFO - ================================================================================
2025-11-30 12:30:11 - INFO - TRAINING COMPLETE
2025-11-30 12:30:11 - INFO - ================================================================================
2025-11-30 12:30:11 - INFO - 
2025-11-30 12:30:11 - INFO - Training duration: 00:09:32 (572.41 seconds)
2025-11-30 12:30:11 - INFO - 
2025-11-30 12:30:11 - INFO - ================================================================================
2025-11-30 12:30:11 - INFO - STEP 8: MODEL SAVING
2025-11-30 12:30:11 - INFO - ================================================================================
2025-11-30 12:30:11 - INFO - 
2025-11-30 12:30:11 - INFO - Saving trained model to: 50_models\dataset_toy\embed128_layers3_heads4_epochs10.pt
2025-11-30 12:30:11 - INFO - 
2025-11-30 12:30:11 - INFO - Saving model components:
2025-11-30 12:30:11 - INFO -   - Model weights (state_dict)
2025-11-30 12:30:11 - INFO -   - Vocabulary mappings (character to index)
2025-11-30 12:30:11 - INFO -   - Model hyperparameters
2025-11-30 12:30:11 - INFO - 
2025-11-30 12:30:11 - INFO - Model saved successfully!
2025-11-30 12:30:11 - INFO -   - File path: 50_models\dataset_toy\embed128_layers3_heads4_epochs10.pt
2025-11-30 12:30:11 - INFO -   - File size: 2.40 MB
2025-11-30 12:30:11 - INFO -   - Time taken: 0.016 seconds
2025-11-30 12:30:11 - INFO - 
2025-11-30 12:30:11 - INFO - ================================================================================
2025-11-30 12:30:11 - INFO - TRAINING SESSION ENDED
2025-11-30 12:30:11 - INFO - ================================================================================
2025-11-30 12:30:11 - INFO - 
