2025-11-30 11:20:15 - INFO - 
2025-11-30 11:20:15 - INFO - ================================================================================
2025-11-30 11:20:15 - INFO - TRAINING SESSION STARTED
2025-11-30 11:20:15 - INFO - ================================================================================
2025-11-30 11:20:15 - INFO - 
2025-11-30 11:20:15 - INFO - Log file: 97_logs\training_20251130_112015.log
2025-11-30 11:20:15 - INFO - Timestamp: 20251130_112015
2025-11-30 11:20:15 - INFO - 
2025-11-30 11:20:15 - INFO - Command line arguments parsed
2025-11-30 11:20:15 - INFO - 
2025-11-30 11:20:15 - INFO - 
2025-11-30 11:20:15 - INFO - ================================================================================
2025-11-30 11:20:15 - INFO - TRAINING CONFIGURATION
2025-11-30 11:20:15 - INFO - ================================================================================
2025-11-30 11:20:15 - INFO - 
2025-11-30 11:20:15 - INFO - Training Parameters:
2025-11-30 11:20:15 - INFO -   - Dataset folder: 40_training_data/dataset_toy/
2025-11-30 11:20:15 - INFO -   - Total epochs: 10
2025-11-30 11:20:15 - INFO -   - Batch size: 32
2025-11-30 11:20:15 - INFO -   - Sequence length (block_size): 128
2025-11-30 11:20:15 - INFO -   - Embedding dimension: 128
2025-11-30 11:20:15 - INFO -   - Attention heads: 4
2025-11-30 11:20:15 - INFO -   - Transformer layers: 3
2025-11-30 11:20:15 - INFO -   - Learning rate: 0.001
2025-11-30 11:20:15 - INFO - 
2025-11-30 11:20:15 - INFO - ================================================================================
2025-11-30 11:20:15 - INFO - STEP 1: DEVICE SELECTION
2025-11-30 11:20:15 - INFO - ================================================================================
2025-11-30 11:20:15 - INFO - 
2025-11-30 11:20:15 - INFO - Selected device: cpu
2025-11-30 11:20:15 - INFO -   - Using CPU (no CUDA available)
2025-11-30 11:20:15 - INFO - 
2025-11-30 11:20:15 - INFO - ================================================================================
2025-11-30 11:20:15 - INFO - STEP 2: DATA LOADING
2025-11-30 11:20:15 - INFO - ================================================================================
2025-11-30 11:20:15 - INFO - 
2025-11-30 11:20:15 - INFO - Loading all .txt files from: 40_training_data/dataset_toy/
2025-11-30 11:20:15 - INFO - 
2025-11-30 11:20:15 - INFO - Data loaded successfully:
2025-11-30 11:20:15 - INFO -   - Total characters: 5,178
2025-11-30 11:20:15 - INFO -   - Total lines: 43
2025-11-30 11:20:15 - INFO - 
2025-11-30 11:20:15 - INFO - ================================================================================
2025-11-30 11:20:15 - INFO - STEP 3: DATASET PREPARATION
2025-11-30 11:20:15 - INFO - ================================================================================
2025-11-30 11:20:15 - INFO - 
2025-11-30 11:20:15 - INFO - Creating character-level dataset...
2025-11-30 11:20:15 - INFO -   - Processing text into sequences of length 128
2025-11-30 11:20:15 - DEBUG - Creating character-level dataset...
2025-11-30 11:20:15 - DEBUG -   - Input text length: 5178 characters
2025-11-30 11:20:15 - DEBUG -   - Block size: 128
2025-11-30 11:20:15 - DEBUG -   - Vocabulary size: 48
2025-11-30 11:20:15 - DEBUG -   - Unique characters: 
 ',-.ABCDEFHILMNPRSTWabcdefghijklmnopqrstuvwxyz
2025-11-30 11:20:15 - DEBUG -   - Encoding text to integer sequences...
2025-11-30 11:20:15 - DEBUG -   - Encoded data length: 5178 tokens
2025-11-30 11:20:15 - INFO - 
2025-11-30 11:20:15 - INFO - Dataset created:
2025-11-30 11:20:15 - INFO -   - Vocabulary size: 48 unique characters
2025-11-30 11:20:15 - INFO -   - Total training sequences: 5,050
2025-11-30 11:20:15 - INFO -   - Total tokens: 5,178
2025-11-30 11:20:15 - INFO - 
2025-11-30 11:20:15 - INFO - ================================================================================
2025-11-30 11:20:15 - INFO - STEP 4: DATALOADER SETUP
2025-11-30 11:20:15 - INFO - ================================================================================
2025-11-30 11:20:15 - INFO - 
2025-11-30 11:20:15 - INFO - Creating DataLoader with batch_size=32 (shuffling enabled)...
2025-11-30 11:20:15 - INFO -   - Total batches per epoch: 158
2025-11-30 11:20:15 - INFO -   - Samples per batch: 32
2025-11-30 11:20:15 - INFO - 
2025-11-30 11:20:15 - INFO - ================================================================================
2025-11-30 11:20:15 - INFO - STEP 5: MODEL INITIALIZATION
2025-11-30 11:20:15 - INFO - ================================================================================
2025-11-30 11:20:15 - INFO - 
2025-11-30 11:20:15 - INFO - Building transformer-based language model...
2025-11-30 11:20:15 - INFO -   - Architecture: 3 transformer layers
2025-11-30 11:20:15 - INFO -   - Embedding dimension: 128
2025-11-30 11:20:15 - INFO -   - Attention heads per layer: 4
2025-11-30 11:20:15 - INFO -   - Vocabulary size: 48
2025-11-30 11:20:15 - INFO -   - Context window: 128 tokens
2025-11-30 11:20:15 - DEBUG - Initializing transformer model components...
2025-11-30 11:20:15 - DEBUG -   - Vocab size: 48, Embed dim: 128
2025-11-30 11:20:15 - DEBUG -   - Heads: 4, Layers: 3, Block size: 128
2025-11-30 11:20:15 - DEBUG -   - Created token embedding: 48 x 128
2025-11-30 11:20:15 - DEBUG -   - Created positional embedding: 128 x 128
2025-11-30 11:20:15 - DEBUG -   - Creating 3 transformer encoder layers...
2025-11-30 11:20:15 - DEBUG -   - Transformer blocks created: 3 layers
2025-11-30 11:20:15 - DEBUG -   - Output layer: Linear(128 -> 48)
2025-11-30 11:20:15 - INFO - 
2025-11-30 11:20:15 - INFO - Model initialized:
2025-11-30 11:20:15 - INFO -   - Total parameters: 623,792
2025-11-30 11:20:15 - INFO -   - Trainable parameters: 623,792
2025-11-30 11:20:15 - INFO -   - Model location: cpu
2025-11-30 11:20:15 - INFO - 
2025-11-30 11:20:15 - INFO - ================================================================================
2025-11-30 11:20:15 - INFO - STEP 6: TRAINING SETUP
2025-11-30 11:20:15 - INFO - ================================================================================
2025-11-30 11:20:15 - INFO - 
2025-11-30 11:20:15 - INFO - Configuring training components:
2025-11-30 11:20:15 - INFO -   - Loss function: CrossEntropyLoss
2025-11-30 11:20:15 - INFO -   - Optimizer: AdamW (learning rate: 0.001)
2025-11-30 11:20:16 - INFO - 
2025-11-30 11:20:16 - INFO - ================================================================================
2025-11-30 11:20:16 - INFO - STEP 7: TRAINING LOOP
2025-11-30 11:20:16 - INFO - ================================================================================
2025-11-30 11:20:16 - INFO - 
2025-11-30 11:20:16 - INFO - Starting training: 10 epochs, 158 batches per epoch
2025-11-30 11:20:16 - INFO - 
2025-11-30 11:20:16 - INFO - 
2025-11-30 11:20:16 - INFO - --------------------------------------------------------------------------------
2025-11-30 11:20:16 - INFO - EPOCH 1/10
2025-11-30 11:20:16 - INFO - --------------------------------------------------------------------------------
2025-11-30 11:20:16 - INFO - 
2025-11-30 11:20:16 - INFO - Processing 158 batches...
2025-11-30 11:20:17 - DEBUG -   First batch gradient norm: 1.5888
2025-11-30 11:20:20 - DEBUG -   Batch 10/158: Current loss = 2.9145, Running average = 3.1792
2025-11-30 11:20:24 - DEBUG -   Batch 20/158: Current loss = 2.6669, Running average = 2.9669
2025-11-30 11:20:27 - DEBUG -   Batch 30/158: Current loss = 2.5654, Running average = 2.8413
2025-11-30 11:20:31 - DEBUG -   Batch 40/158: Current loss = 2.4837, Running average = 2.7595
2025-11-30 11:20:34 - DEBUG -   Batch 50/158: Current loss = 2.4513, Running average = 2.6982
2025-11-30 11:20:37 - DEBUG -   Batch 60/158: Current loss = 2.4489, Running average = 2.6541
2025-11-30 11:20:41 - DEBUG -   Batch 70/158: Current loss = 2.4162, Running average = 2.6193
2025-11-30 11:20:44 - DEBUG -   Batch 80/158: Current loss = 2.3655, Running average = 2.5898
2025-11-30 11:20:47 - DEBUG -   Batch 90/158: Current loss = 2.3547, Running average = 2.5646
2025-11-30 11:20:51 - DEBUG -   Batch 100/158: Current loss = 2.3247, Running average = 2.5419
2025-11-30 11:20:54 - DEBUG -   Batch 110/158: Current loss = 2.2898, Running average = 2.5213
2025-11-30 11:20:57 - DEBUG -   Batch 120/158: Current loss = 2.2334, Running average = 2.5012
2025-11-30 11:21:00 - DEBUG -   Batch 130/158: Current loss = 2.0912, Running average = 2.4752
2025-11-30 11:21:04 - DEBUG -   Batch 140/158: Current loss = 1.7990, Running average = 2.4368
2025-11-30 11:21:07 - DEBUG -   Batch 150/158: Current loss = 1.3852, Running average = 2.3796
2025-11-30 11:21:10 - INFO - Epoch statistics:
2025-11-30 11:21:10 - INFO -   - Average loss: 2.3187
2025-11-30 11:21:10 - INFO -   - Min batch loss: 1.0197
2025-11-30 11:21:10 - INFO -   - Max batch loss: 4.0036
2025-11-30 11:21:10 - INFO - 
2025-11-30 11:21:10 - INFO - Epoch 1/10 completed - Average Loss: 2.3187
2025-11-30 11:21:10 - INFO - 
2025-11-30 11:21:10 - INFO - 
2025-11-30 11:21:10 - INFO - --------------------------------------------------------------------------------
2025-11-30 11:21:10 - INFO - EPOCH 2/10
2025-11-30 11:21:10 - INFO - --------------------------------------------------------------------------------
2025-11-30 11:21:10 - INFO - 
2025-11-30 11:21:10 - INFO - Processing 158 batches...
2025-11-30 11:21:10 - DEBUG -   First batch gradient norm: 0.4595
2025-11-30 11:21:13 - DEBUG -   Batch 10/158: Current loss = 0.6306, Running average = 0.7878
2025-11-30 11:21:16 - DEBUG -   Batch 20/158: Current loss = 0.3880, Running average = 0.6430
2025-11-30 11:21:19 - DEBUG -   Batch 30/158: Current loss = 0.2965, Running average = 0.5400
2025-11-30 11:21:23 - DEBUG -   Batch 40/158: Current loss = 0.2178, Running average = 0.4676
2025-11-30 11:21:26 - DEBUG -   Batch 50/158: Current loss = 0.1852, Running average = 0.4143
2025-11-30 11:21:29 - DEBUG -   Batch 60/158: Current loss = 0.1495, Running average = 0.3726
2025-11-30 11:21:33 - DEBUG -   Batch 70/158: Current loss = 0.1371, Running average = 0.3397
2025-11-30 11:21:36 - DEBUG -   Batch 80/158: Current loss = 0.1199, Running average = 0.3131
2025-11-30 11:21:39 - DEBUG -   Batch 90/158: Current loss = 0.1062, Running average = 0.2911
2025-11-30 11:21:42 - DEBUG -   Batch 100/158: Current loss = 0.1181, Running average = 0.2720
2025-11-30 11:21:46 - DEBUG -   Batch 110/158: Current loss = 0.0923, Running average = 0.2558
2025-11-30 11:21:49 - DEBUG -   Batch 120/158: Current loss = 0.0716, Running average = 0.2411
2025-11-30 11:21:52 - DEBUG -   Batch 130/158: Current loss = 0.0697, Running average = 0.2281
2025-11-30 11:21:56 - DEBUG -   Batch 140/158: Current loss = 0.0545, Running average = 0.2163
2025-11-30 11:21:59 - DEBUG -   Batch 150/158: Current loss = 0.0566, Running average = 0.2056
2025-11-30 11:22:01 - INFO - Epoch statistics:
2025-11-30 11:22:01 - INFO -   - Average loss: 0.1979
2025-11-30 11:22:01 - INFO -   - Min batch loss: 0.0447
2025-11-30 11:22:01 - INFO -   - Max batch loss: 1.0042
2025-11-30 11:22:01 - INFO - 
2025-11-30 11:22:01 - INFO - Epoch 2/10 completed - Average Loss: 0.1979
2025-11-30 11:22:01 - INFO - 
2025-11-30 11:22:01 - INFO - 
2025-11-30 11:22:01 - INFO - --------------------------------------------------------------------------------
2025-11-30 11:22:01 - INFO - EPOCH 3/10
2025-11-30 11:22:01 - INFO - --------------------------------------------------------------------------------
2025-11-30 11:22:01 - INFO - 
2025-11-30 11:22:01 - INFO - Processing 158 batches...
2025-11-30 11:22:02 - DEBUG -   First batch gradient norm: 0.0823
2025-11-30 11:22:05 - DEBUG -   Batch 10/158: Current loss = 0.0464, Running average = 0.0496
2025-11-30 11:22:08 - DEBUG -   Batch 20/158: Current loss = 0.0472, Running average = 0.0491
2025-11-30 11:22:11 - DEBUG -   Batch 30/158: Current loss = 0.0452, Running average = 0.0482
2025-11-30 11:22:15 - DEBUG -   Batch 40/158: Current loss = 0.0412, Running average = 0.0476
2025-11-30 11:22:18 - DEBUG -   Batch 50/158: Current loss = 0.0450, Running average = 0.0469
2025-11-30 11:22:21 - DEBUG -   Batch 60/158: Current loss = 0.0446, Running average = 0.0465
2025-11-30 11:22:25 - DEBUG -   Batch 70/158: Current loss = 0.0390, Running average = 0.0460
2025-11-30 11:22:28 - DEBUG -   Batch 80/158: Current loss = 0.0394, Running average = 0.0455
2025-11-30 11:22:31 - DEBUG -   Batch 90/158: Current loss = 0.0387, Running average = 0.0451
2025-11-30 11:22:35 - DEBUG -   Batch 100/158: Current loss = 0.0377, Running average = 0.0446
2025-11-30 11:22:38 - DEBUG -   Batch 110/158: Current loss = 0.0358, Running average = 0.0439
2025-11-30 11:22:41 - DEBUG -   Batch 120/158: Current loss = 0.0382, Running average = 0.0435
2025-11-30 11:22:45 - DEBUG -   Batch 130/158: Current loss = 0.0362, Running average = 0.0433
2025-11-30 11:22:48 - DEBUG -   Batch 140/158: Current loss = 0.0367, Running average = 0.0429
2025-11-30 11:22:51 - DEBUG -   Batch 150/158: Current loss = 0.0365, Running average = 0.0425
2025-11-30 11:22:54 - INFO - Epoch statistics:
2025-11-30 11:22:54 - INFO -   - Average loss: 0.0423
2025-11-30 11:22:54 - INFO -   - Min batch loss: 0.0308
2025-11-30 11:22:54 - INFO -   - Max batch loss: 0.0572
2025-11-30 11:22:54 - INFO - 
2025-11-30 11:22:54 - INFO - Epoch 3/10 completed - Average Loss: 0.0423
2025-11-30 11:22:54 - INFO - 
2025-11-30 11:22:54 - INFO - 
2025-11-30 11:22:54 - INFO - --------------------------------------------------------------------------------
2025-11-30 11:22:54 - INFO - EPOCH 4/10
2025-11-30 11:22:54 - INFO - --------------------------------------------------------------------------------
2025-11-30 11:22:54 - INFO - 
2025-11-30 11:22:54 - INFO - Processing 158 batches...
2025-11-30 11:22:54 - DEBUG -   First batch gradient norm: 0.0660
2025-11-30 11:22:57 - DEBUG -   Batch 10/158: Current loss = 0.0426, Running average = 0.0366
2025-11-30 11:23:00 - DEBUG -   Batch 20/158: Current loss = 0.0341, Running average = 0.0350
2025-11-30 11:23:04 - DEBUG -   Batch 30/158: Current loss = 0.0276, Running average = 0.0350
2025-11-30 11:23:07 - DEBUG -   Batch 40/158: Current loss = 0.0305, Running average = 0.0349
2025-11-30 11:23:11 - DEBUG -   Batch 50/158: Current loss = 0.0352, Running average = 0.0344
2025-11-30 11:23:14 - DEBUG -   Batch 60/158: Current loss = 0.0291, Running average = 0.0341
2025-11-30 11:23:17 - DEBUG -   Batch 70/158: Current loss = 0.0290, Running average = 0.0340
2025-11-30 11:23:21 - DEBUG -   Batch 80/158: Current loss = 0.0347, Running average = 0.0338
2025-11-30 11:23:24 - DEBUG -   Batch 90/158: Current loss = 0.0288, Running average = 0.0338
2025-11-30 11:23:28 - DEBUG -   Batch 100/158: Current loss = 0.0337, Running average = 0.0337
2025-11-30 11:23:31 - DEBUG -   Batch 110/158: Current loss = 0.0298, Running average = 0.0334
2025-11-30 11:23:34 - DEBUG -   Batch 120/158: Current loss = 0.0318, Running average = 0.0332
2025-11-30 11:23:38 - DEBUG -   Batch 130/158: Current loss = 0.0270, Running average = 0.0330
2025-11-30 11:23:41 - DEBUG -   Batch 140/158: Current loss = 0.0267, Running average = 0.0327
2025-11-30 11:23:44 - DEBUG -   Batch 150/158: Current loss = 0.0245, Running average = 0.0325
2025-11-30 11:23:47 - INFO - Epoch statistics:
2025-11-30 11:23:47 - INFO -   - Average loss: 0.0324
2025-11-30 11:23:47 - INFO -   - Min batch loss: 0.0245
2025-11-30 11:23:47 - INFO -   - Max batch loss: 0.0459
2025-11-30 11:23:47 - INFO - 
2025-11-30 11:23:47 - INFO - Epoch 4/10 completed - Average Loss: 0.0324
2025-11-30 11:23:47 - INFO - 
2025-11-30 11:23:47 - INFO - 
2025-11-30 11:23:47 - INFO - --------------------------------------------------------------------------------
2025-11-30 11:23:47 - INFO - EPOCH 5/10
2025-11-30 11:23:47 - INFO - --------------------------------------------------------------------------------
2025-11-30 11:23:47 - INFO - 
2025-11-30 11:23:47 - INFO - Processing 158 batches...
2025-11-30 11:23:47 - DEBUG -   First batch gradient norm: 0.0428
2025-11-30 11:23:50 - DEBUG -   Batch 10/158: Current loss = 0.0217, Running average = 0.0279
2025-11-30 11:23:54 - DEBUG -   Batch 20/158: Current loss = 0.0223, Running average = 0.0278
2025-11-30 11:23:57 - DEBUG -   Batch 30/158: Current loss = 0.0372, Running average = 0.0282
2025-11-30 11:24:01 - DEBUG -   Batch 40/158: Current loss = 0.0318, Running average = 0.0283
2025-11-30 11:24:05 - DEBUG -   Batch 50/158: Current loss = 0.0228, Running average = 0.0283
2025-11-30 11:24:08 - DEBUG -   Batch 60/158: Current loss = 0.0259, Running average = 0.0282
2025-11-30 11:24:12 - DEBUG -   Batch 70/158: Current loss = 0.0223, Running average = 0.0280
2025-11-30 11:24:15 - DEBUG -   Batch 80/158: Current loss = 0.0280, Running average = 0.0277
2025-11-30 11:24:19 - DEBUG -   Batch 90/158: Current loss = 0.0259, Running average = 0.0276
2025-11-30 11:24:22 - DEBUG -   Batch 100/158: Current loss = 0.0279, Running average = 0.0275
2025-11-30 11:24:25 - DEBUG -   Batch 110/158: Current loss = 0.0253, Running average = 0.0275
2025-11-30 11:24:28 - DEBUG -   Batch 120/158: Current loss = 0.0257, Running average = 0.0275
2025-11-30 11:24:32 - DEBUG -   Batch 130/158: Current loss = 0.0234, Running average = 0.0275
2025-11-30 11:24:35 - DEBUG -   Batch 140/158: Current loss = 0.0268, Running average = 0.0274
2025-11-30 11:24:38 - DEBUG -   Batch 150/158: Current loss = 0.0279, Running average = 0.0273
2025-11-30 11:24:41 - INFO - Epoch statistics:
2025-11-30 11:24:41 - INFO -   - Average loss: 0.0272
2025-11-30 11:24:41 - INFO -   - Min batch loss: 0.0204
2025-11-30 11:24:41 - INFO -   - Max batch loss: 0.0372
2025-11-30 11:24:41 - INFO - 
2025-11-30 11:24:41 - INFO - Epoch 5/10 completed - Average Loss: 0.0272
2025-11-30 11:24:41 - INFO - 
2025-11-30 11:24:41 - INFO - 
2025-11-30 11:24:41 - INFO - --------------------------------------------------------------------------------
2025-11-30 11:24:41 - INFO - EPOCH 6/10
2025-11-30 11:24:41 - INFO - --------------------------------------------------------------------------------
2025-11-30 11:24:41 - INFO - 
2025-11-30 11:24:41 - INFO - Processing 158 batches...
2025-11-30 11:24:41 - DEBUG -   First batch gradient norm: 0.0493
2025-11-30 11:24:44 - DEBUG -   Batch 10/158: Current loss = 0.0251, Running average = 0.0259
2025-11-30 11:24:48 - DEBUG -   Batch 20/158: Current loss = 0.0311, Running average = 0.0258
2025-11-30 11:24:51 - DEBUG -   Batch 30/158: Current loss = 0.0249, Running average = 0.0257
2025-11-30 11:24:54 - DEBUG -   Batch 40/158: Current loss = 0.0288, Running average = 0.0253
2025-11-30 11:24:58 - DEBUG -   Batch 50/158: Current loss = 0.0287, Running average = 0.0256
2025-11-30 11:25:01 - DEBUG -   Batch 60/158: Current loss = 0.0246, Running average = 0.0254
2025-11-30 11:25:04 - DEBUG -   Batch 70/158: Current loss = 0.0221, Running average = 0.0252
2025-11-30 11:25:08 - DEBUG -   Batch 80/158: Current loss = 0.0270, Running average = 0.0253
2025-11-30 11:25:11 - DEBUG -   Batch 90/158: Current loss = 0.0257, Running average = 0.0253
2025-11-30 11:25:14 - DEBUG -   Batch 100/158: Current loss = 0.0273, Running average = 0.0252
2025-11-30 11:25:18 - DEBUG -   Batch 110/158: Current loss = 0.0221, Running average = 0.0252
2025-11-30 11:25:21 - DEBUG -   Batch 120/158: Current loss = 0.0225, Running average = 0.0251
2025-11-30 11:25:24 - DEBUG -   Batch 130/158: Current loss = 0.0222, Running average = 0.0251
2025-11-30 11:25:28 - DEBUG -   Batch 140/158: Current loss = 0.0206, Running average = 0.0250
2025-11-30 11:25:31 - DEBUG -   Batch 150/158: Current loss = 0.0232, Running average = 0.0250
2025-11-30 11:25:34 - INFO - Epoch statistics:
2025-11-30 11:25:34 - INFO -   - Average loss: 0.0250
2025-11-30 11:25:34 - INFO -   - Min batch loss: 0.0203
2025-11-30 11:25:34 - INFO -   - Max batch loss: 0.0343
2025-11-30 11:25:34 - INFO - 
2025-11-30 11:25:34 - INFO - Epoch 6/10 completed - Average Loss: 0.0250
2025-11-30 11:25:34 - INFO - 
2025-11-30 11:25:34 - INFO - 
2025-11-30 11:25:34 - INFO - --------------------------------------------------------------------------------
2025-11-30 11:25:34 - INFO - EPOCH 7/10
2025-11-30 11:25:34 - INFO - --------------------------------------------------------------------------------
2025-11-30 11:25:34 - INFO - 
2025-11-30 11:25:34 - INFO - Processing 158 batches...
2025-11-30 11:25:34 - DEBUG -   First batch gradient norm: 0.0315
2025-11-30 11:25:37 - DEBUG -   Batch 10/158: Current loss = 0.0202, Running average = 0.0236
2025-11-30 11:25:40 - DEBUG -   Batch 20/158: Current loss = 0.0294, Running average = 0.0241
2025-11-30 11:25:43 - DEBUG -   Batch 30/158: Current loss = 0.0205, Running average = 0.0236
2025-11-30 11:25:47 - DEBUG -   Batch 40/158: Current loss = 0.0217, Running average = 0.0235
2025-11-30 11:25:50 - DEBUG -   Batch 50/158: Current loss = 0.0247, Running average = 0.0237
2025-11-30 11:25:53 - DEBUG -   Batch 60/158: Current loss = 0.0255, Running average = 0.0237
2025-11-30 11:25:57 - DEBUG -   Batch 70/158: Current loss = 0.0203, Running average = 0.0236
2025-11-30 11:26:00 - DEBUG -   Batch 80/158: Current loss = 0.0258, Running average = 0.0236
2025-11-30 11:26:04 - DEBUG -   Batch 90/158: Current loss = 0.0215, Running average = 0.0236
2025-11-30 11:26:07 - DEBUG -   Batch 100/158: Current loss = 0.0233, Running average = 0.0236
2025-11-30 11:26:10 - DEBUG -   Batch 110/158: Current loss = 0.0217, Running average = 0.0235
2025-11-30 11:26:14 - DEBUG -   Batch 120/158: Current loss = 0.0231, Running average = 0.0235
2025-11-30 11:26:17 - DEBUG -   Batch 130/158: Current loss = 0.0227, Running average = 0.0234
2025-11-30 11:26:20 - DEBUG -   Batch 140/158: Current loss = 0.0186, Running average = 0.0233
2025-11-30 11:26:24 - DEBUG -   Batch 150/158: Current loss = 0.0226, Running average = 0.0232
2025-11-30 11:26:26 - INFO - Epoch statistics:
2025-11-30 11:26:26 - INFO -   - Average loss: 0.0232
2025-11-30 11:26:26 - INFO -   - Min batch loss: 0.0169
2025-11-30 11:26:26 - INFO -   - Max batch loss: 0.0310
2025-11-30 11:26:26 - INFO - 
2025-11-30 11:26:26 - INFO - Epoch 7/10 completed - Average Loss: 0.0232
2025-11-30 11:26:26 - INFO - 
2025-11-30 11:26:26 - INFO - 
2025-11-30 11:26:26 - INFO - --------------------------------------------------------------------------------
2025-11-30 11:26:26 - INFO - EPOCH 8/10
2025-11-30 11:26:26 - INFO - --------------------------------------------------------------------------------
2025-11-30 11:26:26 - INFO - 
2025-11-30 11:26:26 - INFO - Processing 158 batches...
2025-11-30 11:26:27 - DEBUG -   First batch gradient norm: 0.0409
2025-11-30 11:26:30 - DEBUG -   Batch 10/158: Current loss = 0.0209, Running average = 0.0222
2025-11-30 11:26:33 - DEBUG -   Batch 20/158: Current loss = 0.0240, Running average = 0.0223
2025-11-30 11:26:36 - DEBUG -   Batch 30/158: Current loss = 0.0201, Running average = 0.0222
2025-11-30 11:26:40 - DEBUG -   Batch 40/158: Current loss = 0.0244, Running average = 0.0219
2025-11-30 11:26:43 - DEBUG -   Batch 50/158: Current loss = 0.0211, Running average = 0.0217
2025-11-30 11:26:46 - DEBUG -   Batch 60/158: Current loss = 0.0271, Running average = 0.0218
2025-11-30 11:26:49 - DEBUG -   Batch 70/158: Current loss = 0.0253, Running average = 0.0219
2025-11-30 11:26:53 - DEBUG -   Batch 80/158: Current loss = 0.0198, Running average = 0.0219
2025-11-30 11:26:56 - DEBUG -   Batch 90/158: Current loss = 0.0205, Running average = 0.0219
2025-11-30 11:26:59 - DEBUG -   Batch 100/158: Current loss = 0.0188, Running average = 0.0219
2025-11-30 11:27:03 - DEBUG -   Batch 110/158: Current loss = 0.0206, Running average = 0.0219
2025-11-30 11:27:06 - DEBUG -   Batch 120/158: Current loss = 0.0225, Running average = 0.0219
2025-11-30 11:27:09 - DEBUG -   Batch 130/158: Current loss = 0.0204, Running average = 0.0219
2025-11-30 11:27:13 - DEBUG -   Batch 140/158: Current loss = 0.0217, Running average = 0.0219
2025-11-30 11:27:16 - DEBUG -   Batch 150/158: Current loss = 0.0255, Running average = 0.0219
2025-11-30 11:27:19 - INFO - Epoch statistics:
2025-11-30 11:27:19 - INFO -   - Average loss: 0.0220
2025-11-30 11:27:19 - INFO -   - Min batch loss: 0.0162
2025-11-30 11:27:19 - INFO -   - Max batch loss: 0.0271
2025-11-30 11:27:19 - INFO - 
2025-11-30 11:27:19 - INFO - Epoch 8/10 completed - Average Loss: 0.0220
2025-11-30 11:27:19 - INFO - 
2025-11-30 11:27:19 - INFO - 
2025-11-30 11:27:19 - INFO - --------------------------------------------------------------------------------
2025-11-30 11:27:19 - INFO - EPOCH 9/10
2025-11-30 11:27:19 - INFO - --------------------------------------------------------------------------------
2025-11-30 11:27:19 - INFO - 
2025-11-30 11:27:19 - INFO - Processing 158 batches...
2025-11-30 11:27:19 - DEBUG -   First batch gradient norm: 0.0285
2025-11-30 11:27:22 - DEBUG -   Batch 10/158: Current loss = 0.0203, Running average = 0.0204
2025-11-30 11:27:25 - DEBUG -   Batch 20/158: Current loss = 0.0209, Running average = 0.0202
2025-11-30 11:27:29 - DEBUG -   Batch 30/158: Current loss = 0.0236, Running average = 0.0206
2025-11-30 11:27:32 - DEBUG -   Batch 40/158: Current loss = 0.0186, Running average = 0.0206
2025-11-30 11:27:35 - DEBUG -   Batch 50/158: Current loss = 0.0207, Running average = 0.0210
2025-11-30 11:27:39 - DEBUG -   Batch 60/158: Current loss = 0.0208, Running average = 0.0211
2025-11-30 11:27:42 - DEBUG -   Batch 70/158: Current loss = 0.0206, Running average = 0.0210
2025-11-30 11:27:46 - DEBUG -   Batch 80/158: Current loss = 0.0237, Running average = 0.0209
2025-11-30 11:27:49 - DEBUG -   Batch 90/158: Current loss = 0.0203, Running average = 0.0210
2025-11-30 11:27:52 - DEBUG -   Batch 100/158: Current loss = 0.0210, Running average = 0.0211
2025-11-30 11:27:55 - DEBUG -   Batch 110/158: Current loss = 0.0200, Running average = 0.0211
2025-11-30 11:27:59 - DEBUG -   Batch 120/158: Current loss = 0.0231, Running average = 0.0211
2025-11-30 11:28:02 - DEBUG -   Batch 130/158: Current loss = 0.0205, Running average = 0.0211
2025-11-30 11:28:06 - DEBUG -   Batch 140/158: Current loss = 0.0208, Running average = 0.0211
2025-11-30 11:28:10 - DEBUG -   Batch 150/158: Current loss = 0.0243, Running average = 0.0211
2025-11-30 11:28:13 - INFO - Epoch statistics:
2025-11-30 11:28:13 - INFO -   - Average loss: 0.0211
2025-11-30 11:28:13 - INFO -   - Min batch loss: 0.0149
2025-11-30 11:28:13 - INFO -   - Max batch loss: 0.0273
2025-11-30 11:28:13 - INFO - 
2025-11-30 11:28:13 - INFO - Epoch 9/10 completed - Average Loss: 0.0211
2025-11-30 11:28:13 - INFO - 
2025-11-30 11:28:13 - INFO - 
2025-11-30 11:28:13 - INFO - --------------------------------------------------------------------------------
2025-11-30 11:28:13 - INFO - EPOCH 10/10
2025-11-30 11:28:13 - INFO - --------------------------------------------------------------------------------
2025-11-30 11:28:13 - INFO - 
2025-11-30 11:28:13 - INFO - Processing 158 batches...
2025-11-30 11:28:13 - DEBUG -   First batch gradient norm: 0.0479
2025-11-30 11:28:17 - DEBUG -   Batch 10/158: Current loss = 0.0188, Running average = 0.0198
2025-11-30 11:28:20 - DEBUG -   Batch 20/158: Current loss = 0.0199, Running average = 0.0198
2025-11-30 11:28:24 - DEBUG -   Batch 30/158: Current loss = 0.0196, Running average = 0.0199
2025-11-30 11:28:28 - DEBUG -   Batch 40/158: Current loss = 0.0187, Running average = 0.0202
2025-11-30 11:28:32 - DEBUG -   Batch 50/158: Current loss = 0.0185, Running average = 0.0202
2025-11-30 11:28:36 - DEBUG -   Batch 60/158: Current loss = 0.0208, Running average = 0.0201
2025-11-30 11:28:40 - DEBUG -   Batch 70/158: Current loss = 0.0168, Running average = 0.0201
2025-11-30 11:28:44 - DEBUG -   Batch 80/158: Current loss = 0.0226, Running average = 0.0204
2025-11-30 11:28:48 - DEBUG -   Batch 90/158: Current loss = 0.0221, Running average = 0.0205
2025-11-30 11:28:52 - DEBUG -   Batch 100/158: Current loss = 0.0220, Running average = 0.0206
2025-11-30 11:28:56 - DEBUG -   Batch 110/158: Current loss = 0.0199, Running average = 0.0206
2025-11-30 11:28:59 - DEBUG -   Batch 120/158: Current loss = 0.0191, Running average = 0.0206
2025-11-30 11:29:03 - DEBUG -   Batch 130/158: Current loss = 0.0224, Running average = 0.0207
2025-11-30 11:29:07 - DEBUG -   Batch 140/158: Current loss = 0.0204, Running average = 0.0206
2025-11-30 11:29:11 - DEBUG -   Batch 150/158: Current loss = 0.0224, Running average = 0.0207
2025-11-30 11:29:14 - INFO - Epoch statistics:
2025-11-30 11:29:14 - INFO -   - Average loss: 0.0207
2025-11-30 11:29:14 - INFO -   - Min batch loss: 0.0155
2025-11-30 11:29:14 - INFO -   - Max batch loss: 0.0257
2025-11-30 11:29:14 - INFO - 
2025-11-30 11:29:14 - INFO - Epoch 10/10 completed - Average Loss: 0.0207
2025-11-30 11:29:14 - INFO - 
2025-11-30 11:29:14 - INFO - ================================================================================
2025-11-30 11:29:14 - INFO - TRAINING COMPLETE
2025-11-30 11:29:14 - INFO - ================================================================================
2025-11-30 11:29:14 - INFO - 
2025-11-30 11:29:14 - INFO - Training duration: 00:08:58 (538.17 seconds)
2025-11-30 11:29:14 - INFO - 
2025-11-30 11:29:14 - INFO - ================================================================================
2025-11-30 11:29:14 - INFO - STEP 8: MODEL SAVING
2025-11-30 11:29:14 - INFO - ================================================================================
2025-11-30 11:29:14 - INFO - 
2025-11-30 11:29:14 - INFO - Saving trained model to: 50_models\dataset_toy\embed128_layers3_heads4_epochs10.pt
2025-11-30 11:29:14 - INFO - 
2025-11-30 11:29:14 - INFO - Saving model components:
2025-11-30 11:29:14 - INFO -   - Model weights (state_dict)
2025-11-30 11:29:14 - INFO -   - Vocabulary mappings (character to index)
2025-11-30 11:29:14 - INFO -   - Model hyperparameters
2025-11-30 11:29:14 - INFO - 
2025-11-30 11:29:14 - INFO - Model saved successfully!
2025-11-30 11:29:14 - INFO -   - File path: 50_models\dataset_toy\embed128_layers3_heads4_epochs10.pt
2025-11-30 11:29:14 - INFO -   - File size: 2.40 MB
2025-11-30 11:29:14 - INFO - 
2025-11-30 11:29:14 - INFO - ================================================================================
2025-11-30 11:29:14 - INFO - TRAINING SESSION ENDED
2025-11-30 11:29:14 - INFO - ================================================================================
2025-11-30 11:29:14 - INFO - 
